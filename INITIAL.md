# INITIAL - Sistema de Aprendizaje Progresivo para Marketing Brain

## üìã INFORMACI√ìN DEL PROYECTO

```yaml
nombre: "Marketing Brain - Sistema de Aprendizaje Progresivo desde Libros"
version: "3.0.0 - Knowledge Enhancement Module"
fecha_inicio: "2026-01-30"
tipo_proyecto: "Feature Addition - Knowledge Ingestion System"
proyecto_base: "Marketing Second Brain (EXISTENTE y FUNCIONANDO)"
referencia_base: "@PRPs/marketing-brain-system-v3.md, @INITIAL2.md"
```

---

## üéØ OBJETIVO PRINCIPAL DE ESTE M√ìDULO

**‚ö†Ô∏è CR√çTICO - LEER PRIMERO:**

Este m√≥dulo **NO ES UN PROYECTO NUEVO**. Es una **EXTENSI√ìN** del sistema Marketing Brain que **YA EXISTE Y FUNCIONA**.

**Lo que YA tenemos funcionando:**
- ‚úÖ Backend FastAPI completo con agentes IA
- ‚úÖ Frontend Next.js 14 con interfaz de chat
- ‚úÖ Base de datos Supabase con pgvector
- ‚úÖ Sistema de memoria triple (short-term, long-term, semantic)
- ‚úÖ RAGService para b√∫squeda sem√°ntica
- ‚úÖ LLMService para interacci√≥n con OpenAI/OpenRouter
- ‚úÖ Agentes: RouterAgent, BuyerPersonaAgent, ContentGeneratorAgent, GeneralChatAgent
- ‚úÖ Sistema de documentos: usuarios pueden subir .txt, .pdf, .docx

**Lo que queremos AGREGAR:**
- üéØ Sistema de "aprendizaje progresivo" para que el agente **comprenda** libros completos
- üéØ Pipeline que toma un libro ‚Üí lo divide en chunks ‚Üí el LLM "aprende" ‚Üí guarda conceptos clave
- üéØ Memoria estructurada que almacena conocimiento **destilado** (no texto completo)
- üéØ Integraci√≥n perfecta con RAGService existente

---

## üß† CONCEPTO: APRENDIZAJE PROGRESIVO (No Almacenamiento Crudo)

### La Diferencia Clave

**‚ùå Sistema RAG tradicional (lo que NO queremos):**
```
Libro (500 p√°ginas) ‚Üí Chunks de 1000 tokens ‚Üí Embeddings ‚Üí Vector DB
Usuario pregunta ‚Üí B√∫squeda sem√°ntica ‚Üí Retorna chunks crudos ‚Üí LLM responde
```
**Problema:** El agente no "aprendi√≥" el libro, solo lo tiene almacenado. Cada consulta depende de encontrar el chunk exacto.

**‚úÖ Sistema de Aprendizaje Progresivo (lo que S√ç queremos - H√çBRIDO):**
```
Libro (500 p√°ginas) ‚Üí 
  Chunks de 1500 tokens con overlap 200 ‚Üí
  Por cada chunk: LLM extrae conceptos clave (200-300 palabras) ‚Üí
  Por cada grupo de chunks relacionados: LLM genera resumen tem√°tico ‚Üí
  Resumen global del libro con √≠ndice de conceptos ‚Üí
  
Almacenamiento en 3 capas:
  1. Conceptos extra√≠dos (embeddings) ‚Üê b√∫squeda sem√°ntica r√°pida
  2. Res√∫menes tem√°ticos (texto estructurado) ‚Üê contexto medio
  3. √çndice global de conceptos (JSON) ‚Üê navegaci√≥n r√°pida

Usuario pregunta ‚Üí 
  B√∫squeda en conceptos extra√≠dos (capa 1) ‚Üí
  Si necesita m√°s contexto: res√∫menes tem√°ticos (capa 2) ‚Üí
  LLM responde con conocimiento "comprendido"
```

**Beneficio:** El agente "estudi√≥" el libro como lo har√≠a un humano. No memoriza todo, pero entiende los conceptos y puede consultarlos r√°pidamente.

---

## üèóÔ∏è INTEGRACI√ìN CON STACK EXISTENTE

### ‚ö†Ô∏è REGLA DE ORO: APROVECHAR, NO DUPLICAR

**Antes de escribir UNA SOLA L√çNEA de c√≥digo, el desarrollador DEBE:**

1. **Leer y entender el proyecto existente:**
   ```bash
   # OBLIGATORIO - Usar MCP Serena para an√°lisis
   get_symbols_overview('backend/src/')
   get_symbols_overview('frontend/app/')
   
   # Entender servicios existentes
   find_symbol('RAGService', 'backend/src/services/rag_service.py', True)
   find_symbol('LLMService', 'backend/src/services/llm_service.py', True)
   find_symbol('EmbeddingService', 'backend/src/services/embedding_service.py', True)
   find_symbol('DocumentProcessor', 'backend/src/services/document_processor.py', True)
   
   # Entender estructura de base de datos
   view('backend/src/db/models.py')
   view('backend/db/001_initial_schema.sql')
   view('backend/db/002_add_user_document_summary.sql')
   ```

2. **Identificar qu√© c√≥digo REUTILIZAR:**
   - ‚úÖ `RAGService.search_relevant_docs()` ‚Üí Ya hace b√∫squeda sem√°ntica
   - ‚úÖ `EmbeddingService.generate_embeddings()` ‚Üí Ya genera embeddings
   - ‚úÖ `LLMService.generate()` ‚Üí Ya interact√∫a con LLM
   - ‚úÖ `DocumentProcessor` ‚Üí Ya tiene parsers para .txt, .pdf, .docx
   - ‚úÖ `RecursiveCharacterTextSplitter` ‚Üí Ya hace chunking

3. **Identificar qu√© c√≥digo EXTENDER (no reemplazar):**
   - üîß `marketing_knowledge_base` tabla ‚Üí Agregar columna `knowledge_type` para diferenciar
   - üîß `RAGService` ‚Üí Agregar m√©todo `search_learned_concepts()` especializado
   - üîß Crear `BookLearningService` que **usa** servicios existentes

4. **Identificar qu√© c√≥digo CREAR (nuevo):**
   - ‚ú® `backend/src/services/book_learning_service.py` (orquestador del pipeline)
   - ‚ú® `backend/src/api/knowledge.py` (endpoints para subir/gestionar libros)
   - ‚ú® Nueva tabla: `marketing_learned_books` (metadata de libros procesados)

---

## üìä ARQUITECTURA DEL SISTEMA H√çBRIDO (DETALLADA)

### Stack Tecnol√≥gico Existente (NO CAMBIAR)

```yaml
Backend: FastAPI (Python 3.11) ‚úÖ
ORM: SQLAlchemy 2.0 async ‚úÖ
Validaci√≥n: Pydantic v2 ‚úÖ
Base de Datos: Supabase Postgres + pgvector ‚úÖ
LLM: OpenAI / OpenRouter (v√≠a LLMService) ‚úÖ
Embeddings: OpenAI text-embedding-3-small (v√≠a EmbeddingService) ‚úÖ
Frontend: Next.js 14, App Router, TypeScript, Tailwind ‚úÖ
```

### Componentes a Integrar (NUEVOS)

#### 1. BookLearningService (Orquestador Principal)

**Ubicaci√≥n:** `backend/src/services/book_learning_service.py`

**Responsabilidades:**
- Coordinar todo el pipeline de aprendizaje
- **USAR** servicios existentes (DocumentProcessor, LLMService, EmbeddingService, RAGService)
- **NO duplicar** l√≥gica de chunking ni embeddings

**Flujo de procesamiento:**
```python
class BookLearningService:
    def __init__(self, llm_service: LLMService, embedding_service: EmbeddingService):
        self.llm = llm_service
        self.embeddings = embedding_service
        self.chunker = RecursiveCharacterTextSplitter(
            chunk_size=1500,
            chunk_overlap=200
        )
    
    async def process_book(self, file_path: str, project_id: str) -> LearnedBook:
        """
        Pipeline completo de aprendizaje progresivo
        
        IMPORTANTE: Este m√©todo orquesta, NO reimplementa.
        Usa servicios existentes para cada paso.
        """
        # 1. Extraer texto (USAR DocumentProcessor existente)
        raw_text = await self._extract_text_from_file(file_path)
        
        # 2. Chunking con overlap (USAR RecursiveCharacterTextSplitter)
        chunks = self.chunker.split_text(raw_text)
        
        # 3. CAPA 1 - Extracci√≥n de conceptos por chunk
        chunk_concepts = []
        for i, chunk in enumerate(chunks):
            concepts = await self._extract_concepts_from_chunk(chunk, i)
            chunk_concepts.append(concepts)
        
        # 4. CAPA 2 - Agrupar chunks relacionados y generar res√∫menes tem√°ticos
        thematic_summaries = await self._generate_thematic_summaries(
            chunks, chunk_concepts
        )
        
        # 5. CAPA 3 - Resumen global + √≠ndice de conceptos
        global_summary = await self._generate_global_summary(
            chunk_concepts, thematic_summaries
        )
        
        # 6. Generar embeddings (USAR EmbeddingService existente)
        concept_embeddings = await self._embed_concepts(chunk_concepts)
        
        # 7. Almacenar en base de datos (NUEVAS tablas + marketing_knowledge_base)
        learned_book = await self._save_learned_book(
            project_id=project_id,
            chunk_concepts=chunk_concepts,
            thematic_summaries=thematic_summaries,
            global_summary=global_summary,
            embeddings=concept_embeddings
        )
        
        return learned_book
    
    async def _extract_concepts_from_chunk(
        self, chunk: str, chunk_index: int
    ) -> ConceptExtraction:
        """
        Usa LLM para extraer conceptos clave de un chunk
        
        IMPORTANTE: Esto NO es un resumen. Es una "destilaci√≥n" sem√°ntica.
        El LLM identifica: conceptos clave, relaciones, ejemplos importantes.
        """
        prompt = f"""
Eres un experto en marketing que est√° estudiando un libro.
Tu tarea es extraer los conceptos M√ÅS IMPORTANTES de este fragmento.

NO hagas un resumen. Extrae CONCEPTOS CLAVE que un estudiante deber√≠a recordar.

Fragmento del libro (chunk {chunk_index}):
{chunk}

Extrae:
1. Conceptos principales (m√°ximo 5)
2. Relaciones entre conceptos
3. Ejemplos o casos clave mencionados
4. T√©rminos t√©cnicos importantes

Formato de respuesta (JSON):
{{
    "main_concepts": ["concepto1", "concepto2", ...],
    "relationships": ["concepto1 causa concepto2", ...],
    "key_examples": ["ejemplo1", ...],
    "technical_terms": {{"t√©rmino": "definici√≥n", ...}}
}}

S√© CONCISO. M√°ximo 200-300 palabras en total.
"""
        
        # USAR LLMService existente
        response = await self.llm.generate(
            prompt=prompt,
            temperature=0.3  # Baja temperatura para extracci√≥n precisa
        )
        
        return ConceptExtraction.parse_from_llm_response(response)
```

#### 2. Nuevas Tablas en Base de Datos

**‚ö†Ô∏è CR√çTICO:** Estas tablas se **INTEGRAN** con el esquema existente, no lo reemplazan.

**Tabla 1: `marketing_learned_books` (metadata de libros procesados)**

```sql
CREATE TABLE marketing_learned_books (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    project_id UUID REFERENCES marketing_projects(id) NOT NULL,
    
    -- Metadata del libro
    title VARCHAR(500) NOT NULL,
    author VARCHAR(255),
    file_path VARCHAR(1000), -- Ruta al archivo original
    file_type VARCHAR(10), -- 'pdf', 'txt', 'docx'
    
    -- Estado del procesamiento
    processing_status VARCHAR(50) DEFAULT 'pending', -- 'pending', 'processing', 'completed', 'failed'
    total_chunks INTEGER,
    processed_chunks INTEGER DEFAULT 0,
    
    -- Resumen global del libro
    global_summary JSONB, -- Estructura: {summary: str, key_topics: [], concept_index: {}}
    
    -- Timestamps
    created_at TIMESTAMP DEFAULT NOW(),
    completed_at TIMESTAMP,
    
    -- √çndices
    INDEX idx_learned_books_project (project_id),
    INDEX idx_learned_books_status (processing_status)
);
```

**Tabla 2: Extender `marketing_knowledge_base` (AGREGAR columna, no crear tabla nueva)**

```sql
-- MIGRACI√ìN: Agregar columna para diferenciar tipos de conocimiento
ALTER TABLE marketing_knowledge_base 
ADD COLUMN knowledge_type VARCHAR(50) DEFAULT 'raw_chunk';

-- Valores posibles de knowledge_type:
-- 'raw_chunk' = chunk tradicional de RAG (existente)
-- 'extracted_concept' = concepto extra√≠do por LLM (NUEVO)
-- 'thematic_summary' = resumen tem√°tico (NUEVO)
-- 'user_document' = documento subido por usuario (existente)

-- Agregar columna para enlazar con libro aprendido
ALTER TABLE marketing_knowledge_base 
ADD COLUMN learned_book_id UUID REFERENCES marketing_learned_books(id) ON DELETE CASCADE;

-- √çndice para b√∫squedas por tipo
CREATE INDEX idx_knowledge_type ON marketing_knowledge_base(knowledge_type);
CREATE INDEX idx_learned_book ON marketing_knowledge_base(learned_book_id);
```

**Tabla 3: `marketing_book_concepts` (conceptos extra√≠dos estructurados)**

```sql
CREATE TABLE marketing_book_concepts (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    learned_book_id UUID REFERENCES marketing_learned_books(id) ON DELETE CASCADE,
    chunk_index INTEGER NOT NULL,
    
    -- Conceptos extra√≠dos (estructura JSON del LLM)
    main_concepts TEXT[], -- Array de conceptos principales
    relationships TEXT[], -- Array de relaciones entre conceptos
    key_examples TEXT[], -- Array de ejemplos importantes
    technical_terms JSONB, -- {t√©rmino: definici√≥n}
    
    -- Texto resumido del chunk (200-300 palabras)
    condensed_text TEXT,
    
    -- Embedding del concepto condensado
    embedding VECTOR(1536),
    
    created_at TIMESTAMP DEFAULT NOW(),
    
    -- √çndices
    INDEX idx_book_concepts_book (learned_book_id),
    INDEX idx_book_concepts_embedding USING ivfflat (embedding vector_cosine_ops)
);
```

#### 3. Endpoints de API (NUEVOS)

**Archivo:** `backend/src/api/knowledge.py`

```python
from fastapi import APIRouter, UploadFile, Depends
from typing import List
from ..services.book_learning_service import BookLearningService
from ..schemas.knowledge import (
    LearnedBookResponse, 
    BookProcessingStatus,
    ConceptSearchRequest,
    ConceptSearchResponse
)

router = APIRouter(prefix="/api/knowledge", tags=["knowledge"])

@router.post("/books/upload")
async def upload_book_for_learning(
    file: UploadFile,
    project_id: str,
    book_learning_service: BookLearningService = Depends()
):
    """
    Sube un libro para que el agente lo "aprenda"
    
    Proceso:
    1. Valida archivo (.pdf, .txt, .docx)
    2. Guarda temporalmente
    3. Inicia procesamiento as√≠ncrono (background task)
    4. Retorna ID de tarea para seguimiento
    """
    # IMPORTANTE: Reutilizar validaciones existentes de DocumentProcessor
    # NO duplicar c√≥digo de validaci√≥n de archivos
    
@router.get("/books/{book_id}/status")
async def get_book_processing_status(
    book_id: str
) -> BookProcessingStatus:
    """
    Consulta el estado del procesamiento de un libro
    
    Retorna:
    - processing_status: 'pending' | 'processing' | 'completed' | 'failed'
    - processed_chunks: X de Y
    - estimated_completion: timestamp (si est√° procesando)
    """
    
@router.get("/books")
async def list_learned_books(
    project_id: str
) -> List[LearnedBookResponse]:
    """
    Lista todos los libros que el agente ha "aprendido"
    
    Retorna metadata: t√≠tulo, autor, estado, fecha, conceptos clave
    """
    
@router.post("/concepts/search")
async def search_learned_concepts(
    request: ConceptSearchRequest,
    project_id: str
) -> ConceptSearchResponse:
    """
    B√∫squeda sem√°ntica en conceptos aprendidos
    
    IMPORTANTE: Usa RAGService existente, pero filtra por knowledge_type
    
    Retorna:
    - Conceptos relevantes (no chunks crudos)
    - Res√∫menes tem√°ticos relacionados
    - Referencias a libros de origen
    """
    # USAR RAGService.search_relevant_docs() con filtro adicional
    
@router.delete("/books/{book_id}")
async def delete_learned_book(
    book_id: str,
    project_id: str
):
    """
    Elimina un libro aprendido y todos sus conceptos
    
    IMPORTANTE: Eliminar tambi√©n de marketing_knowledge_base
    """
```

#### 4. Integraci√≥n con Agentes Existentes

**‚ö†Ô∏è CR√çTICO:** Los agentes existentes **NO se reescriben**, se **EXTIENDEN**.

**Modificaci√≥n en `ContentGeneratorAgent`:**

```python
# backend/src/agents/content_generator_agent.py

class ContentGeneratorAgent(BaseAgent):
    async def execute(self, user_message: str, chat_id: str) -> str:
        # ... c√≥digo existente ...
        
        # NUEVO: Adem√°s de buscar en documentos y knowledge base tradicional,
        # buscar en conceptos aprendidos
        
        learned_concepts = await self._search_learned_concepts(
            query=user_message,
            project_id=self.project_id,
            limit=5
        )
        
        # Construir prompt con contexto enriquecido
        context = {
            "buyer_persona": buyer_persona,  # Existente
            "customer_journey": customer_journey,  # Existente
            "relevant_docs": relevant_docs,  # Existente
            "learned_concepts": learned_concepts  # NUEVO
        }
        
        # ... resto del c√≥digo ...
    
    async def _search_learned_concepts(
        self, query: str, project_id: str, limit: int
    ) -> List[ConceptExtraction]:
        """
        NUEVO m√©todo que busca en conceptos aprendidos
        
        IMPORTANTE: Usa RAGService existente con filtro knowledge_type
        """
        # NO duplicar c√≥digo de b√∫squeda sem√°ntica
        # USAR RAGService.search_relevant_docs() con filtro
```

---

## üîå MCPs A UTILIZAR

### 1. MCP Serena (‚ö° OBLIGATORIO - USAR PRIMERO)

```yaml
Prop√≥sito: An√°lisis simb√≥lico del proyecto existente

ANTES DE CODIFICAR CUALQUIER COSA:
  1. get_symbols_overview('backend/src/services/')
     ‚Üí Ver qu√© servicios ya existen
  
  2. find_symbol('RAGService', 'backend/src/services/rag_service.py', True)
     ‚Üí Leer implementaci√≥n completa de b√∫squeda sem√°ntica
  
  3. find_symbol('DocumentProcessor', 'backend/src/services/document_processor.py', True)
     ‚Üí Ver c√≥mo se procesan documentos actualmente
  
  4. search_for_pattern('async def.*embedding', 'backend/src/')
     ‚Üí Encontrar todas las funciones de embeddings
  
  5. find_referencing_symbols('LLMService', 'services/llm_service.py')
     ‚Üí Ver d√≥nde se usa LLMService para no duplicar

REGLA: Si Serena encuentra c√≥digo similar, REUTILIZARLO, NO reescribirlo
```

### 2. MCP Archon (‚ö° PRIORITARIO - Documentaci√≥n)

```yaml
Prop√≥sito: Consultar documentaci√≥n oficial

Usar para:
  - "langchain recursive text splitter overlap"
  - "pydantic v2 nested model validation"
  - "fastapi background tasks async"
  - "supabase pgvector similarity threshold"
  - "openai embeddings batch processing"

Ejemplo:
  rag_search_knowledge_base(
    query="langchain text splitter chunk overlap best practices",
    source_id="src_langchain",
    match_count=5
  )
```

### 3. MCP Custom (Futuro - Fase 2)

```yaml
Prop√≥sito: Exponer funcionalidad del sistema

Tools a implementar (despu√©s de MVP):
  - analyze_learned_books(project_id)
  - search_concepts(query, book_id)
  - get_book_summary(book_id)
```

---

## üìö SKILLS A UTILIZAR

### Fase de An√°lisis del Proyecto Existente

```yaml
Skills:

1. @.cursor/skills/code-comprehension/SKILL.md
   Cu√°ndo: Al inicio, para entender proyecto existente
   Por qu√©: Evitar duplicaci√≥n de c√≥digo
   Activaci√≥n: "analizar proyecto", "entender arquitectura"

2. @.cursor/skills/architecture/SKILL.md
   Cu√°ndo: Decidir c√≥mo integrar el nuevo m√≥dulo
   Por qu√©: Evaluar trade-offs de diferentes enfoques
   Activaci√≥n: "decidir integraci√≥n", "evaluar arquitectura"
```

### Fase de Desarrollo

```yaml
Skills:

3. @.cursor/skills/python-patterns/SKILL.md
   Cu√°ndo: Escribiendo c√≥digo Python
   Por qu√©: Mejores pr√°cticas, async patterns
   Activaci√≥n autom√°tica: Proyecto Python detectado

4. @.cursor/skills/clean-code/SKILL.md
   Cu√°ndo: Durante todo el desarrollo
   Por qu√©: C√≥digo conciso, directo, sin duplicaci√≥n
   Activaci√≥n autom√°tica: Al escribir c√≥digo

5. @.cursor/skills/rag-implementation/SKILL.md
   Cu√°ndo: Extendiendo RAGService
   Por qu√©: Chunking optimization, retrieval strategies
   Invocaci√≥n: @.cursor/skills/rag-implementation/SKILL.md extender RAG

6. @.cursor/skills/agent-memory-systems/SKILL.md
   Cu√°ndo: Integrando con sistema de memoria existente
   Por qu√©: Entender c√≥mo agregar nueva capa de memoria
   Invocaci√≥n: @.cursor/skills/agent-memory-systems/SKILL.md integrar memoria
```

### Fase de Testing

```yaml
Skills:

7. @.cursor/skills/testing-patterns/SKILL.md
   Cu√°ndo: Escribiendo tests de integraci√≥n
   Por qu√©: Validar que nuevo c√≥digo funciona con c√≥digo existente
   Activaci√≥n autom√°tica: Al escribir tests
```

---

## üöß FASES DEL PROYECTO

### FASE 0: An√°lisis del Proyecto Existente (‚ö° OBLIGATORIA)

```yaml
Objetivo: Entender completamente el c√≥digo existente antes de tocar nada

Tareas:

1. An√°lisis de Servicios Existentes (con Serena):
   - Leer RAGService completo
   - Leer LLMService completo
   - Leer EmbeddingService completo
   - Leer DocumentProcessor completo
   - Identificar funciones reutilizables

2. An√°lisis de Base de Datos:
   - Leer schema actual (001_initial_schema.sql, 002_*.sql)
   - Entender tabla marketing_knowledge_base
   - Identificar campos que necesitamos agregar

3. An√°lisis de Agentes:
   - Leer RouterAgent, ContentGeneratorAgent, GeneralChatAgent
   - Identificar puntos de integraci√≥n
   - Entender flujo de memoria actual

4. Documentar Hallazgos:
   - Crear docs/INTEGRATION_ANALYSIS.md
   - Listar servicios a reutilizar
   - Listar c√≥digo a extender
   - Listar c√≥digo nuevo a crear

Criterios de aceptaci√≥n:
  - [ ] Serena ejecutado en todos los m√≥dulos clave
  - [ ] Documento INTEGRATION_ANALYSIS.md creado
  - [ ] Lista clara de "reutilizar vs extender vs crear"
  - [ ] Diagrama de integraci√≥n dibujado
```

### FASE 1: Migraci√≥n de Base de Datos

```yaml
Objetivo: Agregar tablas y columnas necesarias SIN romper existentes

Tareas:

1. Crear migraci√≥n 003_book_learning_system.sql:
   - CREATE TABLE marketing_learned_books
   - ALTER TABLE marketing_knowledge_base ADD COLUMN knowledge_type
   - ALTER TABLE marketing_knowledge_base ADD COLUMN learned_book_id
   - CREATE TABLE marketing_book_concepts
   - Crear √≠ndices necesarios

2. Actualizar models.py:
   - Agregar modelo MarketingLearnedBook
   - Agregar modelo MarketingBookConcept
   - Extender modelo MarketingKnowledgeBase (agregar campos nuevos)

3. Validar migraci√≥n:
   - Ejecutar en DB de desarrollo
   - Verificar que datos existentes no se afectan
   - Verificar que √≠ndices se crean correctamente

Archivos a crear:
  - backend/db/003_book_learning_system.sql
  
Archivos a modificar:
  - backend/src/db/models.py (AGREGAR modelos, no modificar existentes)

Skills a usar:
  - @.cursor/skills/database-design/SKILL.md

MCPs a consultar:
  - Archon: "sqlalchemy 2.0 alembic migrations"
  - Serena: Analizar models.py existente antes de modificar

Criterios de aceptaci√≥n:
  - [ ] Migraci√≥n ejecutada sin errores
  - [ ] Datos existentes intactos
  - [ ] Nuevos modelos en models.py
  - [ ] Tests de migraci√≥n pasando
```

### FASE 2: BookLearningService (Pipeline de Aprendizaje)

```yaml
Objetivo: Crear servicio que orquesta aprendizaje progresivo

Tareas:

1. Crear BookLearningService:
   - __init__ con dependencias (LLMService, EmbeddingService, etc.)
   - process_book() m√©todo principal
   - _extract_text_from_file() REUTILIZA DocumentProcessor
   - _extract_concepts_from_chunk() usa LLMService
   - _generate_thematic_summaries() usa LLMService
   - _generate_global_summary() usa LLMService
   - _embed_concepts() REUTILIZA EmbeddingService
   - _save_learned_book() guarda en DB

2. Crear schemas Pydantic:
   - ConceptExtraction
   - ThematicSummary
   - LearnedBook
   - BookProcessingStatus

3. Implementar procesamiento por lotes:
   - Procesar chunks en batches de 10 (evitar rate limits)
   - Progress tracking en DB (processed_chunks)
   - Retry logic con exponential backoff

Archivos a crear:
  - backend/src/services/book_learning_service.py
  - backend/src/schemas/knowledge.py

Archivos a REUTILIZAR (no modificar):
  - backend/src/services/llm_service.py
  - backend/src/services/embedding_service.py
  - backend/src/services/document_processor.py

Skills a usar:
  - @.cursor/skills/python-patterns/SKILL.md (autom√°tico)
  - @.cursor/skills/clean-code/SKILL.md (autom√°tico)

MCPs a consultar:
  - Archon: "langchain recursive character text splitter"
  - Serena: Ver c√≥mo DocumentProcessor parsea archivos

Validaci√≥n:
  - [ ] BookLearningService instanciable
  - [ ] process_book() ejecuta sin errores (archivo de prueba)
  - [ ] Conceptos extra√≠dos tienen formato correcto
  - [ ] Embeddings generados correctamente
  - [ ] Datos guardados en las 3 tablas nuevas
```

### FASE 3: API Endpoints

```yaml
Objetivo: Exponer funcionalidad v√≠a API REST

Tareas:

1. Crear backend/src/api/knowledge.py:
   - POST /api/knowledge/books/upload
   - GET /api/knowledge/books/{book_id}/status
   - GET /api/knowledge/books (list)
   - POST /api/knowledge/concepts/search
   - DELETE /api/knowledge/books/{book_id}

2. Implementar background tasks:
   - upload endpoint inicia procesamiento en background
   - FastAPI BackgroundTasks para no bloquear request

3. Validaciones:
   - Tipo de archivo (.pdf, .txt, .docx)
   - Tama√±o m√°ximo (50MB para libros)
   - project_id del usuario autenticado

4. Registrar router en main.py:
   - app.include_router(knowledge.router)

Archivos a crear:
  - backend/src/api/knowledge.py

Archivos a modificar:
  - backend/src/main.py (include_router)

MCPs a consultar:
  - Archon: "fastapi background tasks async"
  - Serena: Ver c√≥mo est√°n estructurados otros routers (chat.py, documents.py)

Validaci√≥n:
  - [ ] Endpoints registrados en /docs
  - [ ] upload endpoint acepta archivos
  - [ ] Background task inicia correctamente
  - [ ] Status endpoint retorna progreso
  - [ ] Search endpoint retorna conceptos
```

### FASE 4: Integraci√≥n con RAGService

```yaml
Objetivo: Extender RAGService para buscar en conceptos aprendidos

Tareas:

1. Extender RAGService:
   - Agregar m√©todo search_learned_concepts()
   - Modificar search_relevant_docs() para aceptar knowledge_type filter
   - Mantener compatibilidad con c√≥digo existente

2. Crear m√©todo h√≠brido:
   - search_with_learned_knowledge(query, project_id)
   - Busca en: conceptos aprendidos + documentos + knowledge base tradicional
   - Combina y rankea resultados

3. Optimizar b√∫squeda:
   - Usar √≠ndice ivfflat eficientemente
   - Threshold de similaridad ajustable
   - Limitar resultados por tipo (5 conceptos + 5 docs + 5 chunks)

Archivos a modificar:
  - backend/src/services/rag_service.py (EXTENDER, no reescribir)

Skills a usar:
  - @.cursor/skills/rag-implementation/SKILL.md

MCPs a consultar:
  - Archon: "supabase pgvector cosine similarity"
  - Serena: Leer RAGService completo antes de modificar

Validaci√≥n:
  - [ ] search_learned_concepts() funciona
  - [ ] Resultados ordenados por relevancia
  - [ ] C√≥digo existente NO roto (tests pasan)
  - [ ] Performance <100ms para b√∫squeda
```

### FASE 5: Integraci√≥n con Agentes

```yaml
Objetivo: Que agentes usen conceptos aprendidos en sus respuestas

Tareas:

1. Modificar ContentGeneratorAgent:
   - Agregar m√©todo _search_learned_concepts()
   - Incluir conceptos en contexto de prompt
   - Mantener funcionalidad existente intacta

2. Modificar GeneralChatAgent:
   - Usar conceptos aprendidos para respuestas m√°s informadas
   - Similar a ContentGeneratorAgent

3. Probar integraci√≥n end-to-end:
   - Subir libro de prueba
   - Esperar procesamiento
   - Hacer pregunta al agente
   - Verificar que usa conceptos del libro

Archivos a modificar:
  - backend/src/agents/content_generator_agent.py
  - backend/src/agents/general_chat_agent.py

‚ö†Ô∏è CR√çTICO: NO reescribir agentes, solo AGREGAR m√©todo nuevo

Skills a usar:
  - @.cursor/skills/agent-memory-systems/SKILL.md

Validaci√≥n:
  - [ ] Agentes usan conceptos aprendidos
  - [ ] Respuestas m√°s ricas y contextuales
  - [ ] Funcionalidad existente NO afectada
  - [ ] Tests de agentes pasan
```

### FASE 6: Frontend - UI para Gesti√≥n de Libros

```yaml
Objetivo: Permitir al usuario subir/gestionar libros desde UI

Tareas:

1. Crear componente BookUpload:
   - Drag & drop para archivos
   - Validaci√≥n de tipo (.pdf, .txt, .docx)
   - Progress bar durante procesamiento
   - Feedback de √©xito/error

2. Crear p√°gina /dashboard/knowledge:
   - Lista de libros aprendidos
   - Tarjetas con: t√≠tulo, autor, estado, conceptos clave
   - Bot√≥n para eliminar libro
   - Filtros por estado

3. Crear componente ConceptsViewer:
   - Visualizaci√≥n de conceptos extra√≠dos
   - B√∫squeda en conceptos
   - √Årbol de conceptos relacionados

4. Integrar en layout existente:
   - Agregar link en sidebar: "Biblioteca de Conocimiento"
   - Mantener consistencia con dise√±o existente

Componentes a crear:
  - frontend/app/components/BookUpload.tsx
  - frontend/app/dashboard/knowledge/page.tsx
  - frontend/app/components/ConceptsViewer.tsx
  - frontend/app/components/LearnedBookCard.tsx

Archivos a modificar:
  - frontend/app/components/Sidebar.tsx (agregar link)

Skills a usar:
  - @.cursor/skills/nextjs-best-practices/SKILL.md (autom√°tico)
  - @.cursor/skills/react-patterns/SKILL.md (autom√°tico)
  - @.cursor/skills/frontend-design/SKILL.md

MCPs a consultar:
  - Serena: Ver estructura de componentes existentes (ChatInterface, Sidebar)

Validaci√≥n:
  - [ ] Usuario puede subir libro
  - [ ] Progress bar funciona
  - [ ] Lista de libros se actualiza
  - [ ] UI consistente con resto del sistema
  - [ ] Responsive en mobile
```

### FASE 7: Testing y Documentaci√≥n

```yaml
Objetivo: Validar integraci√≥n completa y documentar

Tareas:

1. Tests de integraci√≥n:
   - Test: Subir libro ‚Üí procesar ‚Üí buscar concepto ‚Üí obtener respuesta
   - Test: Eliminar libro ‚Üí verificar cleanup completo
   - Test: Agente sin libros vs con libros (comparar respuestas)

2. Tests unitarios:
   - BookLearningService._extract_concepts_from_chunk()
   - RAGService.search_learned_concepts()
   - Endpoints de API

3. Documentaci√≥n:
   - docs/BOOK_LEARNING_SYSTEM.md (arquitectura)
   - Actualizar README.md (nueva funcionalidad)
   - Diagramas de flujo

4. Validar performance:
   - Benchmark: procesamiento de libro de 300 p√°ginas
   - Benchmark: b√∫squeda en 10 libros aprendidos
   - Optimizar si necesario

Archivos a crear:
  - backend/tests/integration/test_book_learning.py
  - backend/tests/unit/test_book_learning_service.py
  - docs/BOOK_LEARNING_SYSTEM.md

Archivos a modificar:
  - README.md

Skills a usar:
  - @.cursor/skills/testing-patterns/SKILL.md
  - @.cursor/skills/systematic-debugging/SKILL.md

Criterios de aceptaci√≥n:
  - [ ] Coverage >80% en c√≥digo nuevo
  - [ ] Tests de integraci√≥n pasan
  - [ ] Documentaci√≥n completa
  - [ ] Performance aceptable (<5 min para libro de 300 p√°gs)
```

---

## ‚ö†Ô∏è CONSIDERACIONES T√âCNICAS CR√çTICAS

### üö´ PROHIBICIONES ABSOLUTAS

```yaml
1. DUPLICAR C√ìDIGO EXISTENTE:
   ‚ùå NUNCA: Copiar funciones de RAGService, LLMService, etc.
   ‚úÖ SIEMPRE: Importar y usar servicios existentes
   
   ‚ùå NUNCA: Crear nueva funci√≥n de embeddings
   ‚úÖ SIEMPRE: Usar EmbeddingService.generate_embeddings()

2. MODIFICAR TABLAS EXISTENTES SIN MIGRACI√ìN:
   ‚ùå NUNCA: Cambiar columnas de marketing_knowledge_base directamente
   ‚úÖ SIEMPRE: Crear migraci√≥n SQL numerada (003_*.sql)

3. ROMPER COMPATIBILIDAD:
   ‚ùå NUNCA: Cambiar firma de m√©todos existentes
   ‚úÖ SIEMPRE: Agregar m√©todos nuevos o par√°metros opcionales

4. IGNORAR AN√ÅLISIS PREVIO:
   ‚ùå NUNCA: Codificar sin haber usado Serena primero
   ‚úÖ SIEMPRE: get_symbols_overview() antes de modificar archivos
```

### üîê Seguridad

```yaml
1. Validaci√≥n de Archivos:
   - Validar MIME type (no solo extensi√≥n)
   - L√≠mite de tama√±o: 50MB para libros
   - Escanear con antivirus si es posible
   - Sanitizar nombres de archivo

2. Aislamiento por Proyecto:
   - TODOS los queries incluyen WHERE project_id = ?
   - Libros aprendidos son privados por proyecto
   - No mezclar conceptos entre proyectos

3. Rate Limiting:
   - L√≠mite de 5 libros por d√≠a por usuario
   - L√≠mite de 100 b√∫squedas de conceptos por hora
```

### ‚ö° Performance

```yaml
1. Procesamiento de Libros:
   - Procesamiento as√≠ncrono (background task)
   - Chunks procesados en batches de 10
   - Progress tracking para UX
   - Timeout de 30 min por libro

2. B√∫squeda de Conceptos:
   - √çndice ivfflat optimizado
   - Cache en Redis (TTL 1 hora)
   - L√≠mite de 10 conceptos por b√∫squeda
   - <100ms target latency

3. Embeddings:
   - Batch processing (evitar rate limits)
   - Retry con exponential backoff
   - Cache de embeddings id√©nticos
```

### üêõ Gotchas Espec√≠ficos del Sistema de Aprendizaje

```yaml
1. LLM Context Window:
   GOTCHA: "Chunks muy largos pueden exceder context window"
   SOLUCI√ìN: Limitar chunks a 1500 tokens, verificar antes de enviar

2. Extracci√≥n de Conceptos:
   GOTCHA: "LLM puede ser inconsistente en formato JSON"
   SOLUCI√ìN: Usar Pydantic para parsear y validar, retry si falla

3. Overlap en Chunking:
   GOTCHA: "Sin overlap se pierden conceptos en fronteras"
   SOLUCI√ìN: Overlap de 200 tokens, experimentar con 150-300

4. Embedding Similarity Threshold:
   GOTCHA: "Threshold muy bajo retorna conceptos irrelevantes"
   SOLUCI√ìN: Experimentar con 0.7-0.8, ajustar seg√∫n feedback

5. Procesamiento Largo:
   GOTCHA: "Usuario puede cerrar navegador y perder progreso"
   SOLUCI√ìN: Background task + polling de status, guardar progreso en DB
```

---

## üéØ CRITERIOS DE √âXITO DEL M√ìDULO

```yaml
Funcionalidad:
  - [ ] Usuario puede subir libro (.pdf, .txt, .docx)
  - [ ] Sistema procesa libro en background
  - [ ] Usuario ve progreso de procesamiento
  - [ ] Conceptos extra√≠dos correctamente
  - [ ] Agentes usan conceptos en respuestas
  - [ ] B√∫squeda de conceptos funcional
  - [ ] Usuario puede eliminar libro
  - [ ] Cleanup completo al eliminar

Integraci√≥n:
  - [ ] NO duplica c√≥digo existente
  - [ ] USA servicios existentes (RAG, LLM, Embeddings)
  - [ ] EXTIENDE agentes sin romperlos
  - [ ] Compatibilidad con c√≥digo existente al 100%
  - [ ] Tests existentes siguen pasando

Calidad:
  - [ ] Coverage >80% en c√≥digo nuevo
  - [ ] Sin errores de linting (ruff, mypy)
  - [ ] Sin errores de tipos (TypeScript)
  - [ ] Documentaci√≥n completa

Performance:
  - [ ] Libro de 300 p√°ginas procesado <5 min
  - [ ] B√∫squeda de conceptos <100ms
  - [ ] UI responsive durante procesamiento
```

---

## üöÄ EJEMPLO DE USO DEL SISTEMA

```yaml
Escenario: Usuario quiere entrenar agente con "Influence" de Robert Cialdini

Paso 1 - Subir libro:
  Usuario ‚Üí Dashboard ‚Üí Biblioteca de Conocimiento ‚Üí Subir Libro
  Selecciona: "Influence.pdf" (450 p√°ginas)
  Sistema: Inicia procesamiento en background

Paso 2 - Procesamiento (autom√°tico):
  Sistema divide en ~300 chunks (1500 tokens cada uno)
  Por cada chunk:
    - LLM extrae conceptos (5 conceptos principales)
    - LLM identifica relaciones entre conceptos
    - LLM extrae ejemplos clave
    - Sistema genera embedding del concepto
  
  Sistema agrupa chunks relacionados:
    - Cap√≠tulo 1: Principio de Reciprocidad ‚Üí Resumen tem√°tico
    - Cap√≠tulo 2: Principio de Compromiso ‚Üí Resumen tem√°tico
    - ...
  
  Sistema genera resumen global:
    - 6 principios de influencia
    - √çndice de conceptos clave
    - Mapa de relaciones

Paso 3 - Uso por el agente:
  Usuario pregunta: "Dame 5 ideas de posts sobre reciprocidad en marketing"
  
  ContentGeneratorAgent:
    1. Busca en buyer_persona (contexto del cliente)
    2. Busca en customer_journey (fase de conciencia)
    3. Busca en conceptos aprendidos: "reciprocidad marketing"
       ‚Üí Encuentra: Concepto de reciprocidad de Cialdini
       ‚Üí Encuentra: Ejemplos del libro (muestras gratis, etc.)
    4. Genera ideas basadas en conceptos del libro + contexto del cliente
  
  Resultado:
    "Bas√°ndome en el principio de reciprocidad de Cialdini y tu buyer persona:
    
    1. Post: 'Gu√≠a gratuita de X' ‚Üí explicar c√≥mo genera reciprocidad
    2. Post: Video tutorial gratis ‚Üí activar deseo de retribuir
    3. ...
    
    (Conceptos extra√≠dos de 'Influence' de Robert Cialdini, Cap. 2)"

Valor generado:
  ‚úÖ Agente tiene conocimiento profundo del libro
  ‚úÖ Respuestas basadas en principios validados (no inventados)
  ‚úÖ Cita fuentes (transparencia)
  ‚úÖ Contexto personalizado (libro + buyer persona)
```

---

## üìù NOTAS FINALES

```yaml
Filosof√≠a de Integraci√≥n:
  1. "Read first, code later": Usar Serena ANTES de tocar c√≥digo
  2. "Reuse over rewrite": Si existe, usarlo; si falta, agregarlo
  3. "Extend, don't break": Mantener compatibilidad con c√≥digo existente
  4. "Test integration, not isolation": Validar que todo funciona junto
  5. "Document for future you": El pr√≥ximo dev debe entender la integraci√≥n

Por qu√© Sistema H√≠brido:
  - Combina lo mejor de RAG tradicional (b√∫squeda r√°pida) con comprensi√≥n profunda
  - Reduce dependencia de encontrar el chunk exacto
  - Permite respuestas m√°s ricas y contextuales
  - El agente "entiende" el material, no solo lo tiene almacenado

Tiempo Estimado:
  - Fase 0 (An√°lisis): 1-2 d√≠as
  - Fase 1 (DB): 1 d√≠a
  - Fase 2 (BookLearningService): 3-4 d√≠as
  - Fase 3 (API): 2 d√≠as
  - Fase 4 (RAG Integration): 2 d√≠as
  - Fase 5 (Agents): 2 d√≠as
  - Fase 6 (Frontend): 3 d√≠as
  - Fase 7 (Testing): 2 d√≠as
  - TOTAL: 16-19 d√≠as
  - REALISTA: 3-4 semanas

Riesgos y Mitigaciones:
  - Riesgo: Romper c√≥digo existente
    Mitigaci√≥n: Tests de regresi√≥n exhaustivos
  
  - Riesgo: Performance degradada
    Mitigaci√≥n: Benchmarks antes/despu√©s, optimizar √≠ndices
  
  - Riesgo: Extracci√≥n inconsistente de conceptos
    Mitigaci√≥n: Prompts bien dise√±ados, validaci√≥n Pydantic, retry logic
  
  - Riesgo: Rate limits de OpenAI
    Mitigaci√≥n: Batch processing, exponential backoff, considerar tier pagado
```

---

**üéØ Este INITIAL.md est√° listo para generar un PRP completo que INTEGRE el sistema de aprendizaje progresivo con el proyecto Marketing Brain existente, sin duplicar c√≥digo y aprovechando toda la infraestructura ya construida.**

**√ânfasis clave:**
- ‚úÖ Reutilizar servicios existentes (RAG, LLM, Embeddings)
- ‚úÖ Extender agentes sin romperlos
- ‚úÖ Agregar tablas sin modificar existentes
- ‚úÖ Mantener compatibilidad total
- ‚úÖ An√°lisis obligatorio con Serena antes de codificar