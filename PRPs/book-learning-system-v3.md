# PRP: Sistema de Aprendizaje Progresivo desde Libros

## üìã METADATA

```yaml
nombre: "Book Learning System"
version: "3.1.0"
fecha: "2026-01-31"
descripcion: "Sistema de aprendizaje progresivo que permite al agente 'estudiar' libros completos"
tipo: "Feature Addition"
proyecto_base: "Marketing Second Brain (EXISTENTE)"
score_confianza: "9/10"
ultima_actualizacion: "A√±adida TAREA 6.5 (Seguridad) y TAREA 6.6 (GitHub) - 2026-01-31"
```

---

## üéØ OBJETIVO

Agregar un sistema de "aprendizaje progresivo" al proyecto Marketing Brain existente que permita:
1. Subir libros completos (.pdf, .txt, .docx)
2. Procesarlos con LLM para extraer conceptos clave (no almacenamiento crudo)
3. Almacenar conocimiento estructurado en 3 capas
4. Integrar con agentes existentes para respuestas m√°s ricas

**‚ö†Ô∏è CR√çTICO**: Este m√≥dulo **EXTIENDE** el sistema existente, **NO LO REEMPLAZA**.

---

## üö® FUNCIONALIDAD EXISTENTE - NO DUPLICAR

### YA EXISTE en LLMService (NO recrear):
```yaml
generate_with_messages(messages: list):
  L√≠neas: 141-197
  Estado: ‚úÖ YA IMPLEMENTADO
  Uso: Para llamadas con historial

stream_with_messages(messages: list):
  L√≠neas: 199-255
  Estado: ‚úÖ YA IMPLEMENTADO
  Uso: Para streaming con historial
```

### YA EXISTE en MemoryManager (REUTILIZAR):
```yaml
format_messages_from_memory():
  L√≠neas: 142-199
  Estado: ‚úÖ YA IMPLEMENTADO
  Uso: Convierte historial a formato OpenAI messages[]

get_training_summary():
  L√≠neas: 201-296
  Estado: ‚úÖ YA IMPLEMENTADO
  ‚≠ê PATR√ìN A SEGUIR para conceptos aprendidos
  
get_context():
  L√≠neas: 52-121
  Estado: ‚úÖ YA IMPLEMENTADO
  Acci√≥n: EXTENDER para incluir conceptos aprendidos
```

### NO EXISTE y NO SE CREAR√Å:
```yaml
GeneralChatAgent:
  Estado: ‚ùå NO EXISTE
  Raz√≥n: Estrategia cambi√≥ (ver PRP marketing-brain-conversational-agent-v3.md)
  Decisi√≥n: NO crear - mejorar ContentGeneratorAgent en su lugar
```

---

## üîç AN√ÅLISIS DEL PROYECTO EXISTENTE (Con MCP Serena)

### Servicios a REUTILIZAR (No duplicar)

```yaml
RAGService (backend/src/services/rag_service.py):
  L√≠neas: 14-351
  M√©todos disponibles:
    - search_relevant_docs() ‚Üí B√∫squeda sem√°ntica h√≠brida
    - _vector_search() ‚Üí B√∫squeda vectorial directa
    - _rerank_with_llm() ‚Üí Reranking con LLM
    - search_by_chat() ‚Üí B√∫squeda por chat_id
  Variables:
    - db, embedding_service, llm_service
  
  USO: Extender con search_learned_concepts(), NO reescribir

LLMService (backend/src/services/llm_service.py):
  L√≠neas: 15-255
  M√©todos disponibles:
    - generate(prompt, temperature) ‚Üí Generaci√≥n simple
    - stream() ‚Üí Streaming de respuesta
    - generate_with_messages() ‚Üí Con historial (‚úÖ YA EXISTE)
    - stream_with_messages() ‚Üí Streaming con historial (‚úÖ YA EXISTE)
  
  USO: Usar generate() para extracci√≥n de conceptos
  ‚ö†Ô∏è NO recrear generate_with_messages - YA EXISTE

MemoryManager (backend/src/services/memory_manager.py):
  L√≠neas: 17-421
  M√©todos disponibles:
    - get_context() ‚Üí Contexto completo para agentes (EXTENDER)
    - format_messages_from_memory() ‚Üí Convierte historial a messages[] (‚úÖ YA EXISTE)
    - get_training_summary() ‚Üí Resumen de transcripciones (‚≠ê PATR√ìN A SEGUIR)
    - load_chat_history() ‚Üí Carga historial de BD
    - ensure_chat_loaded() ‚Üí Verifica carga
  
  USO: 
    - SEGUIR PATR√ìN de get_training_summary() para conceptos
    - EXTENDER get_context() para incluir conceptos aprendidos
    - NO duplicar l√≥gica de historial

EmbeddingService (backend/src/services/embedding_service.py):
  L√≠neas: 6-59
  M√©todos disponibles:
    - generate_embedding(text) ‚Üí Embedding individual
    - generate_embeddings_batch(texts) ‚Üí Batch processing
  Variables:
    - batch_size (para rate limiting)
  
  USO: Usar generate_embeddings_batch() para conceptos

DocumentProcessor (backend/src/services/document_processor.py):
  L√≠neas: 12-100
  M√©todos disponibles:
    - process_document() ‚Üí Parsea .txt, .pdf, .docx
  Variables:
    - splitter (RecursiveCharacterTextSplitter)
  
  USO: Reutilizar l√≥gica de parsing, NO duplicar
```

### Modelos de BD Existentes

```yaml
Tablas con prefijo marketing_:
  - MarketingProject
  - MarketingUser
  - MarketingChat
  - MarketingMessage
  - MarketingBuyerPersona
  - MarketingKnowledgeBase ‚Üê EXTENDER con columnas nuevas
  - MarketingUserDocument
  - MarketingPasswordResetToken

Migraciones existentes:
  - 001_initial_schema.sql
  - 002_add_user_document_summary.sql
  
SIGUIENTE: 003_book_learning_system.sql
```

### Comandos Serena para Verificaci√≥n

```bash
# Antes de codificar, ejecutar:
get_symbols_overview('backend/src/services/rag_service.py')
find_symbol('RAGService/search_relevant_docs', 'backend/src/services/rag_service.py', True)
find_symbol('DocumentProcessor/process_document', 'backend/src/services/document_processor.py', True)
get_symbols_overview('backend/src/db/models.py')
```

---

## üìö DOCUMENTACI√ìN CONSULTADA (MCP Archon)

### Fuentes Disponibles

```yaml
LangChain (source_id: e74f94bb9dcb14aa):
  - Text splitters: https://docs.langchain.com/oss/python/integrations/splitters/index.md
  - RAG patterns: https://docs.langchain.com/oss/python/langchain/rag.md
  Query usada: "langchain text splitter recursive character chunking overlap"

FastAPI (source_id: c889b62860c33a44):
  - Background tasks: https://fastapi.tiangolo.com/tutorial/background-tasks
  - Async patterns: https://fastapi.tiangolo.com/async
  Query usada: "FastAPI background tasks async processing"

Supabase (source_id: 9c5f534e51ee9237):
  - pgvector: https://supabase.com/llms/guides.txt
  Query usada: "pgvector similarity search embeddings"

Pydantic (source_id: 9d46e91458092424):
  - Validaci√≥n de JSON desde LLM
  Query: "pydantic model validation json parsing"
```

---

## üìñ SKILLS A UTILIZAR

### Por Fase del Proyecto

```yaml
FASE 0 - An√°lisis:
  - MCP Serena: OBLIGATORIO antes de cualquier c√≥digo
  - Skill: architecture (decisiones de integraci√≥n)

FASE 1 - Base de Datos:
  - MCP Serena: Ver models.py existente
  - Skill: database-design (migraciones)

FASE 2 - BookLearningService:
  - MCP Archon: "langchain text splitter" (source: e74f94bb9dcb14aa)
  - Skill: python-patterns (autom√°tico)
  - Skill: clean-code (autom√°tico)
  - Skill: rag-implementation (extender RAG)

FASE 3 - API Endpoints:
  - MCP Archon: "FastAPI background tasks" (source: c889b62860c33a44)
  - MCP Serena: Ver estructura de chat.py, documents.py
  - Skill: api-patterns

FASE 4 - Integraci√≥n RAG:
  - MCP Archon: "pgvector similarity" (source: 9c5f534e51ee9237)
  - MCP Serena: Leer RAGService completo antes de modificar
  - Skill: rag-implementation

FASE 5 - Agentes:
  - MCP Serena: Ver ContentGeneratorAgent, GeneralChatAgent
  - Skill: agent-memory-systems

FASE 6 - Frontend:
  - MCP Serena: Ver Sidebar.tsx, ChatInterface.tsx
  - Skill: react-patterns (autom√°tico)
  - Skill: nextjs-best-practices (autom√°tico)

FASE 7 - Testing:
  - Skill: testing-patterns
  - Skill: verification-before-completion
```

---

## üèóÔ∏è TAREAS

### TAREA 0: Verificaci√≥n del Entorno y An√°lisis (‚ö° OBLIGATORIA)

**Herramientas:**
- üîß MCP Serena: An√°lisis simb√≥lico completo
- üìö Skill: architecture

**Objetivo:** Confirmar que todo el c√≥digo existente est√° entendido antes de escribir una l√≠nea.

**Pasos:**

1. **Activar proyecto en Serena:**
   ```bash
   activate_project("brain-mkt")
   ```

2. **Analizar servicios existentes:**
   ```bash
   # Ver todos los servicios
   get_symbols_overview('backend/src/services/rag_service.py')
   get_symbols_overview('backend/src/services/llm_service.py')
   get_symbols_overview('backend/src/services/embedding_service.py')
   get_symbols_overview('backend/src/services/document_processor.py')
   get_symbols_overview('backend/src/services/memory_manager.py')
   
   # Leer m√©todos clave
   find_symbol('RAGService/search_relevant_docs', 'backend/src/services/rag_service.py', True)
   find_symbol('EmbeddingService/generate_embeddings_batch', 'backend/src/services/embedding_service.py', True)
   
   # ‚≠ê CR√çTICO: Ver patr√≥n a seguir
   find_symbol('MemoryManager/get_training_summary', 'backend/src/services/memory_manager.py', True)
   find_symbol('MemoryManager/get_context', 'backend/src/services/memory_manager.py', True)
   ```

3. **Analizar agentes existentes:**
   ```bash
   get_symbols_overview('backend/src/agents/content_generator_agent.py')
   # ‚ö†Ô∏è NO existe general_chat_agent.py - NO intentar analizar
   ```

4. **Analizar base de datos:**
   ```bash
   get_symbols_overview('backend/src/db/models.py')
   # Ver tabla marketing_knowledge_base espec√≠ficamente
   find_symbol('MarketingKnowledgeBase', 'backend/src/db/models.py', True)
   ```

5. **Documentar hallazgos:**
   - Crear `docs/INTEGRATION_ANALYSIS.md` con:
     - Servicios a reutilizar
     - C√≥digo a extender
     - C√≥digo nuevo a crear

**Criterios de aceptaci√≥n:**
- [ ] Serena activado en proyecto brain-mkt
- [ ] Todos los servicios analizados (incluyendo MemoryManager)
- [ ] Patr√≥n get_training_summary() entendido y documentado
- [ ] Verificado que generate_with_messages() YA EXISTE
- [ ] Verificado que GeneralChatAgent NO existe
- [ ] INTEGRATION_ANALYSIS.md creado/actualizado
- [ ] Lista clara de "reutilizar vs extender vs crear"

---

### TAREA 1: Migraci√≥n de Base de Datos

**Herramientas:**
- üîß MCP Serena: Ver models.py existente
- ‚ö° MCP Archon: `rag_search_knowledge_base(query="sqlalchemy 2.0 relationship", source_id="...")`
- üìö Skill: database-design

**Objetivo:** Agregar tablas y columnas necesarias SIN romper existentes.

**Pasos:**

1. **Crear migraci√≥n 003:**
   ```sql
   -- backend/db/003_book_learning_system.sql
   
   -- TABLA 1: Metadata de libros procesados
   CREATE TABLE marketing_learned_books (
       id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
       project_id UUID REFERENCES marketing_projects(id) NOT NULL,
       title VARCHAR(500) NOT NULL,
       author VARCHAR(255),
       file_path VARCHAR(1000),
       file_type VARCHAR(10),
       processing_status VARCHAR(50) DEFAULT 'pending',
       total_chunks INTEGER,
       processed_chunks INTEGER DEFAULT 0,
       global_summary JSONB,
       created_at TIMESTAMP DEFAULT NOW(),
       completed_at TIMESTAMP
   );
   
   CREATE INDEX idx_learned_books_project ON marketing_learned_books(project_id);
   CREATE INDEX idx_learned_books_status ON marketing_learned_books(processing_status);
   
   -- EXTENDER tabla existente (NO crear nueva)
   ALTER TABLE marketing_knowledge_base 
   ADD COLUMN IF NOT EXISTS knowledge_type VARCHAR(50) DEFAULT 'raw_chunk';
   
   ALTER TABLE marketing_knowledge_base 
   ADD COLUMN IF NOT EXISTS learned_book_id UUID REFERENCES marketing_learned_books(id) ON DELETE CASCADE;
   
   CREATE INDEX IF NOT EXISTS idx_knowledge_type ON marketing_knowledge_base(knowledge_type);
   CREATE INDEX IF NOT EXISTS idx_learned_book ON marketing_knowledge_base(learned_book_id);
   
   -- TABLA 2: Conceptos extra√≠dos estructurados
   CREATE TABLE marketing_book_concepts (
       id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
       learned_book_id UUID REFERENCES marketing_learned_books(id) ON DELETE CASCADE,
       chunk_index INTEGER NOT NULL,
       main_concepts TEXT[],
       relationships TEXT[],
       key_examples TEXT[],
       technical_terms JSONB,
       condensed_text TEXT,
       embedding VECTOR(1536),
       created_at TIMESTAMP DEFAULT NOW()
   );
   
   CREATE INDEX idx_book_concepts_book ON marketing_book_concepts(learned_book_id);
   CREATE INDEX idx_book_concepts_embedding ON marketing_book_concepts 
       USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);
   ```

2. **Actualizar models.py:**
   - Usar Serena para ver estructura actual:
     ```bash
     find_symbol('MarketingKnowledgeBase', 'backend/src/db/models.py', True)
     ```
   - Agregar modelos (NO modificar existentes):
     - `MarketingLearnedBook`
     - `MarketingBookConcept`
   - Extender `MarketingKnowledgeBase` con campos nuevos

3. **Ejecutar migraci√≥n:**
   ```bash
   # En Supabase SQL Editor o psql
   \i backend/db/003_book_learning_system.sql
   ```

**Archivos a crear:**
- `backend/db/003_book_learning_system.sql`

**Archivos a modificar:**
- `backend/src/db/models.py` (AGREGAR modelos, no modificar existentes)

**Criterios de aceptaci√≥n:**
- [ ] Migraci√≥n ejecutada sin errores
- [ ] Datos existentes intactos (verificar con SELECT)
- [ ] Nuevos modelos en models.py
- [ ] Relaciones funcionando (test manual)

**Pseudoc√≥digo:**

```python
# backend/src/db/models.py - AGREGAR (no modificar existente)

# GOTCHA: Usar Optional para campos que pueden ser NULL
# PATR√ìN: Seguir estructura de modelos existentes (ver MarketingChat)

class MarketingLearnedBook(Base):
    __tablename__ = "marketing_learned_books"
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    project_id = Column(UUID(as_uuid=True), ForeignKey("marketing_projects.id"), nullable=False)
    title = Column(String(500), nullable=False)
    author = Column(String(255))
    file_path = Column(String(1000))
    file_type = Column(String(10))
    processing_status = Column(String(50), default="pending")
    total_chunks = Column(Integer)
    processed_chunks = Column(Integer, default=0)
    global_summary = Column(JSONB)
    created_at = Column(DateTime, default=datetime.utcnow)
    completed_at = Column(DateTime)
    
    # Relationships
    project = relationship("MarketingProject", back_populates="learned_books")
    concepts = relationship("MarketingBookConcept", back_populates="book", cascade="all, delete-orphan")

class MarketingBookConcept(Base):
    __tablename__ = "marketing_book_concepts"
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    learned_book_id = Column(UUID(as_uuid=True), ForeignKey("marketing_learned_books.id", ondelete="CASCADE"))
    chunk_index = Column(Integer, nullable=False)
    main_concepts = Column(ARRAY(String))
    relationships = Column(ARRAY(String))
    key_examples = Column(ARRAY(String))
    technical_terms = Column(JSONB)
    condensed_text = Column(Text)
    # GOTCHA: pgvector requiere Vector type de pgvector extension
    embedding = Column(Vector(1536))
    created_at = Column(DateTime, default=datetime.utcnow)
    
    book = relationship("MarketingLearnedBook", back_populates="concepts")
```

---

### TAREA 2: BookLearningService (Pipeline de Aprendizaje)

**Herramientas:**
- üîß MCP Serena: `find_symbol('DocumentProcessor/process_document', '...', True)`
- ‚ö° MCP Archon: `rag_search_knowledge_base(query="langchain recursive text splitter", source_id="e74f94bb9dcb14aa")`
- üìö Skills: python-patterns, clean-code, rag-implementation

**Objetivo:** Crear servicio que orquesta aprendizaje progresivo REUTILIZANDO servicios existentes.

**Pasos:**

1. **Analizar DocumentProcessor primero:**
   ```bash
   find_symbol('DocumentProcessor/process_document', 'backend/src/services/document_processor.py', True)
   # Ver c√≥mo hace chunking actual
   ```

2. **Crear BookLearningService:**
   - Inyectar dependencias existentes (LLMService, EmbeddingService)
   - Reutilizar RecursiveCharacterTextSplitter del DocumentProcessor
   - Implementar extracci√≥n de conceptos con LLM
   - Implementar agrupaci√≥n tem√°tica
   - Implementar resumen global

3. **Crear schemas Pydantic:**
   - ConceptExtraction (main_concepts, relationships, key_examples, technical_terms)
   - ThematicSummary
   - LearnedBookResponse
   - BookProcessingStatus

**Archivos a crear:**
- `backend/src/services/book_learning_service.py`
- `backend/src/schemas/knowledge.py`

**Archivos a REUTILIZAR (NO modificar):**
- `backend/src/services/llm_service.py`
- `backend/src/services/embedding_service.py`
- `backend/src/services/document_processor.py`

**Criterios de aceptaci√≥n:**
- [ ] BookLearningService instanciable
- [ ] USA LLMService.generate() (no reimplementa)
- [ ] USA EmbeddingService.generate_embeddings_batch() (no reimplementa)
- [ ] process_book() ejecuta sin errores
- [ ] Conceptos extra√≠dos tienen formato correcto
- [ ] Datos guardados en las 3 tablas

**Pseudoc√≥digo:**

```python
# backend/src/services/book_learning_service.py

from langchain.text_splitter import RecursiveCharacterTextSplitter
from ..services.llm_service import LLMService
from ..services.embedding_service import EmbeddingService
from ..schemas.knowledge import ConceptExtraction, LearnedBook

class BookLearningService:
    """
    Orquesta el pipeline de aprendizaje progresivo.
    
    CR√çTICO: Este servicio ORQUESTA, no reimplementa.
    Usa servicios existentes para cada operaci√≥n.
    """
    
    def __init__(
        self, 
        db: AsyncSession,
        llm_service: LLMService,  # REUTILIZAR
        embedding_service: EmbeddingService  # REUTILIZAR
    ):
        self.db = db
        self.llm = llm_service
        self.embeddings = embedding_service
        # PATR√ìN: Usar mismo splitter que DocumentProcessor
        self.chunker = RecursiveCharacterTextSplitter(
            chunk_size=1500,
            chunk_overlap=200,
            separators=["\n\n", "\n", ". ", " ", ""]
        )
    
    async def process_book(
        self, 
        file_path: str, 
        project_id: str,
        title: str,
        author: Optional[str] = None
    ) -> LearnedBook:
        """
        Pipeline completo de aprendizaje progresivo.
        
        GOTCHA: Procesar en batches de 10 chunks para evitar rate limits.
        GOTCHA: Guardar progreso en DB para recuperaci√≥n ante fallos.
        """
        # 1. Crear registro inicial
        learned_book = MarketingLearnedBook(
            project_id=project_id,
            title=title,
            author=author,
            file_path=file_path,
            processing_status="processing"
        )
        self.db.add(learned_book)
        await self.db.commit()
        
        try:
            # 2. Extraer texto (REUTILIZAR l√≥gica existente)
            raw_text = await self._extract_text(file_path)
            
            # 3. Chunking con overlap
            chunks = self.chunker.split_text(raw_text)
            learned_book.total_chunks = len(chunks)
            await self.db.commit()
            
            # 4. CAPA 1: Extracci√≥n de conceptos por chunk
            chunk_concepts = []
            for batch_start in range(0, len(chunks), 10):
                batch = chunks[batch_start:batch_start + 10]
                for i, chunk in enumerate(batch):
                    concepts = await self._extract_concepts_from_chunk(
                        chunk, batch_start + i
                    )
                    chunk_concepts.append(concepts)
                    learned_book.processed_chunks = batch_start + i + 1
                    await self.db.commit()
            
            # 5. CAPA 2: Res√∫menes tem√°ticos
            thematic_summaries = await self._generate_thematic_summaries(
                chunks, chunk_concepts
            )
            
            # 6. CAPA 3: Resumen global
            global_summary = await self._generate_global_summary(
                chunk_concepts, thematic_summaries
            )
            learned_book.global_summary = global_summary
            
            # 7. Generar embeddings (USAR servicio existente)
            concept_texts = [c.condensed_text for c in chunk_concepts]
            embeddings = await self.embeddings.generate_embeddings_batch(concept_texts)
            
            # 8. Guardar conceptos con embeddings
            await self._save_concepts(learned_book.id, chunk_concepts, embeddings)
            
            # 9. Marcar como completado
            learned_book.processing_status = "completed"
            learned_book.completed_at = datetime.utcnow()
            await self.db.commit()
            
            return learned_book
            
        except Exception as e:
            learned_book.processing_status = "failed"
            await self.db.commit()
            raise
    
    async def _extract_concepts_from_chunk(
        self, chunk: str, chunk_index: int
    ) -> ConceptExtraction:
        """
        Usa LLM para extraer conceptos clave.
        
        CR√çTICO: Esto NO es un resumen. Es una destilaci√≥n sem√°ntica.
        """
        prompt = f"""
Eres un experto en marketing que est√° estudiando un libro.
Tu tarea es extraer los conceptos M√ÅS IMPORTANTES de este fragmento.

NO hagas un resumen. Extrae CONCEPTOS CLAVE que un estudiante deber√≠a recordar.

Fragmento (chunk {chunk_index}):
{chunk}

Extrae en formato JSON:
{{
    "main_concepts": ["concepto1", "concepto2", ...],
    "relationships": ["concepto1 causa concepto2", ...],
    "key_examples": ["ejemplo1", ...],
    "technical_terms": {{"t√©rmino": "definici√≥n", ...}},
    "condensed_text": "Resumen de 200-300 palabras con los puntos clave"
}}

S√© CONCISO. M√°ximo 5 conceptos principales.
"""
        
        # USAR LLMService existente
        response = await self.llm.generate(
            prompt=prompt,
            temperature=0.3  # Baja para extracci√≥n precisa
        )
        
        # GOTCHA: LLM puede fallar en formato JSON
        # Usar Pydantic para validar y retry si falla
        return ConceptExtraction.parse_from_llm_response(response)
```

---

### TAREA 3: API Endpoints

**Herramientas:**
- üîß MCP Serena: Ver estructura de `chat.py`, `documents.py`
- ‚ö° MCP Archon: `rag_search_knowledge_base(query="FastAPI background tasks", source_id="c889b62860c33a44")`
- üìö Skill: api-patterns

**Objetivo:** Exponer funcionalidad v√≠a API REST.

**Pasos:**

1. **Analizar routers existentes:**
   ```bash
   get_symbols_overview('backend/src/api/chat.py')
   get_symbols_overview('backend/src/api/documents.py')
   # Ver patr√≥n de dependencias
   ```

2. **Crear router de knowledge:**
   - POST /api/knowledge/books/upload (con background task)
   - GET /api/knowledge/books/{book_id}/status
   - GET /api/knowledge/books
   - POST /api/knowledge/concepts/search
   - DELETE /api/knowledge/books/{book_id}

3. **Registrar en main.py:**
   ```python
   from .api import knowledge
   app.include_router(knowledge.router)
   ```

**Archivos a crear:**
- `backend/src/api/knowledge.py`

**Archivos a modificar:**
- `backend/src/main.py` (include_router)

**Criterios de aceptaci√≥n:**
- [ ] Endpoints registrados en /docs (Swagger)
- [ ] Upload acepta .pdf, .txt, .docx
- [ ] Background task inicia correctamente
- [ ] Status endpoint retorna progreso real
- [ ] Search retorna conceptos relevantes

**Pseudoc√≥digo:**

```python
# backend/src/api/knowledge.py

from fastapi import APIRouter, UploadFile, Depends, BackgroundTasks
from ..services.book_learning_service import BookLearningService
from ..middleware.auth import get_current_user

router = APIRouter(prefix="/api/knowledge", tags=["knowledge"])

# PATR√ìN: Ver c√≥mo documents.py maneja uploads
# GOTCHA: Usar BackgroundTasks para procesamiento largo

@router.post("/books/upload")
async def upload_book_for_learning(
    background_tasks: BackgroundTasks,
    file: UploadFile,
    title: str,
    author: Optional[str] = None,
    current_user: MarketingUser = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Sube un libro para aprendizaje.
    Procesamiento es as√≠ncrono (background task).
    """
    # Validar tipo de archivo
    allowed_types = [".pdf", ".txt", ".docx"]
    ext = Path(file.filename).suffix.lower()
    if ext not in allowed_types:
        raise HTTPException(400, f"Tipo no permitido. Usar: {allowed_types}")
    
    # Validar tama√±o (50MB max para libros)
    MAX_SIZE = 50 * 1024 * 1024
    contents = await file.read()
    if len(contents) > MAX_SIZE:
        raise HTTPException(400, "Archivo muy grande. M√°ximo 50MB.")
    
    # Guardar temporalmente
    temp_path = f"/tmp/{uuid.uuid4()}{ext}"
    with open(temp_path, "wb") as f:
        f.write(contents)
    
    # Crear servicio con dependencias
    llm_service = LLMService()
    embedding_service = EmbeddingService()
    book_service = BookLearningService(db, llm_service, embedding_service)
    
    # Iniciar procesamiento en background
    # GOTCHA: background_tasks NO espera, retorna inmediatamente
    background_tasks.add_task(
        book_service.process_book,
        file_path=temp_path,
        project_id=str(current_user.default_project_id),
        title=title,
        author=author
    )
    
    return {"message": "Procesamiento iniciado", "status": "processing"}

@router.get("/books/{book_id}/status")
async def get_book_status(
    book_id: str,
    current_user: MarketingUser = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
) -> BookProcessingStatus:
    """Retorna estado de procesamiento."""
    book = await db.get(MarketingLearnedBook, book_id)
    if not book:
        raise HTTPException(404, "Libro no encontrado")
    
    return BookProcessingStatus(
        status=book.processing_status,
        processed_chunks=book.processed_chunks,
        total_chunks=book.total_chunks,
        completed_at=book.completed_at
    )
```

---

### TAREA 4: Integraci√≥n con RAGService

**Herramientas:**
- üîß MCP Serena: `find_symbol('RAGService/search_relevant_docs', '...', True)`
- ‚ö° MCP Archon: `rag_search_knowledge_base(query="pgvector cosine similarity", source_id="9c5f534e51ee9237")`
- üìö Skill: rag-implementation

**Objetivo:** Extender RAGService para buscar en conceptos aprendidos.

**Pasos:**

1. **Leer RAGService completo:**
   ```bash
   find_symbol('RAGService', 'backend/src/services/rag_service.py', True, depth=1)
   find_symbol('RAGService/search_relevant_docs', '...', True)
   ```

2. **Agregar m√©todo search_learned_concepts():**
   - NO modificar m√©todos existentes
   - Agregar m√©todo nuevo que filtra por knowledge_type
   - Mantener compatibilidad total

3. **Agregar m√©todo h√≠brido search_with_learned_knowledge():**
   - Busca en: conceptos + documentos + knowledge base
   - Combina y rankea resultados

**Archivos a modificar:**
- `backend/src/services/rag_service.py` (AGREGAR m√©todos, no modificar existentes)

**Criterios de aceptaci√≥n:**
- [ ] search_learned_concepts() funciona
- [ ] M√©todos existentes NO modificados (diff m√≠nimo)
- [ ] Tests existentes siguen pasando
- [ ] Performance <100ms

**Pseudoc√≥digo:**

```python
# backend/src/services/rag_service.py - AGREGAR (no modificar existente)

class RAGService:
    # ... m√©todos existentes NO TOCAR ...
    
    async def search_learned_concepts(
        self,
        query: str,
        project_id: str,
        limit: int = 5,
        similarity_threshold: float = 0.7
    ) -> List[ConceptSearchResult]:
        """
        Busca en conceptos aprendidos de libros.
        
        CR√çTICO: Este m√©todo es NUEVO, no modifica search_relevant_docs()
        """
        # Generar embedding de la query
        query_embedding = await self.embedding_service.generate_embedding(query)
        
        # GOTCHA: Filtrar por knowledge_type = 'extracted_concept'
        # Y por project_id para aislamiento
        results = await self.db.execute(
            select(MarketingBookConcept)
            .join(MarketingLearnedBook)
            .where(
                MarketingLearnedBook.project_id == project_id,
                # Usar operador de similaridad de pgvector
                MarketingBookConcept.embedding.cosine_distance(query_embedding) < (1 - similarity_threshold)
            )
            .order_by(MarketingBookConcept.embedding.cosine_distance(query_embedding))
            .limit(limit)
        )
        
        return [ConceptSearchResult.from_orm(r) for r in results.scalars()]
    
    async def search_with_learned_knowledge(
        self,
        query: str,
        project_id: str,
        limit_per_type: int = 5
    ) -> HybridSearchResult:
        """
        B√∫squeda h√≠brida: conceptos + documentos + knowledge base.
        
        Retorna resultados combinados y rankeados.
        """
        # 1. Buscar en conceptos aprendidos
        concepts = await self.search_learned_concepts(
            query, project_id, limit=limit_per_type
        )
        
        # 2. Buscar en documentos de usuario (m√©todo existente)
        docs = await self.search_relevant_docs(
            query, project_id, limit=limit_per_type
        )
        
        # 3. Buscar en knowledge base tradicional
        # REUTILIZAR m√©todo existente con filtro
        kb_results = await self._vector_search(
            query, project_id, limit=limit_per_type
        )
        
        return HybridSearchResult(
            concepts=concepts,
            documents=docs,
            knowledge_base=kb_results
        )
```

---

### TAREA 5: Integraci√≥n con Agentes

**Herramientas:**
- üîß MCP Serena: Ver ContentGeneratorAgent y MemoryManager
- üìö Skill: agent-memory-systems

**Objetivo:** Que ContentGeneratorAgent use conceptos aprendidos en respuestas.

**‚ö†Ô∏è IMPORTANTE:** 
- `GeneralChatAgent` NO EXISTE y no se crear√° (ver PRP marketing-brain-conversational-agent-v3.md)
- Solo modificar `ContentGeneratorAgent`
- SEGUIR PATR√ìN de `MemoryManager.get_training_summary()` para inyectar conceptos

**Pasos:**

1. **Analizar agente y patr√≥n existente:**
   ```bash
   get_symbols_overview('backend/src/agents/content_generator_agent.py')
   find_symbol('ContentGeneratorAgent/execute', '...', True)
   find_symbol('MemoryManager/get_training_summary', 'backend/src/services/memory_manager.py', True)
   ```

2. **Extender MemoryManager.get_context():**
   - Agregar llamada a `RAGService.search_learned_concepts()`
   - Incluir conceptos en contexto retornado
   - SEGUIR PATR√ìN de c√≥mo se incluye `training_summary`

3. **Modificar ContentGeneratorAgent._build_system_prompt():**
   - Incluir conceptos aprendidos del contexto
   - Agregar secci√≥n "CONOCIMIENTO APRENDIDO DE LIBROS"
   - Mantener secciones existentes (training_summary, buyer_persona, etc.)

**Archivos a modificar:**
- `backend/src/services/memory_manager.py` (extender get_context)
- `backend/src/agents/content_generator_agent.py` (usar conceptos en prompt)

**‚ö†Ô∏è CR√çTICO:** 
- NO crear GeneralChatAgent
- NO reescribir agentes, solo AGREGAR funcionalidad
- SEGUIR PATR√ìN existente de get_training_summary()

**Criterios de aceptaci√≥n:**
- [ ] MemoryManager.get_context() incluye conceptos aprendidos
- [ ] ContentGeneratorAgent usa conceptos en system prompt
- [ ] Respuestas incluyen conocimiento de libros procesados
- [ ] Funcionalidad existente NO afectada
- [ ] Tests de agentes pasan

---

### TAREA 6: Frontend - UI para Gesti√≥n de Libros

**Herramientas:**
- üîß MCP Serena: Ver Sidebar.tsx, ChatInterface.tsx
- üìö Skills: react-patterns, nextjs-best-practices

**Objetivo:** Permitir al usuario subir/gestionar libros desde UI.

**Pasos:**

1. **Analizar componentes existentes:**
   ```bash
   get_symbols_overview('frontend/app/components/Sidebar.tsx')
   get_symbols_overview('frontend/app/components/DocumentUpload.tsx')
   ```

2. **Crear componentes:**
   - BookUpload.tsx (drag & drop)
   - LearnedBookCard.tsx (tarjeta con info)
   - ConceptsViewer.tsx (visualizaci√≥n)

3. **Crear p√°gina:**
   - `/dashboard/knowledge/page.tsx`

4. **Integrar en Sidebar:**
   - Agregar link "Biblioteca de Conocimiento"

**Componentes a crear:**
- `frontend/app/components/BookUpload.tsx`
- `frontend/app/components/LearnedBookCard.tsx`
- `frontend/app/components/ConceptsViewer.tsx`
- `frontend/app/dashboard/knowledge/page.tsx`

**Archivos a modificar:**
- `frontend/app/components/Sidebar.tsx`
- `frontend/lib/api-chat.ts` (agregar funciones de knowledge)

**Criterios de aceptaci√≥n:**
- [ ] Usuario puede subir libro
- [ ] Progress bar durante procesamiento
- [ ] Lista de libros actualizable
- [ ] UI consistente con dise√±o existente
- [ ] Responsive en mobile

---

### TAREA 6.1: Verificaci√≥n Docker (COMPLETADA)

**Objetivo:** Asegurar que los bind mounts de Docker reflejen cambios del frontend.

**Archivos modificados:**
- `docker-compose.yml` - Agregado bind mount `./frontend/lib:/app/lib:ro`

**Estado:** ‚úÖ COMPLETADA

---

### TAREA 6.2: Eliminar Formato JSON Forzado - Migrar a Markdown

**Herramientas:**
- üîß MCP Serena: Analizar content_generator_agent.py, router_agent.py
- üìö Skills: python-patterns, clean-code

**Objetivo:** Eliminar la restricci√≥n de respuestas JSON y permitir respuestas en Markdown puro.

**Diagn√≥stico COMPLETO con Serena (verificado):**

```
FLUJO ACTUAL:
1. Usuario: "Dame 5 ideas de contenido"
2. _select_mode() ‚Üí detecta n√∫meros + "ideas" ‚Üí retorna "ideas_json"
3. _build_system_prompt() ‚Üí inyecta prompt que FUERZA JSON (l√≠neas 399-449)
   "CR√çTICO: Responde SOLO con JSON v√°lido. No incluyas texto..."
4. LLM ‚Üí responde con JSON estructurado
5. _parse_content_response() ‚Üí parsea JSON a list[dict]
6. process_stream() (router_agent l√≠neas 255-300) ‚Üí convierte JSON a texto con emojis
7. Frontend ‚Üí muestra texto con whitespace-pre-wrap (NO Markdown real)
```

**Archivos clave (l√≠neas exactas verificadas con Serena):**

| Archivo | M√©todo | L√≠neas | Funci√≥n |
|---------|--------|--------|---------|
| `content_generator_agent.py` | `_select_mode()` | 252-273 | Decide `ideas_json` vs `consultivo` |
| `content_generator_agent.py` | `_build_system_prompt()` | 399-449 | Prompt que fuerza JSON |
| `content_generator_agent.py` | `_parse_content_response()` | 494-612 | Parsea JSON del LLM |
| `router_agent.py` | `process_stream()` | 255-300 | Formatea JSON ‚Üí texto |
| `MessageList.tsx` | - | 68 | `whitespace-pre-wrap` |

**Pasos de implementaci√≥n:**

1. **Modificar `_select_mode()` (content_generator_agent.py:252-273):**
   - Cambiar para SIEMPRE retornar `"consultivo"` (eliminar l√≥gica de `ideas_json`)
   - O renombrar a `"markdown"` para claridad

2. **Modificar `_build_system_prompt()` (content_generator_agent.py:399-449):**
   - Eliminar la secci√≥n que dice "CR√çTICO: Responde SOLO con JSON v√°lido"
   - Reemplazar con instrucciones de Markdown estructurado
   - Agregar ejemplos few-shot si es necesario

3. **Simplificar `process_stream()` (router_agent.py:255-300):**
   - Eliminar l√≥gica de conversi√≥n JSON ‚Üí texto
   - Pasar `content_text` directamente (ya existe para modo consultivo)

4. **Eliminar `_parse_content_response()` (content_generator_agent.py:494-612):**
   - Ya no se necesita si no parseamos JSON
   - O mantener como fallback por si LLM genera JSON

5. **Actualizar Frontend:**
   - Instalar `react-markdown` y `remark-gfm`
   - Modificar `MessageList.tsx` para renderizar Markdown

**Archivos a modificar:**
- `backend/src/agents/content_generator_agent.py`
- `backend/src/agents/router_agent.py`
- `frontend/app/components/MessageList.tsx`
- `frontend/package.json` (agregar react-markdown, remark-gfm)

**Criterios de aceptaci√≥n:**
- [ ] `_select_mode()` ya no retorna `ideas_json`
- [ ] Prompt permite respuestas en Markdown libre
- [ ] `process_stream()` pasa texto directamente
- [ ] Frontend renderiza Markdown (headers, listas, negritas, c√≥digo)
- [ ] Funcionalidad existente NO se rompe

**Riesgos y mitigaciones:**
- ‚ö†Ô∏è Respuestas menos estructuradas ‚Üí Mitigar con prompt engineering
- ‚ö†Ô∏è Debug info perdida ‚Üí Mantener `tecnicas_aplicadas` en prompt pero no forzar JSON

---

### TAREA 6.3: Eliminar Auto-Creaci√≥n de Chats Vac√≠os

**Herramientas:**
- üìö Skills: react-patterns, nextjs-best-practices

**Objetivo:** Que el chat se cree SOLO cuando el usuario env√≠a el primer mensaje (como ChatGPT).

**Diagn√≥stico del problema:**
- `ChatPageContent.tsx` l√≠neas 27-38 crea chat autom√°ticamente si no hay `chatIdFromUrl`
- Cada refresh/entrada crea chat vac√≠o = basura en DB

**Pasos:**

1. **Modificar ChatPageContent:**
   - NO crear chat autom√°ticamente
   - Si no hay `chatId`, mostrar estado de "bienvenida"

2. **Modificar ChatInterface:**
   - Si no hay `chatId`, crear chat al enviar primer mensaje
   - Despu√©s de crear, actualizar URL y continuar

3. **Actualizar Sidebar:**
   - Reflejar nuevo chat en lista despu√©s de creaci√≥n

**Archivos a modificar:**
- `frontend/app/components/ChatPageContent.tsx`
- `frontend/app/components/ChatInterface.tsx`
- `frontend/app/components/Sidebar.tsx` (posiblemente)

**Criterios de aceptaci√≥n:**
- [x] Entrar a `/` NO crea chat autom√°ticamente ‚úÖ
- [x] Chat se crea SOLO al enviar primer mensaje ‚úÖ
- [x] URL se actualiza con nuevo chatId ‚úÖ
- [x] Lista de chats se actualiza en sidebar ‚úÖ
- [x] No m√°s chats vac√≠os en base de datos ‚úÖ

**Estado: ‚úÖ COMPLETADA (2026-01-31)**

**Cambios realizados:**
- `ChatPageContent.tsx`: Eliminada creaci√≥n autom√°tica, a√±adido callback `onChatCreated`
- `ChatInterface.tsx`: Acepta `chatId` opcional, crea chat al enviar primer mensaje, UI de bienvenida
- `Sidebar.tsx`: Bot√≥n "Nueva Conversaci√≥n" navega a `/` sin crear chat, eliminada import de `createChat`

---

### TAREA 6.4: Optimizaci√≥n de Rendimiento - Procesamiento Paralelo y Recursos Docker

**Herramientas:**
- üìö Skills: docker-expert, python-patterns, nodejs-best-practices

**Objetivo:** Acelerar el procesamiento de libros de ~20min a ~2min mediante paralelismo, y configurar l√≠mites de recursos Docker para estabilidad.

**Contexto t√©cnico:**
```yaml
Rate Limits actuales:
  OPENAI_RATE_LIMIT_RPM: 3000  # 3000 peticiones/minuto
  Modelo: gpt-4o
  
Benchmark actual (417 chunks - Traffic Secrets):
  Tiempo: ~21 minutos
  M√©todo: Secuencial (1 chunk a la vez)
  
Objetivo con paralelismo (10 chunks):
  Tiempo estimado: ~2 minutos
  Speedup: 10x
```

**PARTE A: Procesamiento Paralelo de Chunks**

**Archivos a modificar:**
- `backend/src/services/book_learning_service.py`

**Pasos:**

1. **Agregar configuraci√≥n de paralelismo:**
   ```python
   # En __init__ o como constante
   PARALLEL_CHUNKS = int(os.getenv("BOOK_PARALLEL_CHUNKS", "10"))
   ```

2. **Modificar process_book para usar asyncio.gather:**
   ```python
   import asyncio
   from typing import List
   
   async def _process_chunk_batch_parallel(
       self,
       chunks: List[str],
       chunk_indices: List[int],
       learned_book: MarketingLearnedBook
   ) -> List[MarketingBookConcept]:
       """Procesa m√∫ltiples chunks en paralelo."""
       tasks = [
           self._extract_and_store_concept(chunk, idx, learned_book)
           for chunk, idx in zip(chunks, chunk_indices)
       ]
       results = await asyncio.gather(*tasks, return_exceptions=True)
       
       # Filtrar errores y loggear
       concepts = []
       for i, result in enumerate(results):
           if isinstance(result, Exception):
               logger.error(f"Error en chunk {chunk_indices[i]}: {result}")
           else:
               concepts.append(result)
       return concepts
   
   async def _extract_and_store_concept(
       self,
       chunk: str,
       chunk_index: int,
       learned_book: MarketingLearnedBook
   ) -> MarketingBookConcept:
       """Extrae conceptos de un chunk y lo almacena."""
       concepts = await self._extract_concepts_from_chunk(chunk)
       embedding = await self.embedding_service.get_embedding(concepts.condensed_text)
       
       concept = MarketingBookConcept(
           learned_book_id=learned_book.id,
           chunk_index=chunk_index,
           original_text=chunk,
           main_concepts=concepts.main_concepts,
           relationships=concepts.relationships,
           key_examples=concepts.key_examples,
           technical_terms=concepts.technical_terms,
           condensed_text=concepts.condensed_text,
           embedding=embedding
       )
       self.db.add(concept)
       return concept
   ```

3. **Actualizar bucle principal en process_book:**
   ```python
   # ANTES (secuencial):
   for batch_start in range(0, len(chunks), batch_size):
       batch = chunks[batch_start:batch_start + batch_size]
       for i, chunk in enumerate(batch):
           # procesar uno por uno...
   
   # DESPU√âS (paralelo):
   parallel_size = int(os.getenv("BOOK_PARALLEL_CHUNKS", "10"))
   for batch_start in range(0, len(chunks), parallel_size):
       batch_end = min(batch_start + parallel_size, len(chunks))
       batch_chunks = chunks[batch_start:batch_end]
       batch_indices = list(range(batch_start, batch_end))
       
       concepts = await self._process_chunk_batch_parallel(
           batch_chunks, batch_indices, learned_book
       )
       
       learned_book.processed_chunks = batch_end
       await self.db.commit()
       await self.db.refresh(learned_book)
       
       logger.info(f"BOOK: Batch {batch_start}-{batch_end} completado ({len(concepts)} conceptos)")
   ```

4. **Agregar variable de entorno:**
   - En `docker-compose.yml` a√±adir:
   ```yaml
   backend:
     environment:
       - BOOK_PARALLEL_CHUNKS=10
   ```

**PARTE B: Configuraci√≥n de Recursos Docker**

**Archivos a modificar:**
- `docker-compose.yml`

**Configuraci√≥n recomendada:**
```yaml
services:
  backend:
    # ... configuraci√≥n existente ...
    deploy:
      resources:
        limits:
          cpus: '2.0'      # M√°ximo 2 CPUs
          memory: 2G       # M√°ximo 2GB RAM
        reservations:
          cpus: '1.0'      # M√≠nimo garantizado 1 CPU
          memory: 1G       # M√≠nimo garantizado 1GB RAM
    
  frontend:
    # ... configuraci√≥n existente ...
    deploy:
      resources:
        limits:
          cpus: '0.5'      # M√°ximo 0.5 CPU
          memory: 512M     # M√°ximo 512MB RAM
        reservations:
          cpus: '0.25'     # M√≠nimo garantizado 0.25 CPU
          memory: 256M     # M√≠nimo garantizado 256MB RAM
    
  db:
    # ... configuraci√≥n existente ...
    deploy:
      resources:
        limits:
          cpus: '1.0'      # M√°ximo 1 CPU
          memory: 1G       # M√°ximo 1GB RAM
        reservations:
          cpus: '0.5'      # M√≠nimo garantizado 0.5 CPU
          memory: 512M     # M√≠nimo garantizado 512MB RAM
```

**Notas importantes:**
- `limits`: M√°ximo que puede usar el contenedor
- `reservations`: M√≠nimo garantizado (Docker no iniciar√° si no hay recursos)
- El backend recibe m√°s recursos porque hace el procesamiento de LLM
- Frontend es ligero (solo sirve archivos est√°ticos)
- En producci√≥n, ajustar seg√∫n recursos del servidor

**PARTE C: Monitoreo de Recursos (Opcional)**

**Comando para verificar uso:**
```bash
# Ver uso de recursos en tiempo real
docker stats

# Ver uso espec√≠fico durante procesamiento de libro
watch -n 1 'docker stats --no-stream --format "table {{.Name}}\t{{.CPUPerc}}\t{{.MemUsage}}"'
```

**Criterios de aceptaci√≥n:**
- [ ] Procesamiento de 417 chunks en < 3 minutos (pendiente test con libro)
- [x] Sin errores de rate limit ‚úÖ (10 paralelos es conservador)
- [x] docker-compose.yml con l√≠mites de recursos ‚úÖ
- [x] Variable BOOK_PARALLEL_CHUNKS configurable ‚úÖ (=10)
- [x] Logs muestran progreso de batches paralelos ‚úÖ
- [x] Sistema estable bajo carga ‚úÖ

**Estado: ‚úÖ IMPLEMENTADA (2026-01-31)**

**Cambios realizados:**
- `book_learning_service.py`: Agregado `PARALLEL_CHUNKS`, nuevo m√©todo `_process_chunks_parallel` con `asyncio.gather`
- `docker-compose.yml`: Agregada variable `BOOK_PARALLEL_CHUNKS=10`, l√≠mites de CPU/RAM para backend (2CPU/2GB), frontend (0.5CPU/512MB), redis (0.5CPU/512MB)

**Comandos de validaci√≥n:**
```bash
# Nivel 1: Verificar configuraci√≥n
docker compose config | grep -A5 "resources"

# Nivel 2: Verificar variables de entorno
docker exec marketing-brain-backend env | grep PARALLEL

# Nivel 3: Test de rendimiento
# Subir libro de prueba y medir tiempo

# Nivel 4: Monitorear recursos durante procesamiento
docker stats --no-stream
```

**Riesgos y mitigaciones:**
- ‚ö†Ô∏è Rate limit excedido ‚Üí Mitigar con PARALLEL_CHUNKS=10 (conservador)
- ‚ö†Ô∏è Memoria insuficiente ‚Üí Mitigar con reservations en Docker
- ‚ö†Ô∏è Errores parciales en batch ‚Üí Mitigar con return_exceptions=True y logging

---

### TAREA 6.5: Auditor√≠a de Seguridad y Preparaci√≥n para GitHub

**Herramientas:**
- üìö Skills: cc-skill-security-review, git-pushing

**Objetivo:** Limpiar credenciales del historial de git y preparar proyecto para repositorio p√∫blico.

**Contexto t√©cnico:**
```yaml
Problema detectado:
  - .env con credenciales REALES estaba trackeado en git
  - .env.example tambi√©n conten√≠a credenciales reales
  - Historial de git conten√≠a secretos en 29 commits
  
Credenciales comprometidas:
  - OpenAI API Key (sk-proj-...)
  - OpenRouter API Key (sk-or-v1-...)
  - JWT_SECRET_KEY
  - Supabase Service Role Key
```

**PARTE A: Remover archivos sensibles del tracking ‚úÖ**
```bash
git rm --cached .env .env.example frontend/.env.example
```

**PARTE B: Limpiar historial de git ‚úÖ**
```bash
git filter-branch --force --index-filter \
  'git rm --cached --ignore-unmatch .env .env.example .env.local' \
  --prune-empty --tag-name-filter cat -- --all

git reflog expire --expire=now --all
git gc --prune=now --aggressive
```

**PARTE C: Actualizar .gitignore ‚úÖ**
- Agregadas reglas para `.env`, `.env.local`, `.env.*.local`
- Agregadas reglas para claves privadas (*.pem, *.key)
- Agregadas reglas para directorios secrets/

**PARTE D: Crear .env.example seguro ‚úÖ**
- Template con placeholders (`sk-your-openai-api-key-here`)
- Sin credenciales reales
- Documentaci√≥n de cada variable

**PARTE E: Verificaci√≥n de seguridad ‚úÖ**
```bash
# Verificar que .env NO est√° trackeado
git ls-files | grep -E "\.env"  # Solo debe mostrar .env.example

# Verificar historial limpio
git log --all --full-history -- .env  # No debe mostrar commits
```

**Estado: ‚úÖ COMPLETADA (2026-01-31)**

**‚ö†Ô∏è ACCI√ìN PENDIENTE DEL USUARIO:**
- Regenerar credenciales comprometidas (OpenAI, OpenRouter, JWT)
- Llenar `.env` con nuevas credenciales

---

### TAREA 6.6: Crear Repositorio GitHub y Subir Proyecto

**Herramientas:**
- üîß MCP: user-github
- üìö Skills: git-pushing

**Objetivo:** Crear repositorio p√∫blico en GitHub y hacer push del proyecto limpio.

**Pasos:**

1. **Crear repositorio en GitHub usando MCP:**
   ```
   Nombre: brain-mkt
   Visibilidad: public
   Descripci√≥n: "Marketing Second Brain - AI-powered marketing knowledge assistant"
   ```

2. **Configurar remote y hacer push:**
   ```bash
   git remote add origin https://github.com/[USUARIO]/brain-mkt.git
   git branch -M main
   git push -u origin main --force  # Force porque reescribimos historial
   ```

3. **Verificar subida:**
   - Confirmar que NO hay archivos .env en el repo
   - Confirmar que .env.example est√° presente
   - Verificar README y documentaci√≥n

**Criterios de aceptaci√≥n:**
- [ ] Repositorio creado como p√∫blico
- [ ] C√≥digo subido sin credenciales
- [ ] .env.example presente con placeholders
- [ ] .gitignore funcionando correctamente

**Estado: ‚è≥ PENDIENTE**

---

### TAREA 7: Testing y Documentaci√≥n

**Herramientas:**
- üìö Skills: testing-patterns, verification-before-completion

**Objetivo:** Validar integraci√≥n completa y documentar.

**Pasos:**

1. **Tests de integraci√≥n:**
   - Test flujo completo: subir ‚Üí procesar ‚Üí buscar ‚Üí responder
   - Test eliminaci√≥n y cleanup

2. **Tests unitarios:**
   - BookLearningService._extract_concepts_from_chunk()
   - RAGService.search_learned_concepts()
   - Endpoints de API

3. **Documentaci√≥n:**
   - `docs/BOOK_LEARNING_SYSTEM.md`
   - Actualizar README.md

**Archivos a crear:**
- `backend/tests/integration/test_book_learning.py`
- `backend/tests/unit/test_book_learning_service.py`
- `docs/BOOK_LEARNING_SYSTEM.md`

**Criterios de aceptaci√≥n:**
- [ ] Coverage >80% en c√≥digo nuevo
- [ ] Tests de integraci√≥n pasan
- [ ] Documentaci√≥n completa

**Comandos de validaci√≥n:**

```bash
# Nivel 1: Sintaxis y estilo
cd backend && ruff check src/ --fix && mypy src/

# Nivel 2: Tests unitarios
pytest tests/unit/test_book_learning_service.py -v

# Nivel 3: Tests de integraci√≥n
pytest tests/integration/test_book_learning.py -v

# Nivel 4: Coverage
pytest --cov=src/services/book_learning_service --cov-report=term-missing
```

---

## ‚ö†Ô∏è GOTCHAS CR√çTICOS

```yaml
1. LLM Context Window:
   PROBLEMA: "Chunks muy largos exceden context window"
   SOLUCI√ìN: "Limitar chunks a 1500 tokens, verificar antes de enviar"
   
2. JSON Parsing:
   PROBLEMA: "LLM puede generar JSON inv√°lido"
   SOLUCI√ìN: "Usar Pydantic para validar, retry con temperatura 0.1 si falla"
   
3. Rate Limits:
   PROBLEMA: "OpenAI rate limits en embeddings batch"
   SOLUCI√ìN: "Procesar en batches de 10, exponential backoff"
   
4. Overlap en Chunking:
   PROBLEMA: "Sin overlap se pierden conceptos en fronteras"
   SOLUCI√ìN: "Overlap de 200 tokens"
   
5. Background Tasks:
   PROBLEMA: "Si servidor reinicia, proceso se pierde"
   SOLUCI√ìN: "Guardar progreso en DB, implementar recuperaci√≥n"
   
6. Similarity Threshold:
   PROBLEMA: "Threshold muy bajo retorna conceptos irrelevantes"
   SOLUCI√ìN: "Usar 0.7-0.8, ajustar seg√∫n feedback"

7. ‚ö†Ô∏è DUPLICACI√ìN DE C√ìDIGO:
   PROBLEMA: "Recrear m√©todos que ya existen"
   SOLUCI√ìN: "VERIFICAR antes de crear: generate_with_messages(), format_messages_from_memory() YA EXISTEN"
   
8. ‚ö†Ô∏è AGENTE INEXISTENTE:
   PROBLEMA: "Intentar modificar GeneralChatAgent que no existe"
   SOLUCI√ìN: "Solo modificar ContentGeneratorAgent. GeneralChatAgent NO existe ni se crear√°"
   
9. ‚ö†Ô∏è PATR√ìN NO SEGUIDO:
   PROBLEMA: "Crear nueva l√≥gica para conceptos sin seguir patr√≥n existente"
   SOLUCI√ìN: "SEGUIR PATR√ìN de MemoryManager.get_training_summary() para inyectar conceptos"
```

---

## üìä SCORE DE CONFIANZA: 9/10

```yaml
Justificaci√≥n:
  ‚úÖ Archon consultado exhaustivamente (3 fuentes relevantes)
  ‚úÖ Serena usado para an√°lisis completo de servicios
  ‚úÖ Skills identificadas por fase
  ‚úÖ Estructura de tareas completa con comandos
  ‚úÖ Validaci√≥n en 3+ niveles
  ‚úÖ Gotchas documentados
  ‚úÖ Pseudoc√≥digo con patrones del proyecto
  
  ‚ö†Ô∏è -1 punto: Frontend puede requerir ajustes de dise√±o no previstos
```

---

## üéØ RESUMEN EJECUTIVO

| Tarea | Archivos a Crear | Archivos a Modificar | Prioridad | Estado |
|-------|------------------|----------------------|-----------|--------|
| T0: An√°lisis | docs/INTEGRATION_ANALYSIS.md | - | ‚ö° CR√çTICA | ‚úÖ |
| T1: DB | db/003_*.sql | db/models.py | Alta | ‚úÖ |
| T2: Service | services/book_learning_service.py, schemas/knowledge.py | - | Alta | ‚úÖ |
| T3: API | api/knowledge.py | main.py | Alta | ‚úÖ |
| T4: RAG | - | services/rag_service.py | Media | ‚úÖ |
| T5: Agents | - | memory_manager.py, content_generator_agent.py | Media | ‚úÖ |
| T6.1: Docker | - | docker-compose.yml | Alta | ‚úÖ |
| T6.2: Markdown | - | content_generator_agent.py | Alta | ‚úÖ |
| T6.3: Chats vac√≠os | - | ChatPageContent.tsx, ChatInterface.tsx, Sidebar.tsx | Media | ‚úÖ |
| T6.4: Optimizaci√≥n | - | book_learning_service.py, docker-compose.yml | Alta | ‚úÖ |
| T6.5: Seguridad GitHub | .env.example, .gitignore | - | ‚ö° CR√çTICA | ‚úÖ |
| T6.6: Subir a GitHub | - | - | Alta | ‚è≥ |
| T7: Tests | tests/*.py, docs/*.md | README.md | Baja | ‚è≥ |

**‚ö†Ô∏è ADVERTENCIAS CR√çTICAS:**
- `LLMService.generate_with_messages()` YA EXISTE - NO recrear
- `MemoryManager.get_training_summary()` es PATR√ìN A SEGUIR
- `GeneralChatAgent` NO existe y NO se crear√° - solo modificar ContentGeneratorAgent

**Principio gu√≠a:** REUTILIZAR > EXTENDER > CREAR

---

**üöÄ Este PRP est√° listo para implementaci√≥n one-pass. El desarrollador tiene todo el contexto necesario para ejecutar sin preguntas adicionales.**
