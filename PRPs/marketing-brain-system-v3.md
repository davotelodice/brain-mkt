name: "Marketing Second Brain System - PRP v3"
version: "3.0-ES"
descripcion: |
  Sistema web full-stack de segundo cerebro para estrategia de marketing digital.
  Incluye agente IA con memoria persistente (short-term, long-term, semantic),
  generaci√≥n de buyer personas, simulaci√≥n de comportamiento, customer journey,
  y creaci√≥n de contenido on-demand basado en entrenamiento con YouTubers + libros.

---

## üéØ Objetivo

Construir un sistema web completo ("Marketing Second Brain") que funcione como asistente IA para estrategia de marketing digital, con las siguientes capacidades principales:

1. **An√°lisis Profundo de Buyer Persona**: Cuestionario inicial + plantilla completa de 35+ preguntas
2. **Simulaci√≥n de Comportamiento**: Buyer persona actuando en foro de internet (quejas + soluciones)
3. **Customer Journey Detallado**: 3 fases de conciencia con 20 preguntas cada una
4. **Ingesta de Documentos**: Usuario sube archivos (.txt, .pdf, .docx) con info de su negocio
5. **Generaci√≥n de Contenido On-Demand**: Posts, videos, scripts SOLO cuando usuario lo solicita
6. **Sistema de Memoria Triple**: Short-term (ventana), Long-term (DB), Semantic (vector store)
7. **Entrenamiento Contextual**: RAG con transcripciones de YouTubers + libros de marketing

**Estado Final Deseado:**
- Frontend (Next.js 14) con interfaz de chat responsive y moderna
- Backend (FastAPI + Python) con agente IA usando LangChain/LangGraph
- Base de datos (Supabase + pgvector) con aislamiento por `project_id`
- Sistema de autenticaci√≥n manual (sin Supabase Auth por restricci√≥n)
- Docker Compose para desarrollo local
- Documentos procesados y embeddings generados
- Agente que ENTREGA an√°lisis completo y ESPERA peticiones del usuario
- Tests con >80% coverage

---

## üí° Por Qu√©

### Valor de Negocio
- **Diferenciaci√≥n**: No es un chatbot gen√©rico - est√° entrenado espec√≠ficamente en creaci√≥n de contenido
- **Contexto Persistente**: Recuerda buyer persona, puntos de dolor, customer journey, y documentos del usuario
- **Salida Accionable**: Genera contenido listo para usar, no solo teor√≠a
- **ROI Claro**: Ahorra horas de investigaci√≥n y redacci√≥n de contenido

### Impacto en el Usuario
- Estrategia de marketing personalizada en minutos
- Ideas de contenido basadas en comportamiento real del p√∫blico objetivo
- Capacidad de incorporar info del negocio mediante documentos
- Generaci√≥n de contenido bajo demanda (no autom√°tica, controlada por usuario)

### Integraci√≥n
- Primera versi√≥n standalone, futuro: integraci√≥n con redes sociales
- Fundamento para features adicionales: calendario de contenido, an√°lisis de competencia

### Problemas que Resuelve
- **Para marketers**: Falta de insights profundos sobre su audiencia
- **Para creadores de contenido**: No saber qu√© contenido crear para cada fase del customer journey
- **Para emprendedores**: Dificultad para crear estrategia de contenido consistente
- **Para consultores**: Necesidad de herramienta para acelerar an√°lisis de clientes

---

## üìã Qu√©

### Comportamiento Visible para el Usuario

#### FASE 1 - AN√ÅLISIS INICIAL (Autom√°tico):
1. Usuario se registra / inicia sesi√≥n
2. Usuario crea nuevo chat desde dashboard
3. Agente hace 4-5 preguntas iniciales sobre el negocio
4. **[NUEVO]** Usuario OPCIONALMENTE sube documentos (.txt, .pdf, .docx) con info del negocio
5. **[NUEVO]** Agente procesa documentos y extrae informaci√≥n relevante
6. Agente completa Buyer Persona autom√°ticamente (plantilla de 35+ preguntas)
7. Agente simula persona en foro (quejas + soluciones propuestas)
8. Agente extrae 10 puntos de dolor principales
9. Agente genera Customer Journey (3 fases √ó 20 preguntas cada una)
10. Agente ENTREGA documento completo al usuario (formato markdown estructurado)
11. Agente GUARDA todo en memoria (3 tipos: short-term, long-term, semantic + docs)
12. ‚è∏Ô∏è **AGENTE ESPERA PETICIONES DEL USUARIO** (no genera contenido autom√°ticamente)

> ‚úÖ **Estado actual (enero 2026, ya implementado / verificado):**
> - **Tarea 9 (Customer Journey)**: ya existe en `backend/src/agents/buyer_persona_agent.py` (`BuyerPersonaAgent/_generate_customer_journey`) y se persiste en DB como `buyer_persona.customer_journey`. En UI se visualiza en `frontend/app/components/AnalysisPanel.tsx` (bloque ‚ÄúVer customer journey (JSON)‚Äù).  
>   - **Nota de coherencia**: hoy generamos \(por fase\) **10 `busquedas` + 10 `preguntas_cabeza`** (=20 items). Si la intenci√≥n del PRP era ‚Äú20 preguntas *solo* en `preguntas_cabeza`‚Äù, hay que aclararlo aqu√≠ para no ‚Äúromper‚Äù lo que ya funciona.
> - **Tarea 10 (Documento completo en Markdown)**: **no est√° implementada** como ‚Äúexport/entrega de documento completo‚Äù. Actualmente se muestran JSONs (buyer persona/foro/dolor/journey) en el panel de an√°lisis, pero no hay ‚Äúdocumento markdown estructurado‚Äù descargable/compartible.
> - **Tarea 11 (Guardar todo en memoria)**: ya est√° **parcialmente cumplida**:
>   - **Long-term**: buyer persona + foro + pain points + customer journey quedan en DB (modelo `BuyerPersona`).
>   - **Short-term**: conversaci√≥n reciente se mantiene (buffer) y se persiste en mensajes (DB).
>   - **Semantic + docs**: RAG funciona y se usa en generaci√≥n; documentos del usuario entran al retrieval; adem√°s existe ‚Äútraining summary‚Äù inyectado en prompt (con trazabilidad).
>   - **Pendiente recomendado (sin romper nada)**: cuando se implemente Tarea 10, guardar tambi√©n ese **Markdown final** como documento ‚Äúsnapshot‚Äù (para recuperaci√≥n sem√°ntica y auditor√≠a).

#### FASE 2 - GENERACI√ìN DE CONTENIDO (On-Demand):
1. Usuario pide contenido espec√≠fico (ej: "Dame 5 ideas de videos para fase de conciencia")
2. Agente consulta:
   - Buyer persona del chat actual (long-term)
   - Customer journey del chat actual (long-term)
   - Documentos subidos por el usuario (semantic search)
   - Knowledge base global (YouTubers + libros, semantic search)
   - Conversaci√≥n reciente (short-term)
3. Agente GENERA respuesta personalizada basada en contexto completo
4. Usuario recibe ideas accionables
5. Usuario puede pedir refinamientos o scripts completos
6. Ciclo contin√∫a seg√∫n peticiones

### Requisitos T√©cnicos

#### Frontend (Next.js 14 + TypeScript + TailwindCSS + shadcn/ui):
- App Router con Server Components
- P√°ginas de autenticaci√≥n: login, register, recover-password, reset-password/[token]
- Dashboard con sidebar de chats + √°rea principal de chat
- Interfaz de chat con streaming de respuestas (SSE)
- **[NUEVO]** Componente de subida de documentos (drag & drop o selector)
- **[NUEVO]** Lista de documentos subidos con estado de procesamiento
- Visualizaci√≥n especial para buyer persona, forum simulation, customer journey
- Estado global con Zustand (user, currentChat, chats[], documents[])
- Protecci√≥n de rutas con middleware
- Responsive (mobile + desktop)

#### Backend (FastAPI + Python 3.11+):
- Estructura modular: api/, agents/, db/, services/, schemas/, utils/
- Autenticaci√≥n manual con JWT (sin Supabase Auth)
- Recuperaci√≥n de contrase√±a con tokens √∫nicos (sin emails, link m√°gico)
- CRUD de chats y mensajes
- **[NUEVO]** Upload de documentos con validaci√≥n (.txt, .pdf, .docx, max 10MB)
- **[NUEVO]** Procesamiento de documentos (parsers para cada formato)
- **[NUEVO]** Chunking y embedding de documentos subidos
- Sistema de agentes con LangChain/LangGraph:
  - **Router Agent**: Orquestador principal
  - **Document Processor Agent**: Procesa archivos subidos (**NUEVO**)
  - **Buyer Persona Specialist**: Genera an√°lisis completo + busca en docs
  - **Forum Simulator**: Simula comportamiento en foro
  - **Pain Points Extractor**: Extrae 10 puntos principales
  - **Customer Journey Creator**: 3 fases √ó 20 preguntas
  - **Content Generator**: SOLO cuando usuario lo solicita
  - **Memory Manager**: Gestiona 3 tipos de memoria
- Streaming de respuestas con Server-Sent Events (SSE)
- Sistema de memoria triple (short/long/semantic)
- RAG con embeddings de OpenAI
- B√∫squeda vectorial con pgvector

#### Base de Datos (Supabase + PostgreSQL + pgvector):
- **8 tablas** con prefijo `marketing_`:
  1. `marketing_projects` (identificador de proyecto)
  2. `marketing_users` (autenticaci√≥n manual, NO Supabase Auth)
  3. `marketing_chats` (conversaciones con RLS)
  4. `marketing_messages` (historial con metadata)
  5. `marketing_buyer_personas` (an√°lisis completo + embeddings)
  6. `marketing_knowledge_base` (entrenamiento global + **docs de usuario**)
  7. **`marketing_user_documents`** (metadata de archivos subidos) **[NUEVO]**
  8. `marketing_password_reset_tokens` (recuperaci√≥n sin email)
- Extensi√≥n pgvector para embeddings (VECTOR(1536))
- Funci√≥n `marketing_match_documents` para b√∫squeda sem√°ntica
- Row Level Security (RLS) configurado
- √çndices ivfflat para b√∫squeda vectorial r√°pida
- **CR√çTICO**: Aislamiento total por `project_id` (sin mezcla entre proyectos)

#### Infraestructura (Docker + Docker Compose):
- 3 servicios: frontend (puerto 3000), backend (puerto 8000), redis (puerto 6379)
- Vol√∫menes para persistencia de datos
- Supabase remoto (VPS del usuario)
- Variables de entorno desde .env
- Scripts de deployment: docker-build.sh, docker-up.sh

### Criterios de √âxito

**Funcionalidad:**
- [ ] Usuario puede registrarse, login, recuperar contrase√±a
- [ ] Usuario puede crear/editar/eliminar chats
- [ ] Usuario puede subir documentos (.txt, .pdf, .docx) en un chat
- [ ] Usuario puede ver lista de documentos subidos con estado de procesamiento
- [ ] Usuario puede eliminar documentos subidos
- [ ] Agente responde 4-5 preguntas iniciales
- [ ] Agente procesa documentos subidos y extrae informaci√≥n
- [ ] Agente genera buyer persona completo (35+ preguntas) usando docs si existen
- [ ] Agente simula foro y extrae 10 puntos de dolor
- [ ] Agente genera customer journey (3 fases √ó 20 preguntas)
- [ ] Agente ENTREGA an√°lisis y ESPERA peticiones
- [ ] Sistema recuerda contexto (memoria funcional + documentos subidos)
- [ ] Usuario puede pedir ideas de contenido espec√≠ficas
- [ ] Agente genera contenido SOLO cuando se le pide
- [ ] Agente usa: buyer + CJ + docs subidos + entrenamiento global
- [ ] Streaming de respuestas funcional (<3s primera palabra)
- [ ] Datos aislados por `project_id` (sin mezcla entre proyectos)

**Calidad:**
- [ ] C√≥digo sin hardcoding de datos
- [ ] Tests con >80% coverage
- [ ] Sin errores de linting (ruff, mypy)
- [ ] Sin errores de tipos (TypeScript)
- [ ] Documentaci√≥n completa (README, API docs)
- [ ] Docker funcionando en local

**Performance:**
- [ ] B√∫squeda vectorial <100ms
- [ ] Streaming LLM <3s primera respuesta
- [ ] Frontend carga <2s (Lighthouse >90)
- [ ] API responde <200ms (endpoints simples)

**Seguridad:**
- [ ] JWT implementado correctamente
- [ ] Passwords hasheados con bcrypt
- [ ] RLS habilitado en Supabase
- [ ] API keys no expuestas
- [ ] Validaci√≥n de input en todos los endpoints
- [ ] Archivos subidos validados (tipo MIME, tama√±o, sanitizaci√≥n de nombres)

---

## üß∞ Skills del Proyecto a Utilizar

### üìù FASE DE PLANIFICACI√ìN

**Skill: planning-with-files**
- **Cu√°ndo**: Este proyecto es complejo (11 fases, ~30+ tareas)
- **Por qu√©**: Crear task_plan.md, findings.md y progress.md para seguimiento estructurado
- **Activaci√≥n**: Comenzar FASE 0 antes de cualquier implementaci√≥n

**Skill: brainstorming**
- **Cu√°ndo**: ANTES de implementar cada feature cr√≠tica (agentes, memoria, auth)
- **Por qu√©**: Explorar requisitos y decisiones arquitect√≥nicas
- **Cr√≠tico**: OBLIGATORIO antes de implementar agentes IA

**Skill: architecture**
- **Cu√°ndo**: Decidir estructura de agentes y flujo de datos entre ellos
- **Por qu√©**: Evaluar trade-offs (LangChain vs custom, tipo de memoria, embedding strategy)
- **Aplicaci√≥n**: FASE 4 (Agente IA con Memoria)

**Skill: agent-memory-systems**
- **Cu√°ndo**: Dise√±ar sistema de memoria del agente (3 tipos)
- **Por qu√©**: Entender patrones de short-term, long-term y semantic memory
- **Aplicaci√≥n**: FASE 4 (implementaci√≥n de memoria)

**Skill: rag-implementation**
- **Cu√°ndo**: Implementando b√∫squeda sem√°ntica y entrenamiento del agente
- **Por qu√©**: Chunking, embeddings, vector stores, retrieval optimization
- **Aplicaci√≥n**: FASE 5 (entrenamiento) y FASE 3.5 (documentos de usuario)

### üíª FASE DE DESARROLLO

**MCP: Serena** ‚ö° CR√çTICO - INSTALAR PRIMERO
- **Cu√°ndo**: TAREA 0 - Primera acci√≥n en todo proyecto
- **Por qu√©**: An√°lisis simb√≥lico de c√≥digo, ediciones quir√∫rgicas
- **Herramientas principales**:
  - `get_symbols_overview`: Ver estructura sin leer archivos completos
  - `find_symbol`: Buscar s√≠mbolos espec√≠ficos
  - `search_for_pattern`: B√∫squeda r√°pida cuando ubicaci√≥n desconocida
  - `replace_symbol_body`: Edici√≥n quir√∫rgica de funciones/clases
- **Aplicaci√≥n**: TODAS las fases de desarrollo

**Skill: clean-code**
- **Cu√°ndo**: Escribiendo o revisando c√≥digo (siempre)
- **Por qu√©**: C√≥digo conciso, directo, sin sobre-ingenier√≠a
- **Aplicaci√≥n**: Backend + Frontend

**Skill: python-patterns**
- **Cu√°ndo**: Desarrollando backend en Python
- **Por qu√©**: Async patterns, type hints, estructura modular
- **Aplicaci√≥n**: FASES 2-6 (backend)

**Skill: react-patterns**
- **Cu√°ndo**: Desarrollando componentes de UI
- **Por qu√©**: Hooks, composition, performance, TypeScript best practices
- **Aplicaci√≥n**: FASES 7-8 (frontend)

**Skill: nextjs-best-practices**
- **Cu√°ndo**: Trabajando con Next.js App Router
- **Por qu√©**: Server Components, data fetching, routing patterns
- **Aplicaci√≥n**: FASES 7-8 (frontend)

**Skill: autonomous-agents**
- **Cu√°ndo**: Implementando agentes con LangChain/LangGraph
- **Por qu√©**: Agent loops (ReAct), goal decomposition, self-correction
- **Aplicaci√≥n**: FASE 4 (n√∫cleo del sistema)

### üìö FASE DE DOCUMENTACI√ìN

**MCP: Archon** üéØ PRIORIDAD M√ÅXIMA
- **Cu√°ndo**: SIEMPRE - Consultar ANTES que URLs externas
- **Por qu√©**: Base de datos RAG con documentaci√≥n oficial verificada
- **Documentaci√≥n disponible**:
  - Python (varios libros + PEPs)
  - Pydantic v2 (docs completas + ejemplos) - source_id: `9d46e91458092424`
  - FastAPI (tutorial + API reference) - source_id: `c889b62860c33a44`
  - LangChain + LangGraph - source_id: `e74f94bb9dcb14aa`
  - Supabase (docs + pgvector) - source_id: `9c5f534e51ee9237`
  - Next.js 14 - source_id: `77b8a4a07d5230b5`
  - React - source_id: `a931698c21fb8f24`
  - TypeScript - source_id: `d7c76d077e634ab3`
  - shadcn/ui - source_id: `bf102fe8a697ed7c`
- **Herramientas**:
  - `rag_get_available_sources()`: Listar todas las fuentes
  - `rag_search_knowledge_base(query, source_id, match_count)`: Buscar docs
  - `rag_search_code_examples(query, source_id, match_count)`: Buscar ejemplos

**Skill: documentation-templates**
- **Cu√°ndo**: Crear README.md, API.md, DEPLOYMENT.md
- **Por qu√©**: Templates estructurados para documentaci√≥n clara
- **Aplicaci√≥n**: FASE 11 (documentaci√≥n final)

### üß™ FASE DE TESTING

**Skill: test-driven-development**
- **Cu√°ndo**: ANTES de implementar features cr√≠ticas (agentes, auth, memoria)
- **Por qu√©**: Tests primero aseguran calidad desde el inicio
- **Aplicaci√≥n**: FASES 2-6 (backend)

**Skill: testing-patterns**
- **Cu√°ndo**: Escribiendo tests unitarios y de integraci√≥n
- **Por qu√©**: Jest patterns, factory functions, mocking strategies
- **Aplicaci√≥n**: FASE 11 (testing completo)

### ‚úÖ FASE DE VALIDACI√ìN

**Skill: lint-and-validate**
- **Cu√°ndo**: Despu√©s de CADA modificaci√≥n de c√≥digo
- **Por qu√©**: QA autom√°tico, linting, an√°lisis est√°tico
- **Aplicaci√≥n**: TODAS las fases (validaci√≥n continua)

**Skill: verification-before-completion**
- **Cu√°ndo**: Antes de declarar completitud de cada tarea
- **Por qu√©**: Requiere evidencia ejecutable de comandos
- **Aplicaci√≥n**: Final de cada tarea

**Skill: systematic-debugging**
- **Cu√°ndo**: Bugs, test failures, comportamiento inesperado
- **Por qu√©**: An√°lisis sistem√°tico ANTES de proponer fixes
- **Aplicaci√≥n**: Cuando aparezcan errores

### üöÄ FASE DE DEPLOYMENT

**Skill: docker-expert**
- **Cu√°ndo**: Creando Dockerfile y docker-compose.yml
- **Por qu√©**: Multi-stage builds, optimizaci√≥n, seguridad
- **Aplicaci√≥n**: FASE 10 (Docker y deployment local)

**Skill: deployment-procedures**
- **Cu√°ndo**: Preparando deployment a Portainer (futuro)
- **Por qu√©**: Safe deployment workflows, rollback strategies
- **Aplicaci√≥n**: Post-MVP (deployment a VPS)

---

## üîå Gu√≠a de MCPs

### MCP Archon üéØ (USAR SIEMPRE PRIMERO)

**Contexto del Proyecto:**
Este proyecto usa tecnolog√≠as con documentaci√≥n completa en Archon:
- Backend: FastAPI + Pydantic v2 + LangChain/LangGraph
- Frontend: Next.js 14 + React + TypeScript + shadcn/ui
- Database: Supabase + pgvector
- Python: Async patterns, type hints, decorators

**Flujo de trabajo est√°ndar:**

```yaml
Paso 1 - Listar fuentes disponibles (PRIMERO):
  comando: rag_get_available_sources()
  resultado: |
    Lista completa con source_id de cada documentaci√≥n:
    - Pydantic v2: 9d46e91458092424
    - FastAPI: c889b62860c33a44
    - LangChain: e74f94bb9dcb14aa
    - Supabase: 9c5f534e51ee9237
    - Next.js 14: 77b8a4a07d5230b5
    - React: a931698c21fb8f24
    - TypeScript: d7c76d077e634ab3
    - shadcn/ui: bf102fe8a697ed7c

Paso 2 - Buscar documentaci√≥n espec√≠fica:
  comando: |
    rag_search_knowledge_base(
        query="keywords cortos 2-5 palabras",
        source_id="src_xxx",
        match_count=5
    )
  ejemplos_buenos:
    - "pydantic nested model validation"
    - "fastapi streaming response sse"
    - "langchain agent memory conversation"
    - "supabase pgvector similarity search"
    - "nextjs 14 server component"
  ejemplos_malos:
    - "c√≥mo implementar validaci√≥n de modelos anidados en Pydantic v2"
    - "crear respuesta de streaming con FastAPI usando Server-Sent Events"

Paso 3 - Buscar ejemplos de c√≥digo:
  comando: |
    rag_search_code_examples(
        query="pydantic validator custom",
        source_id="9d46e91458092424",
        match_count=3
    )
  cu√°ndo: "Despu√©s de entender conceptos, necesitas ver implementaciones"
```

**Queries Recomendadas por Fase:**

```yaml
FASE 1 (Base de Datos):
  - query: "supabase pgvector create index"
    source_id: "9c5f534e51ee9237"
  - query: "postgres vector similarity search"
    source_id: "9c5f534e51ee9237"

FASE 2 (Backend Setup):
  - query: "fastapi project structure async"
    source_id: "c889b62860c33a44"
  - query: "pydantic v2 model validation"
    source_id: "9d46e91458092424"

FASE 3.5 (Upload de Documentos):
  - query: "fastapi file upload validation"
    source_id: "c889b62860c33a44"
  - query: "python pdf parsing text extraction"
    (buscar en libros de Python disponibles)

FASE 4 (Agente IA con Memoria):
  - query: "langchain agent memory conversation"
    source_id: "e74f94bb9dcb14aa"
  - query: "langgraph stateful agent"
    source_id: "e74f94bb9dcb14aa"
  - query: "langchain tools custom"
    source_id: "e74f94bb9dcb14aa"

FASE 5 (Entrenamiento RAG):
  - query: "langchain document loader pdf"
    source_id: "e74f94bb9dcb14aa"
  - query: "langchain text splitter recursive"
    source_id: "e74f94bb9dcb14aa"

FASE 6 (API Streaming):
  - query: "fastapi sse streaming"
    source_id: "c889b62860c33a44"
  - query: "fastapi server sent events"
    source_id: "c889b62860c33a44"

FASE 7 (Frontend Auth):
  - query: "nextjs 14 middleware authentication"
    source_id: "77b8a4a07d5230b5"
  - query: "react zustand persist"
    source_id: "a931698c21fb8f24"

FASE 8 (Frontend Chat):
  - query: "nextjs sse event source"
    source_id: "77b8a4a07d5230b5"
  - query: "react streaming updates"
    source_id: "a931698c21fb8f24"
```

### MCP Serena ‚ö° (INSTALAR PRIMERO)

**Filosof√≠a de Uso:**
- ‚ùå **PROHIBIDO**: Leer archivos completos sin raz√≥n
- ‚úÖ **OBLIGATORIO**: `get_symbols_overview` ANTES de leer
- ‚úÖ **PREFERIDO**: `find_symbol` para s√≠mbolos espec√≠ficos
- ‚úÖ **RECOMENDADO**: Ediciones simb√≥licas (`replace_symbol_body`)

**Herramientas Principales y Cu√°ndo Usar:**

```yaml
get_symbols_overview(relative_path):
  prop√≥sito: "Ver estructura de archivos sin leer contenido completo"
  cu√°ndo_usar:
    - Antes de modificar cualquier archivo existente
    - Para entender organizaci√≥n de un m√≥dulo
    - Para decidir d√≥nde a√±adir c√≥digo nuevo
  ejemplo: |
    get_symbols_overview('backend/src/agents/')
    # Retorna: lista de clases, funciones, m√©todos (sin bodies)

find_symbol(name_path, relative_path, include_body):
  prop√≥sito: "Buscar y leer s√≠mbolo espec√≠fico (clase, funci√≥n, m√©todo)"
  cu√°ndo_usar:
    - Sabes el nombre del s√≠mbolo que necesitas modificar
    - Quieres leer implementaci√≥n espec√≠fica
  ejemplos: |
    # Leer m√©todo espec√≠fico de clase
    find_symbol('BuyerPersonaAgent/generate_analysis', 
                'backend/src/agents/buyer_persona_agent.py', 
                include_body=True)
    
    # Ver estructura de clase sin bodies
    find_symbol('ChatService', 
                'backend/src/services/chat_service.py', 
                include_body=False)

search_for_pattern(pattern, relative_path):
  prop√≥sito: "B√∫squeda r√°pida cuando no sabes ubicaci√≥n exacta"
  cu√°ndo_usar:
    - Buscas un patr√≥n de c√≥digo en el proyecto
    - No est√°s seguro en qu√© archivo est√°
  ejemplo: |
    search_for_pattern('async def.*authenticate', 'backend/src/')
    # Encuentra todas las funciones async con 'authenticate'

find_referencing_symbols(name_path, relative_path):
  prop√≥sito: "Ver d√≥nde se usa un s√≠mbolo (impacto de cambios)"
  cu√°ndo_usar:
    - Antes de modificar una funci√≥n/clase
    - Para entender dependencias
  ejemplo: |
    find_referencing_symbols('UserModel', 'backend/src/db/models.py')
    # Retorna snippets de c√≥digo donde se referencia UserModel

replace_symbol_body(name_path, relative_path, new_body):
  prop√≥sito: "Reemplazar implementaci√≥n completa de funci√≥n/clase"
  cu√°ndo_usar:
    - Cambios grandes en un s√≠mbolo espec√≠fico
    - Refactoring de m√©todo completo
  ejemplo: |
    replace_symbol_body(
        'ChatService/create_chat',
        'backend/src/services/chat_service.py',
        'async def create_chat(self, user_id, title): ...'
    )
```

**Patrones de Uso Comunes:**

```yaml
PATR√ìN 1 - Modificar archivo existente:
  paso_1: get_symbols_overview('path/to/file.py')
  paso_2: "Identificar s√≠mbolo a modificar"
  paso_3: find_symbol('Class/method', 'path/to/file.py', True)
  paso_4: "Analizar implementaci√≥n"
  paso_5: replace_symbol_body(...) o edici√≥n manual

PATR√ìN 2 - A√±adir c√≥digo nuevo:
  paso_1: get_symbols_overview('path/to/file.py')
  paso_2: "Ver √∫ltimo s√≠mbolo del archivo"
  paso_3: "Usar insert_after_symbol para a√±adir nueva funci√≥n/clase"

PATR√ìN 3 - Entender dependencias:
  paso_1: find_symbol('ClassName', 'path/to/file.py', False)
  paso_2: find_referencing_symbols('ClassName', 'path/to/file.py')
  paso_3: "Analizar impacto de cambios propuestos"

PATR√ìN 4 - Buscar implementaci√≥n desconocida:
  paso_1: search_for_pattern('async def process_document', 'backend/')
  paso_2: "Identificar archivo correcto en resultados"
  paso_3: get_symbols_overview('archivo_encontrado.py')
  paso_4: find_symbol(...) para leer implementaci√≥n
```

**Integraci√≥n con INITIAL.md:**

El INITIAL.md menciona estos patrones existentes que deber√≠as buscar:
- Patr√≥n de agentes con dependencies (ver ejemplos)
- Patr√≥n de tools con decoradores
- Patr√≥n de Pydantic models con Field descriptions
- Patr√≥n de endpoints FastAPI con dependency injection

Usar Serena para encontrarlos:
```bash
search_for_pattern('@research_agent.tool', 'Context-Engineering-Intro/examples/')
search_for_pattern('class.*BaseModel', 'Context-Engineering-Intro/examples/')
```

---

## üì¶ Todo el Contexto Necesario

### Documentaci√≥n & Referencias

**Prioridad de Consulta:**

```yaml
NIVEL 1 - MCP Archon (PRIORITARIO):
  acci√≥n: |
    # 1. Obtener fuentes disponibles
    rag_get_available_sources()
    
    # 2. Identificar source_id relevante
    # Ejemplo para FastAPI:
    source_id = "c889b62860c33a44"
    
    # 3. Buscar documentaci√≥n espec√≠fica
    rag_search_knowledge_base(
        query="fastapi streaming response",
        source_id="c889b62860c33a44",
        match_count=5
    )
  
  fuentes_disponibles:
    pydantic: "9d46e91458092424"
    fastapi: "c889b62860c33a44"
    langchain: "e74f94bb9dcb14aa"
    supabase: "9c5f534e51ee9237"
    nextjs: "77b8a4a07d5230b5"
    react: "a931698c21fb8f24"
    typescript: "d7c76d077e634ab3"
    shadcn_ui: "bf102fe8a697ed7c"
  
  por_qu√©: "Documentaci√≥n oficial verificada y actualizada"

NIVEL 2 - Ejemplos del Proyecto con Serena:
  ubicaci√≥n: "Context-Engineering-Intro/examples/"
  
  ejemplos_relevantes:
    - main_agent_reference/:
        - research_agent.py: "Patr√≥n de agente con pydantic-ai y tools"
        - models.py: "Modelos Pydantic con Field descriptions"
        - providers.py: "Configuraci√≥n de LLM models"
    
    - mcp-server/:
        - src/index.ts: "MCP server con auth"
        - src/tools/: "Tool registration pattern"
        - CLAUDE.md: "Est√°ndares de implementaci√≥n"
  
  acci√≥n: |
    # Ver estructura de ejemplo
    get_symbols_overview('Context-Engineering-Intro/examples/main_agent_reference/research_agent.py')
    
    # Leer patr√≥n espec√≠fico
    find_symbol('research_agent', 'Context-Engineering-Intro/examples/main_agent_reference/research_agent.py', True)
  
  por_qu√©: "Patrones probados del proyecto espec√≠fico"

NIVEL 3 - Archivos del Proyecto (cuando existan):
  acci√≥n: |
    # ANTES de modificar cualquier archivo:
    get_symbols_overview('backend/src/agents/buyer_persona_agent.py')
    
    # Leer s√≠mbolo espec√≠fico:
    find_symbol('BuyerPersonaAgent/generate', 'backend/src/agents/buyer_persona_agent.py', True)
    
    # Ver referencias:
    find_referencing_symbols('BuyerPersonaAgent', 'backend/src/agents/buyer_persona_agent.py')
  
  por_qu√©: "Entender c√≥digo existente sin leer archivos completos"

NIVEL 4 - URLs Externos (√öLTIMO RECURSO):
  cu√°ndo_usar: "Solo si Archon no tiene la informaci√≥n"
  
  ejemplos_v√°lidos:
    - url: "https://docs.anthropic.com/claude/reference"
      por_qu√©: "API espec√≠fica de Claude 3.5 Sonnet"
      secci√≥n: "Streaming responses"
    
    - url: "https://python.langchain.com/docs/modules/agents/agent_types/"
      por_qu√©: "Tipos de agentes espec√≠ficos no cubiertos en Archon"
      secci√≥n: "ReAct agent pattern"
  
  nota: "Archon tiene cobertura completa de Python, Pydantic, FastAPI, LangChain, Supabase, Next.js"
```

### Insights de Ejemplos de Referencia

**Patr√≥n 1: Agentes con Dependencies (de `research_agent.py`):**

```python
# PATR√ìN: Usar @dataclass para dependencies
from dataclasses import dataclass
from pydantic_ai import Agent, RunContext

@dataclass
class ResearchAgentDependencies:
    """Dependencies - solo configuraci√≥n, no instancias de tools"""
    brave_api_key: str
    gmail_credentials_path: str
    session_id: Optional[str] = None

# PATR√ìN: Inicializar agente con deps_type
research_agent = Agent(
    get_llm_model(),
    deps_type=ResearchAgentDependencies,
    system_prompt=SYSTEM_PROMPT
)

# PATR√ìN: Tools reciben contexto con dependencies
@research_agent.tool
async def search_web(
    ctx: RunContext[ResearchAgentDependencies],
    query: str,
    max_results: int = 10
) -> List[Dict[str, Any]]:
    # Acceder a dependencies via ctx.deps
    api_key = ctx.deps.brave_api_key
    results = await search_web_tool(api_key=api_key, query=query)
    return results
```

**Aplicaci√≥n al Proyecto:**
- Usar patr√≥n similar para BuyerPersonaAgent, ContentGeneratorAgent
- Dependencies incluir√°n: supabase_url, anthropic_api_key, project_id
- Tools acceder√°n a DB y LLM via dependencies

**Patr√≥n 2: Modelos Pydantic con Validaci√≥n (de `models.py`):**

```python
# PATR√ìN: Usar Field para descripciones detalladas
from pydantic import BaseModel, Field
from typing import List, Optional

class ResearchQuery(BaseModel):
    """Model for research query requests."""
    query: str = Field(..., description="Research topic to investigate")
    max_results: int = Field(10, ge=1, le=50, description="Maximum number of results")
    include_summary: bool = Field(True, description="Whether to include AI summary")

class BraveSearchResult(BaseModel):
    """Model for individual search results."""
    title: str = Field(..., description="Title of result")
    url: str = Field(..., description="URL of result")
    score: float = Field(0.0, ge=0.0, le=1.0, description="Relevance score")
    
    class Config:
        json_schema_extra = {
            "example": {
                "title": "Example Result",
                "url": "https://example.com",
                "score": 0.95
            }
        }
```

**Aplicaci√≥n al Proyecto:**
- Crear schemas Pydantic para: BuyerPersona, CustomerJourney, ContentIdea, UserDocument
- Incluir Field descriptions detalladas (ayuda al LLM)
- A√±adir ejemplos en Config (documentaci√≥n autom√°tica)

**Patr√≥n 3: MCP Tool Registration (de `mcp-server/`):**

```typescript
// PATR√ìN: Registro centralizado de tools
// src/tools/register-tools.ts
import { McpServer } from "@modelcontextprotocol/sdk/server/mcp.js";

export function registerAllTools(server: McpServer, env: Env, props: Props) {
  // Registro por dominio/feature
  registerDatabaseTools(server, env, props);
  registerAnalyticsTools(server, env, props);
  // ... m√°s tools
}

// PATR√ìN: Cada feature tiene su propio m√≥dulo de tools
// examples/database-tools.ts
export function registerDatabaseTools(server: McpServer, env: Env, props: Props) {
  // Tool disponible para todos
  server.tool("listTables", "Get all tables", ListTablesSchema, async () => {
    // Implementation
  });
  
  // Tool solo para usuarios privilegiados
  if (ALLOWED_USERNAMES.has(props.login)) {
    server.tool("executeDatabase", "Execute SQL", ExecuteSchema, async ({ sql }) => {
      // Implementation
    });
  }
}
```

**Aplicaci√≥n al Proyecto:**
- Crear MCP custom para el proyecto (FASE 9)
- Tools: analyze_buyer_persona, generate_content_ideas, search_knowledge_base
- Implementar permisos por usuario si necesario

### √Årbol del Codebase Actual

```bash
# Estado inicial del proyecto (a crear)
/home/david/brain-mkt/
‚îú‚îÄ‚îÄ README.md (TO CREATE)
‚îú‚îÄ‚îÄ .gitignore (TO CREATE)
‚îú‚îÄ‚îÄ docker-compose.yml (TO CREATE)
‚îÇ
‚îú‚îÄ‚îÄ frontend/ (TO CREATE)
‚îÇ   ‚îú‚îÄ‚îÄ package.json
‚îÇ   ‚îú‚îÄ‚îÄ tsconfig.json
‚îÇ   ‚îú‚îÄ‚îÄ next.config.js
‚îÇ   ‚îú‚îÄ‚îÄ tailwind.config.ts
‚îÇ   ‚îú‚îÄ‚îÄ .env.local
‚îÇ   ‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ layout.tsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ page.tsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ (auth)/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ login/page.tsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ register/page.tsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ recover-password/page.tsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ reset-password/[token]/page.tsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dashboard/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ layout.tsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ page.tsx
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ api/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ chat/route.ts
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ auth/route.ts
‚îÇ   ‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ auth/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ LoginForm.tsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ RegisterForm.tsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chat/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ChatInterface.tsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ MessageList.tsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ChatInput.tsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ DocumentUploader.tsx (NEW)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ DocumentsList.tsx (NEW)
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ BuyerPersonaView.tsx
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dashboard/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ ChatSidebar.tsx
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ ChatHeader.tsx
‚îÇ   ‚îî‚îÄ‚îÄ stores/
‚îÇ       ‚îú‚îÄ‚îÄ authStore.ts
‚îÇ       ‚îî‚îÄ‚îÄ chatStore.ts
‚îÇ
‚îú‚îÄ‚îÄ backend/ (TO CREATE)
‚îÇ   ‚îú‚îÄ‚îÄ pyproject.toml
‚îÇ   ‚îú‚îÄ‚îÄ .env
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ auth.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chat.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ buyer_persona.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ documents.py (NEW)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agents/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ router_agent.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ document_processor_agent.py (NEW)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ buyer_persona_agent.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ forum_simulator_agent.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pain_points_agent.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ customer_journey_agent.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ content_generator_agent.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ memory_manager.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ db/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ supabase_client.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ migrations/
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ 001_initial_schema.sql
‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ 002_create_match_function.sql
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm_service.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ embedding_service.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vector_search.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ document_processor.py (NEW)
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ chat_service.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schemas/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ auth.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chat.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ buyer_persona.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ documents.py (NEW)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ validation.py
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ formatting.py
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ jwt.py
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ password.py
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ file_parsers.py (NEW - txt, pdf, docx)
‚îÇ   ‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ingest_training_data.py
‚îÇ   ‚îî‚îÄ‚îÄ tests/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ test_auth.py
‚îÇ       ‚îú‚îÄ‚îÄ test_chat.py
‚îÇ       ‚îú‚îÄ‚îÄ test_agents.py
‚îÇ       ‚îú‚îÄ‚îÄ test_documents.py (NEW)
‚îÇ       ‚îî‚îÄ‚îÄ fixtures/
‚îÇ
‚îú‚îÄ‚îÄ mcp-server/ (TO CREATE - FASE 9)
‚îÇ   ‚îú‚îÄ‚îÄ server.py
‚îÇ   ‚îú‚îÄ‚îÄ tools/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analyze_buyer_persona.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ generate_content_ideas.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ search_knowledge_base.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ export_strategy.py
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îÇ
‚îú‚îÄ‚îÄ training_data/ (TO PROVIDE BY USER)
‚îÇ   ‚îú‚îÄ‚îÄ videos/ (transcripciones .txt)
‚îÇ   ‚îî‚îÄ‚îÄ books/ (libros .pdf)
‚îÇ
‚îî‚îÄ‚îÄ Context-Engineering-Intro/ (EXISTING)
    ‚îú‚îÄ‚îÄ examples/ (reference patterns)
    ‚îú‚îÄ‚îÄ PRPs/ (este archivo)
    ‚îî‚îÄ‚îÄ validation/
```

### √Årbol del Codebase Deseado (Post-Implementaci√≥n)

```bash
# Estado final esperado (todas las fases completadas)
/home/david/brain-mkt/
‚îú‚îÄ‚îÄ README.md ‚úÖ
‚îú‚îÄ‚îÄ docker-compose.yml ‚úÖ
‚îú‚îÄ‚îÄ .dockerignore ‚úÖ
‚îú‚îÄ‚îÄ .gitignore ‚úÖ
‚îÇ
‚îú‚îÄ‚îÄ frontend/ ‚úÖ (3000 puerto)
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile ‚úÖ
‚îÇ   ‚îú‚îÄ‚îÄ [todos los archivos de estructura inicial] ‚úÖ
‚îÇ   ‚îî‚îÄ‚îÄ [componentes completamente funcionales] ‚úÖ
‚îÇ
‚îú‚îÄ‚îÄ backend/ ‚úÖ (8000 puerto)
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile ‚úÖ
‚îÇ   ‚îú‚îÄ‚îÄ [todos los archivos de estructura inicial] ‚úÖ
‚îÇ   ‚îú‚îÄ‚îÄ [agentes implementados y funcionando] ‚úÖ
‚îÇ   ‚îú‚îÄ‚îÄ [tests >80% coverage] ‚úÖ
‚îÇ   ‚îî‚îÄ‚îÄ storage/ (archivos subidos por usuarios)
‚îÇ       ‚îî‚îÄ‚îÄ documents/ (organizados por project_id/chat_id/)
‚îÇ
‚îú‚îÄ‚îÄ mcp-server/ ‚úÖ
‚îÇ   ‚îú‚îÄ‚îÄ [MCP funcional con 4 tools] ‚úÖ
‚îÇ   ‚îî‚îÄ‚îÄ [documentaci√≥n de uso] ‚úÖ
‚îÇ
‚îú‚îÄ‚îÄ training_data/ ‚úÖ
‚îÇ   ‚îú‚îÄ‚îÄ videos/ (con archivos .txt proporcionados)
‚îÇ   ‚îî‚îÄ‚îÄ books/ (con archivos .pdf proporcionados)
‚îÇ
‚îú‚îÄ‚îÄ docs/ ‚úÖ
‚îÇ   ‚îú‚îÄ‚îÄ API.md ‚úÖ
‚îÇ   ‚îú‚îÄ‚îÄ DEPLOYMENT.md ‚úÖ
‚îÇ   ‚îî‚îÄ‚îÄ MCP_GUIDE.md ‚úÖ
‚îÇ
‚îî‚îÄ‚îÄ scripts/
    ‚îú‚îÄ‚îÄ docker-build.sh ‚úÖ
    ‚îú‚îÄ‚îÄ docker-up.sh ‚úÖ
    ‚îî‚îÄ‚îÄ docker-logs.sh ‚úÖ
```

### Gotchas Conocidos y Cr√≠ticos

**üî¥ NIVEL CR√çTICO - Pueden romper el sistema:**

```python
# GOTCHA 1: Supabase pgvector requiere >1000 rows para √≠ndice ivfflat
# Problema: √çndice ivfflat no es efectivo con <1000 documentos
# Soluci√≥n: Usar hnsw index o asegurar >1000 chunks en knowledge_base
CREATE INDEX ON marketing_knowledge_base 
USING hnsw (embedding vector_cosine_ops); -- Mejor para pocos documentos

# GOTCHA 2: LangChain ConversationBufferMemory crece indefinidamente
# Problema: Memoria crece sin l√≠mite, consume tokens
# Soluci√≥n: Usar ConversationBufferWindowMemory con k=10
from langchain.memory import ConversationBufferWindowMemory
memory = ConversationBufferWindowMemory(k=10, return_messages=True)

# GOTCHA 3: FastAPI StreamingResponse + middleware que lee body
# ‚ö†Ô∏è INTEGRADO EN: TAREA 6 - API de Chat con Streaming (cuando se detalle)
# Problema: Middleware que lee request body rompe streaming
# Soluci√≥n: Excluir /stream endpoints del middleware
@app.middleware("http")
async def log_requests(request: Request, call_next):
    # ‚úÖ Detectar endpoints de streaming ANTES de leer body
    if "/stream" in request.url.path or "/sse" in request.url.path:
        return await call_next(request)  # Skip middleware
    
    # Para endpoints normales, procesar como siempre
    body = await request.body()
    # ... resto del middleware

# GOTCHA 4: Next.js Server Components + useState
# ‚ö†Ô∏è INTEGRADO EN: TAREA 8 - Frontend Chat Interface (cuando se detalle)
# Problema: No puedes usar useState en Server Components
# Soluci√≥n: Marcar componentes interactivos con 'use client'
'use client'  // ‚úÖ Al inicio del archivo
import { useState } from 'react'

# Patr√≥n recomendado:
# - app/page.tsx ‚Üí Server Component (fetch datos, layout)
# - app/components/ChatInterface.tsx ‚Üí 'use client' (useState, eventos)

# GOTCHA 5: Embeddings de OpenAI rate limit (3000 RPM en tier free)
# Problema: Al procesar muchos chunks, se alcanza l√≠mite
# Soluci√≥n: Batch de embeddings + retry con exponential backoff
async def generate_embeddings_batch(texts: List[str]) -> List[List[float]]:
    batch_size = 50
    results = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        try:
            response = await openai.Embedding.create(input=batch, model="text-embedding-3-large")
            results.extend([e['embedding'] for e in response['data']])
        except RateLimitError:
            await asyncio.sleep(2 ** retry_count)  # Exponential backoff
            retry_count += 1
    return results

# GOTCHA 6: Supabase RLS no aplica con service role key
# Problema: Service role key bypasea RLS policies
# Soluci√≥n: Validar project_id manualmente en backend
async def get_chats(user_id: UUID, project_id: UUID):
    # Validar project_id manualmente, no confiar solo en RLS
    user = await db.users.find_one({'id': user_id})
    if user.project_id != project_id:
        raise PermissionError("User not in this project")
    return await db.chats.find({'user_id': user_id, 'project_id': project_id})

# GOTCHA 7: LangChain Tools - Descripciones vagas causan mal uso
# Problema: LLM usa tool incorrecto si descripci√≥n es vaga
# Soluci√≥n: Descripciones espec√≠ficas con ejemplos en docstring
@agent.tool
async def generate_content_ideas(
    ctx: RunContext,
    phase: str,
    content_type: str,
    count: int = 5
) -> List[Dict]:
    """
    Generate content ideas for a specific customer journey phase.
    
    Use this tool when user explicitly asks for content ideas.
    Examples: "Dame 5 ideas de videos", "Crea ideas de posts"
    
    Args:
        phase: One of: "awareness", "consideration", "purchase"
        content_type: One of: "video", "post", "article"
        count: Number of ideas to generate (default 5)
    
    DO NOT use this tool for analyzing buyer persona or creating customer journey.
    """
    ...

# GOTCHA 8: Docker volumes en Windows - Permisos incorrectos
# Problema: Bind mounts tienen permisos raros en Windows
# Soluci√≥n: Usar named volumes en vez de bind mounts
# BAD:
volumes:
  - ./backend:/app  # Bind mount - problemas en Windows
# GOOD:
volumes:
  - backend_data:/app  # Named volume - funciona en todos lados

# GOTCHA 9: pgvector cosine distance no es similarity 0-1
# Problema: 1 - cosine_distance no devuelve score normalizado
# Soluci√≥n: Normalizar embeddings antes de insertar
def normalize_embedding(embedding: List[float]) -> List[float]:
    norm = np.linalg.norm(embedding)
    return [x / norm for x in embedding]

# GOTCHA 10: JWT en localStorage + Server Components
# ‚ö†Ô∏è INTEGRADO EN: TAREA 7 - Frontend Auth + TAREA 8 - Chat Interface (cuando se detallen)
# Problema: localStorage no accesible en Server Components de Next.js
# Soluci√≥n: Usar cookies httpOnly y leer en middleware
// ‚ùå NO HACER: localStorage.setItem('token', jwt)
// ‚úÖ HACER: cookies httpOnly

// Backend: Setear cookie en response de /login
response.set_cookie(
    key="auth_token",
    value=jwt_token,
    httponly=True,  # ‚Üê No accesible desde JavaScript
    secure=True,  # Solo HTTPS en production
    samesite="lax",
    max_age=604800  # 7 d√≠as
)

// Frontend: middleware.ts lee cookie autom√°ticamente
export async function middleware(request: NextRequest) {
    const token = request.cookies.get('auth_token')?.value
    if (!token) {
        return NextResponse.redirect(new URL('/login', request.url))
    }
    // Validar JWT antes de permitir acceso
    // ...
}
```

**üü° NIVEL ADVERTENCIA - Pueden causar problemas de performance:**

```python
# WARNING 1: No usar .all() en queries grandes
# BAD:
all_messages = await db.messages.find().all()  # Puede ser millones
# GOOD:
messages = await db.messages.find({'chat_id': chat_id}).limit(100).all()

# WARNING 2: No generar embeddings s√≠ncronamente
# BAD:
for chunk in chunks:
    embedding = openai.Embedding.create(input=chunk['text'])  # S√≠ncrono
# GOOD:
embeddings = await generate_embeddings_batch([c['text'] for c in chunks])

# WARNING 3: No hacer queries N+1 en loops
# BAD:
for chat_id in chat_ids:
    messages = await db.messages.find({'chat_id': chat_id})  # N queries
# GOOD:
messages = await db.messages.find({'chat_id': {'$in': chat_ids}})  # 1 query

# WARNING 4: No procesar archivos grandes en memoria
# BAD:
content = file.read()  # Todo el archivo en RAM
# GOOD:
async for chunk in file.stream():
    await process_chunk(chunk)  # Stream processing
```

### Validaci√≥n de Restricciones Cr√≠ticas

```yaml
RESTRICCI√ìN 1: Supabase Auth NO disponible
  raz√≥n: "Limitaci√≥n en correos electr√≥nicos"
  soluci√≥n: "Implementar autenticaci√≥n manual con JWT"
  impacto:
    - Crear tabla marketing_users con password_hash
    - Implementar endpoints de auth manualmente
    - Usar bcrypt para passwords
    - JWT para sesiones
  validaci√≥n: |
    # Verificar que NO se use Supabase Auth
    grep -r "supabase.auth" backend/src/
    # Esperado: Sin resultados

RESTRICCI√ìN 2: Aislamiento total por project_id
  raz√≥n: "Multi-tenancy estricto"
  soluci√≥n: "Incluir project_id en TODAS las queries"
  impacto:
    - WHERE project_id = ? en TODAS las queries
    - RLS policies con project_id
    - Middleware para inyectar project_id desde JWT
  validaci√≥n: |
    # Verificar que todas las queries incluyen project_id
    grep -r "SELECT.*FROM marketing_" backend/src/ | grep -v "project_id"
    # Esperado: Sin resultados (todas incluyen project_id)

RESTRICCI√ìN 3: Agente NO genera autom√°ticamente
  raz√≥n: "Usuario controla cu√°ndo generar contenido"
  soluci√≥n: "Content Generator solo se ejecuta con petici√≥n expl√≠cita"
  impacto:
    - Router agent detecta petici√≥n de contenido
    - Content Generator NO se ejecuta en an√°lisis inicial
    - Usuario debe pedir expl√≠citamente
  validaci√≥n: |
    # Verificar que Content Generator no se ejecuta autom√°ticamente
    grep -A10 "class RouterAgent" backend/src/agents/router_agent.py
    # Debe haber l√≥gica de detecci√≥n de petici√≥n de contenido

RESTRICCI√ìN 4: Upload m√°ximo 10MB por archivo
  raz√≥n: "Evitar sobrecarga de servidor y procesamiento lento"
  soluci√≥n: "Validar tama√±o antes de aceptar archivo"
  impacto:
    - Validaci√≥n en frontend (pre-upload)
    - Validaci√≥n en backend (endpoint)
  validaci√≥n: |
    # Verificar l√≠mite en backend
    grep "MAX_FILE_SIZE" backend/src/api/documents.py
    # Esperado: MAX_FILE_SIZE = 10 * 1024 * 1024  # 10MB
```

---

## üèóÔ∏è Blueprint de Implementaci√≥n

### Modelos de Datos y Estructura

#### SQLAlchemy Models (backend/src/db/models.py):

```python
from sqlalchemy import Column, String, UUID, TIMESTAMP, ForeignKey, Integer, Boolean, Text
from sqlalchemy.dialects.postgresql import JSONB
from pgvector.sqlalchemy import Vector
from sqlalchemy.ext.declarative import declarative_base
import uuid
from datetime import datetime

Base = declarative_base()

class MarketingProject(Base):
    __tablename__ = 'marketing_projects'
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    name = Column(String(255), nullable=False)
    owner_user_id = Column(UUID(as_uuid=True), ForeignKey('marketing_users.id'), nullable=True)
    created_at = Column(TIMESTAMP, default=datetime.utcnow, nullable=False)
    updated_at = Column(TIMESTAMP, default=datetime.utcnow, onupdate=datetime.utcnow, nullable=False)

class MarketingUser(Base):
    __tablename__ = 'marketing_users'
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    email = Column(String(255), nullable=False)
    password_hash = Column(String(255), nullable=False)
    full_name = Column(String(255))
    project_id = Column(UUID(as_uuid=True), ForeignKey('marketing_projects.id'), nullable=False)
    created_at = Column(TIMESTAMP, default=datetime.utcnow, nullable=False)
    last_login = Column(TIMESTAMP)
    
    # √çNDICE COMPUESTO: (email, project_id)
    # CONSTRAINT: UNIQUE (email, project_id)

class MarketingChat(Base):
    __tablename__ = 'marketing_chats'
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    user_id = Column(UUID(as_uuid=True), ForeignKey('marketing_users.id'), nullable=False)
    project_id = Column(UUID(as_uuid=True), ForeignKey('marketing_projects.id'), nullable=False)
    title = Column(String(255), default="New Chat", nullable=False)
    created_at = Column(TIMESTAMP, default=datetime.utcnow, nullable=False)
    updated_at = Column(TIMESTAMP, default=datetime.utcnow, onupdate=datetime.utcnow, nullable=False)
    
    # √çNDICE: (user_id, project_id), created_at DESC
    # RLS: WHERE user_id = auth.uid() AND project_id = current_project()

class MarketingMessage(Base):
    __tablename__ = 'marketing_messages'
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    chat_id = Column(UUID(as_uuid=True), ForeignKey('marketing_chats.id', ondelete='CASCADE'), nullable=False)
    project_id = Column(UUID(as_uuid=True), ForeignKey('marketing_projects.id'), nullable=False)
    role = Column(String(20), nullable=False)  # 'user' | 'assistant' | 'system'
    content = Column(Text, nullable=False)
    metadata = Column(JSONB, default={})
    created_at = Column(TIMESTAMP, default=datetime.utcnow, nullable=False)
    
    # √çNDICE: (chat_id, created_at), project_id

class MarketingBuyerPersona(Base):
    __tablename__ = 'marketing_buyer_personas'
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    chat_id = Column(UUID(as_uuid=True), ForeignKey('marketing_chats.id'), nullable=False)
    project_id = Column(UUID(as_uuid=True), ForeignKey('marketing_projects.id'), nullable=False)
    initial_questions = Column(JSONB, nullable=False)  # 4-5 respuestas
    full_analysis = Column(JSONB, nullable=False)  # 35+ preguntas
    forum_simulation = Column(JSONB, nullable=False)  # Array de {queja, solucion}
    pain_points = Column(JSONB, nullable=False)  # Array de strings (10 puntos)
    customer_journey = Column(JSONB, nullable=False)  # {awareness, consideration, purchase}
    embedding = Column(Vector(1536))  # Embedding del an√°lisis completo
    created_at = Column(TIMESTAMP, default=datetime.utcnow, nullable=False)
    
    # √çNDICE: chat_id, project_id, embedding (ivfflat o hnsw)

class MarketingKnowledgeBase(Base):
    __tablename__ = 'marketing_knowledge_base'
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    project_id = Column(UUID(as_uuid=True), ForeignKey('marketing_projects.id'), nullable=True)  # NULL = global
    chat_id = Column(UUID(as_uuid=True), ForeignKey('marketing_chats.id'), nullable=True)  # NULL = global
    content_type = Column(String(50), nullable=False)  # 'video_transcript' | 'book' | 'user_document'
    source_title = Column(String(500), nullable=False)
    chunk_text = Column(Text, nullable=False)
    chunk_index = Column(Integer, nullable=False)
    metadata = Column(JSONB, default={})
    embedding = Column(Vector(1536), nullable=False)
    created_at = Column(TIMESTAMP, default=datetime.utcnow, nullable=False)
    
    # √çNDICE: embedding (ivfflat o hnsw), content_type, project_id, chat_id
    # NOTA:
    #  - project_id NULL + chat_id NULL = conocimiento global (YouTubers + libros)
    #  - project_id NOT NULL + chat_id NOT NULL = documentos subidos por usuario

class MarketingUserDocument(Base):
    __tablename__ = 'marketing_user_documents'
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    chat_id = Column(UUID(as_uuid=True), ForeignKey('marketing_chats.id', ondelete='CASCADE'), nullable=False)
    project_id = Column(UUID(as_uuid=True), ForeignKey('marketing_projects.id'), nullable=False)
    user_id = Column(UUID(as_uuid=True), ForeignKey('marketing_users.id'), nullable=False)
    filename = Column(String(500), nullable=False)
    file_type = Column(String(10), nullable=False)  # '.txt' | '.pdf' | '.docx'
    file_size = Column(Integer, nullable=False)  # bytes
    file_path = Column(String(1000), nullable=False)  # ruta en storage
    chunks_count = Column(Integer, default=0, nullable=False)
    processed = Column(Boolean, default=False, nullable=False)
    created_at = Column(TIMESTAMP, default=datetime.utcnow, nullable=False)
    
    # √çNDICE: (chat_id, user_id), project_id
    # PROP√ìSITO: Tracking de documentos subidos por el usuario

class MarketingPasswordResetToken(Base):
    __tablename__ = 'marketing_password_reset_tokens'
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    user_id = Column(UUID(as_uuid=True), ForeignKey('marketing_users.id'), nullable=False)
    project_id = Column(UUID(as_uuid=True), ForeignKey('marketing_projects.id'), nullable=False)
    token = Column(String(255), unique=True, nullable=False)
    expires_at = Column(TIMESTAMP, nullable=False)
    used = Column(Boolean, default=False, nullable=False)
    
    # √çNDICE: token, (user_id, used)
    # EXPIRA: 1 hora
```

#### Pydantic Schemas (backend/src/schemas/):

**auth.py:**
```python
from pydantic import BaseModel, Field, EmailStr, field_validator
from typing import Optional
from uuid import UUID

class RegisterRequest(BaseModel):
    email: EmailStr = Field(..., description="User email address")
    password: str = Field(..., min_length=8, description="Password (min 8 chars)")
    full_name: Optional[str] = Field(None, description="User full name")
    project_id: UUID = Field(..., description="Project to join")
    
    @field_validator('password')
    def validate_password_strength(cls, v):
        if not any(c.isupper() for c in v):
            raise ValueError('Password must contain uppercase letter')
        if not any(c.isdigit() for c in v):
            raise ValueError('Password must contain digit')
        return v

class LoginRequest(BaseModel):
    email: EmailStr
    password: str
    project_id: UUID

class LoginResponse(BaseModel):
    access_token: str
    token_type: str = "bearer"
    user: dict  # {id, email, full_name, project_id}

class RequestPasswordResetRequest(BaseModel):
    email: EmailStr
    project_id: UUID

class ResetPasswordRequest(BaseModel):
    token: str
    new_password: str = Field(..., min_length=8)
```

**chat.py:**
```python
from pydantic import BaseModel, Field
from typing import Optional, List, Dict, Any
from uuid import UUID
from datetime import datetime

class CreateChatRequest(BaseModel):
    title: Optional[str] = Field("New Chat", description="Chat title")

class CreateChatResponse(BaseModel):
    id: UUID
    title: str
    created_at: datetime

class SendMessageRequest(BaseModel):
    content: str = Field(..., min_length=1, max_length=5000)

class Message(BaseModel):
    id: UUID
    role: str  # 'user' | 'assistant' | 'system'
    content: str
    metadata: Dict[str, Any] = {}
    created_at: datetime

class ChatSummary(BaseModel):
    id: UUID
    title: str
    created_at: datetime
    updated_at: datetime
    message_count: Optional[int] = None
```

**documents.py (NUEVO):**
```python
from pydantic import BaseModel, Field, field_validator
from typing import List
from uuid import UUID
from datetime import datetime

class UploadDocumentResponse(BaseModel):
    id: UUID
    filename: str
    file_type: str
    file_size: int
    processed: bool = False
    created_at: datetime

class DocumentMetadata(BaseModel):
    id: UUID
    filename: str
    file_type: str
    file_size: int
    chunks_count: int
    processed: bool
    created_at: datetime

class ListDocumentsResponse(BaseModel):
    documents: List[DocumentMetadata]
    total: int

# Validaci√≥n de archivo
ALLOWED_FILE_TYPES = {'.txt', '.pdf', '.docx'}
MAX_FILE_SIZE = 10 * 1024 * 1024  # 10MB

@field_validator('file_type')
def validate_file_type(cls, v):
    if v not in ALLOWED_FILE_TYPES:
        raise ValueError(f'File type must be one of {ALLOWED_FILE_TYPES}')
    return v
```

**buyer_persona.py:**
```python
from pydantic import BaseModel, Field
from typing import List, Dict, Any
from uuid import UUID

class InitialQuestions(BaseModel):
    q1_business: str
    q2_ideal_customer: str
    q3_main_problem: str
    q4_desired_outcome: str
    q5_competition: str

class ForumSimulation(BaseModel):
    complaint: str
    desired_solution: str

class CustomerJourneyPhase(BaseModel):
    questions: List[str] = Field(..., min_items=20, max_items=20)
    mindset: str
    content_types: List[str]

class CustomerJourney(BaseModel):
    awareness: CustomerJourneyPhase
    consideration: CustomerJourneyPhase
    purchase: CustomerJourneyPhase

class BuyerPersonaAnalysis(BaseModel):
    initial_questions: InitialQuestions
    full_analysis: Dict[str, Any]  # 35+ preguntas estructuradas
    forum_simulation: List[ForumSimulation]
    pain_points: List[str] = Field(..., min_items=10, max_items=10)
    customer_journey: CustomerJourney
```

#### Arquitectura de Agentes (LangChain + LangGraph):

```python
# backend/src/agents/router_agent.py
from langchain.agents import Agent
from langchain.memory import ConversationBufferWindowMemory
from enum import Enum

class AgentState(str, Enum):
    INITIAL = "initial"
    DOCUMENT_PROCESSING = "document_processing"
    BUYER_PERSONA = "buyer_persona"
    FORUM_SIMULATION = "forum_simulation"
    PAIN_POINTS = "pain_points"
    CUSTOMER_JOURNEY = "customer_journey"
    WAITING = "waiting"
    CONTENT_GENERATION = "content_generation"

class RouterAgent:
    """
    Orquestador principal que decide qu√© agente ejecutar.
    
    Flujo:
    1. Detecta estado del chat (¬øya tiene buyer persona?)
    2. Detecta tipo de petici√≥n del usuario
    3. Ejecuta agente correspondiente
    
    CR√çTICO: Content Generator SOLO se ejecuta con petici√≥n expl√≠cita
    """
    def __init__(self, memory_manager, llm_service):
        self.memory_manager = memory_manager
        self.llm = llm_service
        self.memory = ConversationBufferWindowMemory(k=10)
    
    async def route(self, chat_id: UUID, user_message: str) -> AgentState:
        # 1. Obtener contexto del chat
        chat_context = await self.memory_manager.get_long_term_context(chat_id)
        
        # 2. Decidir estado
        if not chat_context.get('buyer_persona'):
            if not chat_context.get('documents_uploaded'):
                return AgentState.BUYER_PERSONA
            else:
                return AgentState.DOCUMENT_PROCESSING
        
        # 3. Detectar petici√≥n de contenido
        if self._is_content_request(user_message):
            return AgentState.CONTENT_GENERATION
        
        # 4. Por defecto, esperar
        return AgentState.WAITING
    
    def _is_content_request(self, message: str) -> bool:
        """
        Detecta si usuario solicita generaci√≥n de contenido.
        
        Keywords: dame, genera, crea, escribe, ideas, posts, videos, scripts
        """
        keywords = ['dame', 'genera', 'crea', 'escribe', 'ideas', 'posts', 'videos', 'scripts']
        return any(kw in message.lower() for kw in keywords)
```

---

## üéØ Recursos Clave del Proyecto (¬°CR√çTICOS!)

**Estos 3 recursos son la BASE de conocimiento que hace √∫nico a este sistema:**

### RECURSO 1: Plantilla de Buyer Persona üìã

```yaml
Ubicaci√≥n: contenido/buyer-plantilla.md

Prop√≥sito:
  "Estructura base completa que el agente DEBE seguir al crear an√°lisis de buyer persona"

Contenido:
  - 11 categor√≠as organizadas
  - ~40 preguntas espec√≠ficas y detalladas
  - Ejemplo real completo: "Ana" (enfermera, 35 a√±os, Barcelona, preparando EIR)
  
Estructura de las 11 Categor√≠as:
  1. Aspectos Demogr√°ficos (5 preguntas): nombre, edad, estudios, ingresos, ubicaci√≥n, estado civil
  2. Hogar y Familia (3 preguntas): integrantes, ocio, responsabilidades
  3. Trabajo (3 preguntas): d√≥nde trabaja, retos laborales, vida laboral vs personal
  4. Comportamiento (2 preguntas): relaciones, expresiones y lenguaje del grupo social
  5. Problema (2 preguntas): qu√© dolor activa b√∫squeda, c√≥mo tu producto lo soluciona
  6. B√∫squeda de la Soluci√≥n (3 preguntas): d√≥nde busca, c√≥mo te encuentra, reacci√≥n a comerciales
  7. Objeciones y Barreras (2 preguntas): barreras para no comprar, excusas que usa
  8. Miedos e Inseguridades (2 preguntas): qu√© odia encontrar, experiencias negativas previas
  9. Comparaci√≥n con Competencia (5 preguntas): factores de comparaci√≥n, diferencias, mejor/peor, por qu√© elige
  10. Tu Producto o Servicio (4 preguntas): beneficios percibidos/no percibidos, complementarios, dudas postventa

Ejemplo Completo del Caso "Ana":
  - Enfermera de 35 a√±os en Barcelona
  - Contratos temporales en sanidad p√∫blica
  - Salario: ~35.000‚Ç¨/a√±o
  - Problema: Inestabilidad laboral genera ansiedad
  - Soluci√≥n buscada: Preparar examen EIR para plaza fija
  - Lenguaje espec√≠fico: "EIRsilente", "rEIRsilente"
  - Objeciones: Empresa nueva sin estad√≠sticas de aprobados
  - Miedos: Dejar tel√©fono para conseguir info, llamadas insistentes

Uso en el C√≥digo:
  archivo: backend/src/agents/buyer_persona_agent.py
  
  implementaci√≥n: |
    class BuyerPersonaAgent:
        def __init__(self):
            # Cargar plantilla base al inicializar
            with open('contenido/buyer-plantilla.md', 'r', encoding='utf-8') as f:
                self.template_content = f.read()
            
            # Extraer estructura de preguntas para validaci√≥n
            self.required_categories = [
                'Aspectos Demogr√°ficos',
                'Hogar y Familia',
                'Trabajo',
                # ... todas las 11 categor√≠as
            ]
        
        def validate_analysis(self, analysis: Dict) -> bool:
            """Verificar que an√°lisis incluye todas las categor√≠as."""
            for category in self.required_categories:
                if category not in analysis:
                    return False
            return True
  
  en_prompt_del_agente: |
    # Inyectar estructura completa de la plantilla
    system_prompt = f"""
    {self.template_content}
    
    Responde TODAS las preguntas de la plantilla usando:
    - Respuestas iniciales del usuario
    - Documentos subidos (si existen)
    - Inferencia l√≥gica basada en contexto
    """

Valor para el Sistema:
  - Define nivel de detalle esperado del an√°lisis
  - Asegura consistencia en todos los buyer personas generados
  - Proporciona ejemplo real de calidad esperada
  - Cubre aspectos que otros an√°lisis omiten (lenguaje del grupo, miedos espec√≠ficos)
```

---

### RECURSO 2: Prompts Base del Agente (Borradores a Mejorar) üí¨

```yaml
Ubicaci√≥n: contenido/promts_borradores.md

Prop√≥sito:
  "Prompts iniciales que el agente usar√° como BASE y MEJORAR√Å din√°micamente"

Contenido: 3 prompts clave del sistema

PROMPT 1 - Generaci√≥n de Buyer Persona:
  contexto: |
    "Eres un experto en marketing digital con amplios conocimientos en mercadolog√≠a, 
    el contexto de la situaci√≥n es el siguiente, estas por comenzar una campa√±a 
    publicitaria en ADS con un plan de contenidos org√°nico para una empresa..."
  
  input_requerido:
    - Lo que ofrece la empresa: [descripci√≥n del producto/servicio]
    - P√∫blico objetivo: [descripci√≥n del cliente ideal]
    - Tipo de negocio: [B2B o B2P]
  
  instrucciones_clave:
    - "Debes ignorar las respuestas de cada pregunta [de la plantilla], solo √∫salas como gu√≠a"
    - "Responde todas las preguntas de manera completa y eficiente"
    - "Datos se usar√°n para campa√±as en Meta ADS y estrategia de content marketing"
    - "Respuestas basadas en la REALIDAD del p√∫blico (no manipular para favorecer)"
    - "Con datos reales podremos dar soluciones reales"
    - "Establece un paso a paso: analiza primero documento, luego enf√≥cate en necesidad real"
  
  mejoras_a_aplicar:
    - A√±adir secci√≥n de "DOCUMENTOS DEL USUARIO" si existen
    - Enriquecer con t√©cnicas de investigaci√≥n de mercado
    - Personalizar seg√∫n tipo de negocio espec√≠fico
    - Inyectar contexto de transcripciones (t√©cnicas de marketing)

PROMPT 2 - Simulaci√≥n de Foro:
  contexto: |
    "Bas√°ndonos en el buyer persona que me acabas de responder, quiero que ahora 
    tomes el papel de esa persona e imagines que est√°s en un foro de internet..."
  
  escenario:
    - Foro de internet donde personas se quejan/recomiendan servicios
    - El agente ACT√öA COMO el buyer persona
    - Genera quejas de problemas al contratar servicios similares
    - Despu√©s de cada queja: soluci√≥n deseada
  
  output_esperado:
    parte_1: "Array de {queja, solucion_deseada} (5-7 posts)"
    parte_2: "Lista de 10 puntos de dolor del personaje"
    detalles: "Todo lo que piensa y siente ANTES de comprar, criterios, comportamientos"
  
  mejoras_a_aplicar:
    - Enriquecer con contexto de documentos del usuario
    - Usar lenguaje espec√≠fico del buyer persona
    - A√±adir ejemplos de quejas reales (si documentos lo proporcionan)

PROMPT 3 - Customer Journey:
  contexto: |
    "Quiero que act√∫es como un experto en content marketing y bas√°ndote en 
    la documentaci√≥n que te acabo de adjunta de mi buyer personas y de su papel 
    en un foro..."
  
  input_base:
    - Buyer persona completo
    - Simulaci√≥n de foro con puntos de dolor
  
  output_requerido:
    - Customer Journey con 3 fases de conciencia EXTENDIDAS
  
  definiciones_de_fases:
    conciencia: |
      "Todo lo que el cliente hace hasta tomar conciencia de su necesidad.
      El consumidor est√° investigando opciones SIN intenci√≥n comercial."
    
    consideracion: |
      "Tienen 2-3 opciones, pasan a investigaci√≥n de cada una en profundidad.
      Puede tener presente alg√∫n inter√©s comercial."
    
    compra: |
      "El cliente decide comprar, sobre la base de sus investigaciones."
  
  formato_de_salida:
    - CJ con m√≠nimo 10 preguntas por cada fase
    - "Qu√© busca mi cliente ideal en internet" (10 preguntas)
    - "Qu√© quiere saber, qu√© pasa por su cabeza" (10 preguntas)
    - "Debes ser EXTENSO y conocer muy bien a quien le daremos este contenido"
  
  mejoras_a_aplicar:
    - Usar buyer persona + foro para ser espec√≠fico
    - Consultar knowledge base (transcripciones) para tipos de contenido efectivos
    - Personalizar seg√∫n fase del embudo de marketing

Uso en el C√≥digo:
  ubicaci√≥n: backend/src/agents/
  
  archivos_afectados:
    - buyer_persona_agent.py: Usa PROMPT 1 (mejorado)
    - forum_simulator_agent.py: Usa PROMPT 2 (mejorado)
    - customer_journey_agent.py: Usa PROMPT 3 (mejorado)
  
  patr√≥n_de_mejora: |
    class BuyerPersonaAgent:
        def __init__(self):
            # 1. Cargar prompt base desde archivo
            with open('contenido/promts_borradores.md', 'r') as f:
                prompts_raw = f.read()
            
            # 2. Extraer prompt espec√≠fico (parsear markdown)
            self.base_prompt_buyer_persona = self._extract_prompt(prompts_raw, "Promt de buyer persona")
            
            # 3. Mejorar din√°micamente seg√∫n contexto
            self.enhanced_prompt = self._enhance_prompt(
                base=self.base_prompt_buyer_persona,
                template=template_content,
                user_context=user_info,
                documents_context=docs_summary
            )
        
        def _enhance_prompt(self, base, template, user_context, documents_context):
            """
            Mejora el prompt base con:
            - Plantilla completa (contenido/buyer-plantilla.md)
            - Contexto del usuario
            - Informaci√≥n de documentos subidos
            - T√©cnicas de las transcripciones
            """
            return f"""
            {base}
            
            PLANTILLA COMPLETA A SEGUIR:
            {template}
            
            INFORMACI√ìN ADICIONAL DEL NEGOCIO:
            {documents_context}
            
            T√âCNICAS DE MARKETING A APLICAR:
            {marketing_techniques_from_knowledge_base}
            """

Estado Actual:
  - ‚úÖ Prompts base definidos y claros
  - ‚ö†Ô∏è Requieren mejora din√°mica con contexto
  - ‚ö†Ô∏è Deben integrarse con plantilla completa
  - ‚ö†Ô∏è Deben enriquecerse con transcripciones

Siguiente Paso:
  "Al implementar BuyerPersonaAgent (TAREA 4), cargar estos prompts y mejorarlos"
```

---

### RECURSO 3: Material de Entrenamiento - Transcripciones de Andrea Estratega üéì

```yaml
Ubicaci√≥n: contenido/Transcriptions Andrea Estratega/

Prop√≥sito:
  "Entrenar al agente en t√©cnicas REALES de creaci√≥n de contenido viral para redes sociales"

Contenido: 9 transcripciones de videos de YouTube:

1. "6 Carruseles en Instagram que te har√°n viral en 2025 (y su estructura).txt"
   Temas: Formatos de carruseles virales, estructura replicable
   Aplicaci√≥n: Cuando usuario pide ideas de carruseles para Instagram

2. "C√≥mo hacer 7 guiones virales en menos de 30 min CON UNA ESTRUCTURA copia y pega.txt"
   Temas: Templates de guiones r√°pidos, estructura copia-pega
   Aplicaci√≥n: Cuando usuario pide scripts de videos

3. "Domina el Storytelling de tu Rubro en 19 minutos (nunca lo olvidar√°s).txt"
   Temas: Storytelling aplicado, narrativa persuasiva por sector
   Aplicaci√≥n: Cuando usuario pide contenido narrativo

4. "El secreto detr√°s de los videos que no puedes dejar de ver (usan 1 solo formato).txt"
   Temas: Formatos adictivos, retenci√≥n de atenci√≥n, psicolog√≠a del engagement
   Aplicaci√≥n: Cuando usuario pide videos con alta retenci√≥n

5. "El sistema IA que Crea contenido para Redes sociales Todos los d√≠as (100% autom√°tico).txt"
   Temas: Automatizaci√≥n de contenido, sistemas IA, workflows
   Aplicaci√≥n: Cuando usuario pregunta por automatizaci√≥n

6. "El Top Embudo de Redes sociales Explicado en 16 Minutos  Contenido de Valor.txt"
   Temas: Top of funnel, contenido de valor, awareness phase
   Aplicaci√≥n: Cuando usuario pide contenido para fase de conciencia

7. "Estudi√© +50 formatos de video, te revel√≥ los 5 M√ÅS VIRALES.txt"
   Temas: An√°lisis de formatos virales, tendencias probadas
   Aplicaci√≥n: Cuando usuario pide formatos con mayor viralidad

8. "La forma m√°s R√ÅPIDA de crecer tu Instagram y Tiktok en 2026 (5 estrategias).txt"
   Temas: Crecimiento org√°nico 2026, estrategias actualizadas
   Aplicaci√≥n: Cuando usuario pide estrategia de crecimiento

9. "Todo lo que el CEO de Instagram dijo para 2026 (revela los secretos para crecer).txt"
   Temas: Algoritmo de Instagram oficial, tendencias 2026, secretos del CEO
   Aplicaci√≥n: Cuando usuario pregunta por algoritmo o tendencias 2026

Procesamiento T√©cnico:
  
  script: backend/scripts/ingest_training_data.py
  
  pasos:
    1. Leer cada archivo .txt de la carpeta
    2. Chunking con RecursiveCharacterTextSplitter:
       - chunk_size: 1000 tokens
       - chunk_overlap: 200 tokens
    3. Generar embeddings con OpenAI text-embedding-3-large
    4. Insertar en marketing_knowledge_base:
       configuraci√≥n:
         - project_id: NULL (conocimiento GLOBAL, no de un proyecto espec√≠fico)
         - chat_id: NULL (conocimiento GLOBAL, no de un chat espec√≠fico)
         - content_type: 'video_transcript'
         - source_title: nombre del archivo (ej: "6 Carruseles en Instagram...")
         - chunk_text: texto del chunk
         - chunk_index: √≠ndice del chunk en el documento
         - metadata: {"autor": "Andrea Estratega", "tema": "content_marketing", "a√±o": "2025-2026"}
         - embedding: vector de 1536 dimensiones
  
  comando_de_ingesta: |
    python backend/scripts/ingest_training_data.py \
      --source "contenido/Transcriptions Andrea Estratega/" \
      --content-type video_transcript \
      --metadata '{"autor":"Andrea Estratega","tema":"content_marketing"}'
  
  verificaci√≥n: |
    # Verificar chunks en DB
    SELECT 
      COUNT(*) as total_chunks,
      source_title,
      COUNT(DISTINCT chunk_index) as chunks_per_doc
    FROM marketing_knowledge_base
    WHERE content_type = 'video_transcript'
      AND metadata->>'autor' = 'Andrea Estratega'
    GROUP BY source_title;
    
    # Esperado: 
    # - 9 documentos (uno por archivo)
    # - ~200-500 chunks totales (dependiendo de longitud)
    # - ~20-60 chunks por documento promedio

Uso en el Agente (ContentGeneratorAgent):
  
  flujo_de_b√∫squeda: |
    # Cuando usuario pide: "Dame 5 ideas de videos para fase de conciencia"
    
    1. Construir query de b√∫squeda:
       query = "ideas videos contenido fase conciencia awareness engagement"
    
    2. Generar embedding del query:
       query_embedding = await embedding_service.generate_embedding(query)
    
    3. Buscar en knowledge_base:
       results = await vector_search.search(
           query_embedding=query_embedding,
           project_id=user.project_id,  # Incluir√° tambi√©n docs del usuario
           chat_id=chat_id,
           match_count=10
       )
       # Retorna:
       # - Chunks de transcripciones de Andrea (project_id NULL)
       # - Chunks de documentos del usuario (project_id, chat_id espec√≠ficos)
    
    4. Filtrar transcripciones:
       transcription_chunks = [r for r in results if r['content_type'] == 'video_transcript']
    
    5. Inyectar en prompt:
       context = "\n\n".join([
           f"T√âCNICA DE ANDREA ESTRATEGA ({chunk['source_title']}):\n{chunk['content']}"
           for chunk in transcription_chunks[:5]
       ])
       
       prompt = f"""
       BUYER PERSONA DEL USUARIO:
       {buyer_persona}
       
       CUSTOMER JOURNEY (Fase Conciencia):
       {customer_journey['awareness']}
       
       T√âCNICAS DE CREACI√ìN DE CONTENIDO:
       {context}
       
       Genera 5 ideas de videos para fase de conciencia combinando:
       - El perfil espec√≠fico del buyer persona
       - Las t√©cnicas virales de las transcripciones
       - El comportamiento del customer journey
       """

Valor Diferencial:
  - "Este es el conocimiento que diferencia al agente de ChatGPT gen√©rico"
  - "T√©cnicas probadas de creaci√≥n de contenido viral"
  - "Contexto espec√≠fico de redes sociales 2025-2026"
  - "Estrategias basadas en algoritmos reales de Instagram/TikTok"
  - "Material de Andrea Estratega (experta reconocida)"

CR√çTICO - Sin estas transcripciones:
  - El agente ser√≠a un chatbot gen√©rico
  - Respuestas ser√≠an te√≥ricas, no pr√°cticas
  - No tendr√≠a t√©cnicas actualizadas de 2026
  - No conocer√≠a formatos virales probados
```

---

### RECURSO 4: Integraci√≥n de los 3 Recursos en el Flujo del Agente

```yaml
Flujo Completo de Uso:

ETAPA 1 - An√°lisis Inicial:
  input_usuario:
    - 4-5 respuestas a preguntas iniciales
    - (Opcional) Documentos subidos (.txt, .pdf, .docx)
  
  proceso_agente:
    1. Cargar RECURSO 2 (Prompt 1 de buyer persona)
    2. Mejorar prompt con:
       - Respuestas del usuario
       - Info de documentos subidos (si existen)
    3. Usar RECURSO 1 (Plantilla completa) como estructura
    4. Generar an√°lisis completo
    5. Validar que cubre las 11 categor√≠as
  
  output:
    - Buyer persona completo en JSON (almacenar en DB)
    - Documento formateado para el usuario (markdown)

ETAPA 2 - Simulaci√≥n y Extracci√≥n:
  proceso_agente:
    1. Cargar RECURSO 2 (Prompt 2 de foro)
    2. Usar buyer persona generado
    3. Simular comportamiento en foro
    4. Extraer 10 puntos de dolor
  
  output:
    - Forum simulation en JSON
    - Pain points en array de 10 strings

ETAPA 3 - Customer Journey:
  proceso_agente:
    1. Cargar RECURSO 2 (Prompt 3 de CJ)
    2. Usar buyer persona + puntos de dolor
    3. Consultar RECURSO 3 (transcripciones) para tipos de contenido efectivos por fase
    4. Generar 3 fases con m√≠nimo 10 preguntas cada una
  
  output:
    - Customer Journey en JSON (3 fases)
    - Documento completo formateado

ETAPA 4 - Generaci√≥n de Contenido (On-Demand):
  trigger: "Usuario hace petici√≥n expl√≠cita"
  
  ejemplo_petici√≥n: "Dame 5 ideas de videos para fase de conciencia"
  
  proceso_agente:
    1. Recuperar de DB:
       - RECURSO 1 (buyer persona del chat)
       - Customer Journey (fase awareness)
    
    2. Buscar en RECURSO 3 (knowledge base):
       query: "ideas videos contenido conciencia viral"
       resultado: Top-10 chunks de transcripciones relevantes
    
    3. Combinar contextos:
       - Perfil del buyer persona (necesidades, lenguaje, problemas)
       - Fase del customer journey (qu√© busca, qu√© piensa)
       - T√©cnicas de transcripciones (formatos virales, hooks, estructuras)
    
    4. Generar contenido personalizado:
       - 5 ideas de videos
       - Cada idea con: t√≠tulo, hook, estructura, CTA
       - Adaptado espec√≠ficamente al buyer persona
       - Usando t√©cnicas probadas de Andrea Estratega

C√≥digo de Integraci√≥n (en ContentGeneratorAgent):
  
  m√©todo_principal: |
    async def generate_content_ideas(
        self,
        chat_id: UUID,
        project_id: UUID,
        phase: str,  # 'awareness' | 'consideration' | 'purchase'
        content_type: str,  # 'video' | 'post' | 'article'
        count: int = 5
    ) -> List[Dict]:
        # 1. Recuperar contexto de largo plazo
        buyer_persona = await self.memory.get_long_term_context(chat_id, project_id)
        
        # 2. Buscar t√©cnicas en transcripciones (RECURSO 3)
        search_query = f"{content_type} {phase} viral t√©cnicas"
        techniques = await self.memory.get_semantic_context(
            query=search_query,
            project_id=project_id,
            chat_id=chat_id,
            k=10
        )
        
        # Filtrar solo transcripciones
        andrea_techniques = [
            t for t in techniques 
            if t['content_type'] == 'video_transcript'
        ]
        
        # 3. Construir prompt enriquecido
        prompt = self._build_enhanced_prompt(
            buyer_persona=buyer_persona,
            phase=phase,
            techniques=andrea_techniques[:5],
            count=count
        )
        
        # 4. Generar con Claude
        response = await self.llm.generate(prompt)
        
        return response

Valor de la Integraci√≥n:
  - Buyer persona espec√≠fico del usuario (RECURSO 1)
  + Prompts optimizados y mejorados (RECURSO 2)
  + T√©cnicas probadas de contenido viral (RECURSO 3)
  = Contenido ULTRA-PERSONALIZADO y ACCIONABLE
```

---

## üìù Lista de Tareas Detalladas

### TAREA 0: Instalar y Configurar MCP Serena (‚ö° OBLIGATORIO)

**Herramientas a utilizar:**
- ‚ö° MCP Archon: Documentaci√≥n de MCPs
  - Importancia: "Consultar instalaci√≥n de MCPs en Cursor"
  - Comando: `rag_search_knowledge_base(query="MCP installation", match_count=3)`

- üîß MCP Serena: Onboarding del proyecto
  - Importancia: "Activar para an√°lisis simb√≥lico de c√≥digo"

- üìö Skill environment-setup-guide: Setup correcto
  - Importancia: "Gu√≠a de configuraci√≥n de entorno"

**Objetivo:**
Instalar y verificar que Serena est√° activo para an√°lisis simb√≥lico del c√≥digo sin leer archivos completos.

**Pasos a seguir:**
1. Verificar que Serena est√° en configuraci√≥n de Cursor (`~/.cursor/mcp-config.json`)
2. Activar Serena en el proyecto actual
3. Ejecutar onboarding si es necesario
4. Probar con `get_symbols_overview('Context-Engineering-Intro/examples/')`

**Criterios de aceptaci√≥n:**
- [ ] Serena activo y respondiendo comandos
- [ ] Puede ejecutar `get_symbols_overview`
- [ ] Onboarding completado si era necesario

---

### TAREA 1: Configurar Base de Datos en Supabase

**Herramientas a utilizar:**
- ‚ö° MCP Archon: Documentaci√≥n de Supabase + pgvector
  - Importancia: "Necesitamos docs oficiales sobre pgvector y RLS en Supabase"
  - Comando: `rag_search_knowledge_base(query="supabase pgvector create index", source_id="9c5f534e51ee9237", match_count=8)`
  - Comando: `rag_search_knowledge_base(query="supabase row level security", source_id="9c5f534e51ee9237", match_count=5)`

- üîß MCP Serena: An√°lisis de estructura de ejemplo (si existe migration de referencia)
  - Comando: `get_symbols_overview('Context-Engineering-Intro/examples/mcp-server/src/database/')`

- üìö Skill postgres-best-practices: Optimizaci√≥n de queries
  - Importancia: "Patrones de performance para Postgres"

**Objetivo:**
Configurar Supabase con todas las tablas, extensiones, √≠ndices y funciones necesarias para el proyecto.

**Pasos a seguir:**

1. **Habilitar extensi√≥n pgvector:**
   ```sql
   CREATE EXTENSION IF NOT EXISTS vector;
   CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
   ```

2. **Crear 8 tablas con prefijo `marketing_`:**
   - Usar esquema definido en secci√≥n "Modelos de Datos"
   - **CR√çTICO**: Incluir `project_id` en TODAS las tablas
   - Crear √≠ndices seg√∫n especificado
   - Configurar RLS (Row Level Security) en todas las tablas

3. **Crear funci√≥n `marketing_match_documents` para b√∫squeda vectorial:**
   ```sql
   CREATE OR REPLACE FUNCTION marketing_match_documents(
     query_embedding VECTOR(1536),
     match_threshold FLOAT,
     match_count INT,
     filter_project_id UUID,
     filter_chat_id UUID DEFAULT NULL
   )
   RETURNS TABLE (
     id UUID,
     content TEXT,
     content_type VARCHAR,
     source_title VARCHAR,
     similarity FLOAT
   )
   LANGUAGE plpgsql
   AS $$
   BEGIN
     RETURN QUERY
     SELECT
       kb.id,
       kb.chunk_text,
       kb.content_type,
       kb.source_title,
       1 - (kb.embedding <=> query_embedding) AS similarity
     FROM marketing_knowledge_base kb
     WHERE 
       (
         -- Conocimiento global (YouTubers + libros)
         (kb.project_id IS NULL AND kb.chat_id IS NULL)
         
         OR
         
         -- Documentos del usuario en este chat espec√≠fico
         (kb.project_id = filter_project_id AND kb.chat_id = filter_chat_id)
       )
       AND 1 - (kb.embedding <=> query_embedding) > match_threshold
     ORDER BY kb.embedding <=> query_embedding
     LIMIT match_count;
   END;
   $$;
   ```

4. **Crear proyecto de prueba:**
   ```sql
   INSERT INTO marketing_projects (id, name, created_at, updated_at)
   VALUES (uuid_generate_v4(), 'Test Project', NOW(), NOW());
   ```

5. **Configurar pol√≠ticas RLS:**
   ```sql
   -- Ejemplo para marketing_chats
   ALTER TABLE marketing_chats ENABLE ROW LEVEL SECURITY;
   
   CREATE POLICY "Users can only see their own chats"
   ON marketing_chats
   FOR SELECT
   USING (user_id = auth.uid() AND project_id = current_project());
   
   -- Repetir para todas las tablas
   ```

**‚ö†Ô∏è GOTCHA CR√çTICO APLICADO:**
**GOTCHA 1 - pgvector index**: Usar HNSW en vez de ivfflat porque:
- IVFFlat requiere >1000 documentos para ser efectivo
- HNSW funciona bien con cualquier cantidad de datos
- Performance superior en general (validado por Supabase docs)

**SQL para √≠ndices vectoriales (usar HNSW):**
```sql
-- ‚úÖ CORRECTO: HNSW index (funciona con <1000 docs)
CREATE INDEX idx_knowledge_base_embedding_hnsw 
ON marketing_knowledge_base 
USING hnsw (embedding vector_cosine_ops);

CREATE INDEX idx_buyer_personas_embedding_hnsw 
ON marketing_buyer_personas 
USING hnsw (embedding vector_cosine_ops);

-- ‚ùå NO USAR: ivfflat (solo efectivo con >1000 docs)
-- CREATE INDEX ... USING ivfflat ...
```

**Criterios de aceptaci√≥n:**
- [ ] Extensiones pgvector y uuid-ossp habilitadas
- [ ] 8 tablas creadas con prefijo `marketing_`
- [ ] ‚úÖ √çndices HNSW creados (NO ivfflat) para embeddings
- [ ] Funci√≥n `marketing_match_documents` funcional
- [ ] Proyecto de prueba creado exitosamente
- [ ] RLS configurado en todas las tablas
- [ ] Test: `SELECT * FROM marketing_match_documents('[0.1, 0.2, ...]', 0.7, 10, <project_id>)` retorna sin error

**Archivos a crear:**
- `backend/db/migrations/001_initial_schema.sql` (tablas + √≠ndices)
- `backend/db/migrations/002_create_match_function.sql` (funci√≥n vectorial)
- `backend/db/migrations/003_configure_rls.sql` (pol√≠ticas RLS)
- `backend/db/seed_data.sql` (proyecto de prueba)

**Comandos de validaci√≥n:**
```bash
# Ejecutar desde psql o Supabase SQL Editor
psql $DATABASE_URL -f backend/db/migrations/001_initial_schema.sql
psql $DATABASE_URL -f backend/db/migrations/002_create_match_function.sql
psql $DATABASE_URL -f backend/db/migrations/003_configure_rls.sql

# Verificar tablas creadas
psql $DATABASE_URL -c "\dt marketing_*"

# Verificar funci√≥n
psql $DATABASE_URL -c "SELECT routine_name FROM information_schema.routines WHERE routine_name LIKE 'marketing_%';"
```

---

### TAREA 2: Setup Backend con FastAPI + Autenticaci√≥n Manual

**Herramientas a utilizar:**
- ‚ö° MCP Archon: FastAPI + Pydantic
  - Comando: `rag_search_knowledge_base(query="fastapi project structure async", source_id="c889b62860c33a44", match_count=5)`
  - Comando: `rag_search_knowledge_base(query="pydantic v2 model validation", source_id="9d46e91458092424", match_count=5)`
  - Comando: `rag_search_code_examples(query="fastapi jwt authentication", source_id="c889b62860c33a44", match_count=3)`

- üîß MCP Serena: Analizar estructura de ejemplo de agente
  - Comando: `get_symbols_overview('Context-Engineering-Intro/examples/main_agent_reference/research_agent.py')`

- üìö Skills:
  - python-patterns (autom√°tico)
  - clean-code (autom√°tico)

**Objetivo:**
Configurar FastAPI con estructura modular, SQLAlchemy + Alembic, y sistema de autenticaci√≥n manual con JWT.

**Pasos a seguir:**

1. **Consultar Archon sobre estructura de proyecto FastAPI:**
   - Ejecutar comandos de Archon listados arriba
   - Tomar notas sobre mejores pr√°cticas

2. **Crear estructura de carpetas:**
   ```bash
   mkdir -p backend/src/{api,agents,db,services,schemas,utils}
   mkdir -p backend/tests/{unit,integration,fixtures}
   mkdir -p backend/scripts
   touch backend/pyproject.toml backend/.env backend/README.md
   ```

3. **Configurar pyproject.toml con dependencias:**
   ```toml
   [project]
   name = "marketing-brain-backend"
   version = "0.1.0"
   dependencies = [
       "fastapi>=0.109.0",
       "uvicorn[standard]>=0.27.0",
       "pydantic>=2.5.0",
       "pydantic-settings>=2.1.0",
       "sqlalchemy>=2.0.25",
       "alembic>=1.13.0",
       "asyncpg>=0.29.0",
       "langchain>=0.1.0",
       "langgraph>=0.0.20",
       "anthropic>=0.18.0",
       "openai>=1.12.0",
       "pgvector>=0.2.4",
       "bcrypt>=4.1.2",
       "pyjwt>=2.8.0",
       "python-multipart>=0.0.9",
       "redis>=5.0.1",
       "pypdf2>=3.0.1",
       "python-docx>=1.1.0",
   ]
   ```

4. **Implementar modelos SQLAlchemy:**
   - Crear `backend/src/db/models.py` con modelos de secci√≥n "Modelos de Datos"
   - Usar async patterns con SQLAlchemy 2.0

5. **Implementar schemas Pydantic:**
   - `backend/src/schemas/auth.py` (RegisterRequest, LoginRequest, etc.)
   - Usar Field descriptions detalladas
   - Agregar validadores custom

6. **Implementar autenticaci√≥n manual:**
   
   **`backend/src/api/auth.py`:**
   ```python
   from fastapi import APIRouter, HTTPException, Depends
   from ..schemas.auth import RegisterRequest, LoginRequest, LoginResponse
   from ..utils.password import hash_password, verify_password
   from ..utils.jwt import create_access_token
   
   router = APIRouter(prefix="/api/auth", tags=["auth"])
   
   @router.post("/register")
   async def register(request: RegisterRequest):
       # 1. Verificar que email no existe en proyecto
       # 2. Hash password con bcrypt
       # 3. Crear usuario en DB
       # 4. Retornar user info (sin password)
       pass
   
   @router.post("/login", response_model=LoginResponse)
   async def login(request: LoginRequest):
       # 1. Buscar usuario por email + project_id
       # 2. Verificar password
       # 3. Generar JWT con user_id, email, project_id
       # 4. Retornar token + user info
       pass
   
   @router.post("/request-password-reset")
   async def request_password_reset(request: RequestPasswordResetRequest):
       # 1. Buscar usuario
       # 2. Generar token √∫nico (UUID)
       # 3. Guardar en marketing_password_reset_tokens (expires_at = 1 hora)
       # 4. Retornar token (en producci√≥n: enviar link m√°gico)
       pass
   
   @router.post("/reset-password")
   async def reset_password(request: ResetPasswordRequest):
       # 1. Validar token (existe, no usado, no expirado)
       # 2. Hash nuevo password
       # 3. Actualizar password de usuario
       # 4. Marcar token como usado
       pass
   ```
   
   **`backend/src/utils/password.py`:**
   ```python
   import bcrypt
   
   def hash_password(password: str) -> str:
       salt = bcrypt.gensalt(rounds=12)
       return bcrypt.hashpw(password.encode('utf-8'), salt).decode('utf-8')
   
   def verify_password(password: str, hashed: str) -> bool:
       return bcrypt.checkpw(password.encode('utf-8'), hashed.encode('utf-8'))
   ```
   
   **`backend/src/utils/jwt.py`:**
   ```python
   import jwt
   from datetime import datetime, timedelta
   from typing import Dict
   import os
   
   SECRET_KEY = os.getenv('JWT_SECRET_KEY')
   ALGORITHM = "HS256"
   
   def create_access_token(data: Dict, expires_delta: timedelta = timedelta(days=7)) -> str:
       to_encode = data.copy()
       expire = datetime.utcnow() + expires_delta
       to_encode.update({"exp": expire})
       return jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)
   
   def decode_access_token(token: str) -> Dict:
       try:
           payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
           return payload
       except jwt.ExpiredSignatureError:
           raise HTTPException(status_code=401, detail="Token expired")
       except jwt.JWTError:
           raise HTTPException(status_code=401, detail="Invalid token")
   ```

7. **Implementar middleware de autenticaci√≥n:**
   
   **‚ö†Ô∏è GOTCHA CR√çTICO APLICADO:**
   **GOTCHA 6 - Supabase RLS bypass**: Service role key NO respeta RLS policies.
   - Soluci√≥n: Validaci√≥n manual de project_id en TODAS las queries
   - Middleware inyecta project_id desde JWT en request.state
   - NUNCA confiar solo en RLS para aislamiento multi-tenant
   
   **`backend/src/middleware/auth.py`:**
   ```python
   from fastapi import Request, HTTPException
   from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
   from ..utils.jwt import decode_access_token
   
   security = HTTPBearer()
   
   async def get_current_user(credentials: HTTPAuthorizationCredentials = Depends(security)):
       token = credentials.credentials
       payload = decode_access_token(token)
       return payload  # {user_id, email, project_id}
   
   async def inject_project_id(request: Request, call_next):
       """
       ‚ö†Ô∏è CR√çTICO PARA GOTCHA 6
       
       Middleware que inyecta project_id desde JWT en request.state
       para asegurar aislamiento multi-tenant.
       
       Supabase Service Role Key bypasea RLS, por lo que DEBEMOS
       validar project_id manualmente en TODAS las queries.
       """
       public_paths = ["/api/auth/", "/health", "/docs", "/openapi.json"]
       
       if not any(path in request.url.path for path in public_paths):
           # Extraer token de header Authorization
           auth_header = request.headers.get("Authorization")
           if not auth_header or not auth_header.startswith("Bearer "):
               raise HTTPException(status_code=401, detail="Missing or invalid token")
           
           token = auth_header.split(" ")[1]
           payload = decode_access_token(token)
           
           # ‚úÖ Inyectar en request.state (disponible en todos los endpoints)
           request.state.user_id = payload['user_id']
           request.state.project_id = payload['project_id']  # ‚Üê CR√çTICO
           request.state.email = payload['email']
       
       response = await call_next(request)
       return response
   ```
   
   **Ejemplo de uso en endpoint (SIEMPRE incluir project_id):**
   ```python
   @router.get("/api/chats")
   async def get_user_chats(request: Request):
       user_id = request.state.user_id
       project_id = request.state.project_id  # Del JWT, NO del input del usuario
       
       # ‚úÖ CORRECTO: Query con project_id expl√≠cito
       chats = await db.query(
           "SELECT * FROM marketing_chats WHERE user_id = $1 AND project_id = $2",
           user_id, project_id
       )
       
       # ‚ùå INCORRECTO: Query sin project_id
       # chats = await db.query("SELECT * FROM marketing_chats WHERE user_id = $1", user_id)
       
       return {"chats": chats}
   ```

8. **Configurar main.py:**
   ```python
   from fastapi import FastAPI
   from fastapi.middleware.cors import CORSMiddleware
   from .api import auth
   from .middleware.auth import inject_project_id
   
   app = FastAPI(title="Marketing Brain API", version="1.0.0")
   
   # CORS
   app.add_middleware(
       CORSMiddleware,
       allow_origins=["http://localhost:3000"],
       allow_credentials=True,
       allow_methods=["*"],
       allow_headers=["*"],
   )
   
   # Custom middleware
   app.middleware("http")(inject_project_id)
   
   # Routers
   app.include_router(auth.router)
   
   @app.get("/health")
   async def health():
       return {"status": "ok"}
   ```

**Criterios de aceptaci√≥n:**
- [ ] Estructura de carpetas creada
- [ ] pyproject.toml con todas las dependencias
- [ ] Modelos SQLAlchemy creados (8 tablas)
- [ ] Schemas Pydantic creados (auth)
- [ ] Endpoints de auth implementados:
  - [ ] POST /api/auth/register
  - [ ] POST /api/auth/login
  - [ ] POST /api/auth/request-password-reset
  - [ ] POST /api/auth/reset-password
- [ ] Utils de password y JWT funcionan
- [ ] Middleware de autenticaci√≥n inyecta project_id
- [ ] Sin errores de linting: `ruff check backend/src/`
- [ ] Sin errores de tipos: `mypy backend/src/`

**Archivos a crear:**
- `backend/pyproject.toml`
- `backend/src/main.py`
- `backend/src/db/models.py`
- `backend/src/schemas/auth.py`
- `backend/src/api/auth.py`
- `backend/src/utils/password.py`
- `backend/src/utils/jwt.py`
- `backend/src/middleware/auth.py`

**Comandos de validaci√≥n:**
```bash
# Linting
ruff check backend/src/ --fix

# Type checking
mypy backend/src/

# Tests de auth
pytest backend/tests/test_auth.py -v

# Test manual (con servidor corriendo)
curl -X POST http://localhost:8000/api/auth/register \
  -H "Content-Type: application/json" \
  -d '{"email":"test@example.com","password":"Test1234","full_name":"Test User","project_id":"<uuid>"}'
```

---

### TAREA 3: Sistema de Chat B√°sico (sin IA todav√≠a)

**Herramientas a utilizar:**
- ‚ö° MCP Archon: FastAPI + SQLAlchemy async
  - Comando: `rag_search_knowledge_base(query="fastapi dependency injection", source_id="c889b62860c33a44", match_count=5)`
  - Comando: `rag_search_code_examples(query="sqlalchemy async session", source_id="9c5f534e51ee9237", match_count=3)`

- üîß MCP Serena: Analizar implementaci√≥n de auth de TAREA 2
  - Comando: `get_symbols_overview('backend/src/api/auth.py')`
  - Comando: `find_symbol('get_current_user', 'backend/src/middleware/auth.py', True)`

- üìö Skills: clean-code, python-patterns

**Objetivo:**
Implementar CRUD de chats y mensajes (sin IA todav√≠a), con filtrado estricto por `project_id`.

**Pasos a seguir:**

1. **Crear schemas para chat:**
   
   **`backend/src/schemas/chat.py`:**
   - Usar esquema de secci√≥n "Pydantic Schemas"
   - CreateChatRequest, CreateChatResponse, SendMessageRequest, Message, ChatSummary

2. **Implementar servicio de chat:**
   
   **`backend/src/services/chat_service.py`:**
   ```python
   from sqlalchemy.ext.asyncio import AsyncSession
   from uuid import UUID
   from ..db.models import MarketingChat, MarketingMessage
   from ..schemas.chat import CreateChatRequest, Message
   
   class ChatService:
       def __init__(self, db: AsyncSession):
           self.db = db
       
       async def create_chat(self, user_id: UUID, project_id: UUID, title: str = "New Chat"):
           """
           Crear nuevo chat.
           
           CR√çTICO: Incluir project_id para aislamiento.
           """
           chat = MarketingChat(
               user_id=user_id,
               project_id=project_id,
               title=title
           )
           self.db.add(chat)
           await self.db.commit()
           await self.db.refresh(chat)
           return chat
       
       async def list_chats(self, user_id: UUID, project_id: UUID):
           """
           Listar chats del usuario.
           
           CR√çTICO: Filtrar por project_id.
           """
           query = select(MarketingChat).where(
               MarketingChat.user_id == user_id,
               MarketingChat.project_id == project_id
           ).order_by(MarketingChat.created_at.desc())
           
           result = await self.db.execute(query)
           return result.scalars().all()
       
       async def get_chat(self, chat_id: UUID, user_id: UUID, project_id: UUID):
           """
           Obtener chat espec√≠fico.
           
           CR√çTICO: Validar que pertenece al usuario y proyecto.
           """
           query = select(MarketingChat).where(
               MarketingChat.id == chat_id,
               MarketingChat.user_id == user_id,
               MarketingChat.project_id == project_id
           )
           result = await self.db.execute(query)
           chat = result.scalar_one_or_none()
           if not chat:
               raise HTTPException(404, "Chat not found")
           return chat
       
       async def update_chat_title(self, chat_id: UUID, user_id: UUID, project_id: UUID, new_title: str):
           chat = await self.get_chat(chat_id, user_id, project_id)
           chat.title = new_title
           await self.db.commit()
           return chat
       
       async def delete_chat(self, chat_id: UUID, user_id: UUID, project_id: UUID):
           chat = await self.get_chat(chat_id, user_id, project_id)
           await self.db.delete(chat)
           await self.db.commit()
       
       async def get_messages(self, chat_id: UUID, user_id: UUID, project_id: UUID):
           """
           Obtener mensajes del chat.
           
           CR√çTICO: Validar que chat pertenece al usuario y proyecto.
           """
           # Primero validar acceso al chat
           await self.get_chat(chat_id, user_id, project_id)
           
           query = select(MarketingMessage).where(
               MarketingMessage.chat_id == chat_id
           ).order_by(MarketingMessage.created_at.asc())
           
           result = await self.db.execute(query)
           return result.scalars().all()
       
       async def create_message(self, chat_id: UUID, user_id: UUID, project_id: UUID, role: str, content: str):
           # Validar acceso
           await self.get_chat(chat_id, user_id, project_id)
           
           message = MarketingMessage(
               chat_id=chat_id,
               project_id=project_id,
               role=role,
               content=content
           )
           self.db.add(message)
           await self.db.commit()
           await self.db.refresh(message)
           return message
   ```

3. **Implementar endpoints de chat:**
   
   **`backend/src/api/chat.py`:**
   ```python
   from fastapi import APIRouter, Depends, HTTPException
   from uuid import UUID
   from ..schemas.chat import *
   from ..services.chat_service import ChatService
   from ..middleware.auth import get_current_user
   
   router = APIRouter(prefix="/api/chats", tags=["chat"])
   
   @router.get("/")
   async def list_chats(current_user: dict = Depends(get_current_user)):
       service = ChatService(db)
       chats = await service.list_chats(
           user_id=current_user['user_id'],
           project_id=current_user['project_id']
       )
       return {"chats": [ChatSummary.model_validate(c) for c in chats]}
   
   @router.post("/", response_model=CreateChatResponse)
   async def create_chat(
       request: CreateChatRequest,
       current_user: dict = Depends(get_current_user)
   ):
       service = ChatService(db)
       chat = await service.create_chat(
           user_id=current_user['user_id'],
           project_id=current_user['project_id'],
           title=request.title
       )
       return CreateChatResponse.model_validate(chat)
   
   @router.patch("/{chat_id}")
   async def update_chat(
       chat_id: UUID,
       title: str,
       current_user: dict = Depends(get_current_user)
   ):
       service = ChatService(db)
       chat = await service.update_chat_title(
           chat_id, current_user['user_id'], current_user['project_id'], title
       )
       return {"success": True, "chat": chat}
   
   @router.delete("/{chat_id}")
   async def delete_chat(
       chat_id: UUID,
       current_user: dict = Depends(get_current_user)
   ):
       service = ChatService(db)
       await service.delete_chat(chat_id, current_user['user_id'], current_user['project_id'])
       return {"success": True}
   
   @router.get("/{chat_id}/messages")
   async def get_messages(
       chat_id: UUID,
       current_user: dict = Depends(get_current_user)
   ):
       service = ChatService(db)
       messages = await service.get_messages(chat_id, current_user['user_id'], current_user['project_id'])
       return {"messages": [Message.model_validate(m) for m in messages]}
   
   @router.post("/{chat_id}/messages")
   async def send_message(
       chat_id: UUID,
       request: SendMessageRequest,
       current_user: dict = Depends(get_current_user)
   ):
       service = ChatService(db)
       
       # 1. Guardar mensaje del usuario
       user_msg = await service.create_message(
           chat_id, current_user['user_id'], current_user['project_id'],
           role="user", content=request.content
       )
       
       # 2. (Por ahora) Respuesta dummy del asistente
       assistant_msg = await service.create_message(
           chat_id, current_user['user_id'], current_user['project_id'],
           role="assistant", content="Echo: " + request.content
       )
       
       return {"user_message": Message.model_validate(user_msg), "assistant_message": Message.model_validate(assistant_msg)}
   ```

4. **Registrar router en main.py:**
   ```python
   from .api import auth, chat
   
   app.include_router(auth.router)
   app.include_router(chat.router)
   ```

**Criterios de aceptaci√≥n:**
- [ ] ChatService implementado con m√©todos CRUD
- [ ] Todos los m√©todos filtran por `project_id`
- [ ] Endpoints funcionan:
  - [ ] GET /api/chats (listar)
  - [ ] POST /api/chats (crear)
  - [ ] PATCH /api/chats/{id} (editar t√≠tulo)
  - [ ] DELETE /api/chats/{id}
  - [ ] GET /api/chats/{id}/messages
  - [ ] POST /api/chats/{id}/messages
- [ ] Tests con pytest
- [ ] Verificar aislamiento entre proyectos (crear 2 proyectos de prueba)
- [ ] Sin errores de linting/tipos

**Archivos a crear:**
- `backend/src/schemas/chat.py`
- `backend/src/services/chat_service.py`
- `backend/src/api/chat.py`
- `backend/tests/test_chat.py`

**Comandos de validaci√≥n:**
```bash
# Tests
pytest backend/tests/test_chat.py -v

# Test manual (servidor corriendo + token JWT)
curl -X POST http://localhost:8000/api/chats \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"title":"Test Chat"}'

curl -X GET http://localhost:8000/api/chats \
  -H "Authorization: Bearer $TOKEN"
```

---

### TAREA 3.5: Procesamiento de Documentos del Usuario (NUEVO)

**Herramientas a utilizar:**
- ‚ö° MCP Archon: FastAPI file upload + Python parsers
  - Comando: `rag_search_knowledge_base(query="fastapi file upload validation", source_id="c889b62860c33a44", match_count=5)`
  - Comando: `rag_search_knowledge_base(query="python pdf parsing text extraction", match_count=5)`
  - Comando: `rag_search_knowledge_base(query="langchain text splitter", source_id="e74f94bb9dcb14aa", match_count=5)`

- üîß MCP Serena: Analizar servicio de chat de TAREA 3
  - Comando: `get_symbols_overview('backend/src/services/chat_service.py')`

- üìö Skills:
  - python-patterns
  - clean-code

**Objetivo:**
Permitir que usuarios suban archivos (.txt, .pdf, .docx) con informaci√≥n de su negocio, procesarlos, hacer chunking, generar embeddings y almacenarlos en `marketing_knowledge_base` con `project_id` y `chat_id` para b√∫squeda sem√°ntica posterior.

**Pasos a seguir:**

1. **Consultar Archon sobre parsers y chunking:**
   - Ejecutar comandos de Archon listados arriba
   - Entender estrategias de chunking (RecursiveCharacterTextSplitter)

2. **Crear schemas para documentos:**
   
   **`backend/src/schemas/documents.py`:**
   - Usar esquema de secci√≥n "Pydantic Schemas"

3. **Implementar parsers de archivos:**
   
   **`backend/src/utils/file_parsers.py`:**
   ```python
   from pathlib import Path
   import PyPDF2
   import docx
   
   async def parse_txt(file_path: Path) -> str:
       """Parse .txt file and return content."""
       with open(file_path, 'r', encoding='utf-8') as f:
           return f.read()
   
   async def parse_pdf(file_path: Path) -> str:
       """Parse .pdf file and extract text."""
       text = []
       with open(file_path, 'rb') as f:
           reader = PyPDF2.PdfReader(f)
           for page in reader.pages:
               text.append(page.extract_text())
       return "\n".join(text)
   
   async def parse_docx(file_path: Path) -> str:
       """Parse .docx file and extract text."""
       doc = docx.Document(file_path)
       return "\n".join([para.text for para in doc.paragraphs])
   
   async def parse_document(file_path: Path, file_type: str) -> str:
       """Route to appropriate parser based on file type."""
       parsers = {
           '.txt': parse_txt,
           '.pdf': parse_pdf,
           '.docx': parse_docx,
       }
       parser = parsers.get(file_type)
       if not parser:
           raise ValueError(f"Unsupported file type: {file_type}")
       return await parser(file_path)
   ```

4. **Implementar servicio de procesamiento de documentos:**
   
   **`backend/src/services/document_processor.py`:**
   ```python
   from pathlib import Path
   from uuid import UUID
   from langchain.text_splitter import RecursiveCharacterTextSplitter
   from ..utils.file_parsers import parse_document
   from ..services.embedding_service import EmbeddingService
   from ..db.models import MarketingKnowledgeBase, MarketingUserDocument
   
   class DocumentProcessor:
       def __init__(self, db, embedding_service: EmbeddingService):
           self.db = db
           self.embedding_service = embedding_service
           self.splitter = RecursiveCharacterTextSplitter(
               chunk_size=1000,
               chunk_overlap=200,
               length_function=len,
           )
       
       async def process_document(
           self,
           document_id: UUID,
           file_path: Path,
           file_type: str,
           chat_id: UUID,
           project_id: UUID,
           source_title: str
       ):
           """
           Procesar documento: parsear, chunking, embeddings, guardar en DB.
           
           Pasos:
           1. Parsear documento seg√∫n tipo
           2. Hacer chunking (1000 tokens, overlap 200)
           3. Generar embeddings con OpenAI
           4. Guardar chunks en marketing_knowledge_base con:
              - project_id = usuario.project_id
              - chat_id = chat_id actual
              - content_type = 'user_document'
           5. Actualizar marketing_user_documents (processed=True, chunks_count)
           """
           # 1. Parsear
           text = await parse_document(file_path, file_type)
           
           if not text.strip():
               raise ValueError("Document is empty or could not be parsed")
           
           # 2. Chunking
           chunks = self.splitter.split_text(text)
           
           # 3. Generar embeddings
           embeddings = await self.embedding_service.generate_embeddings_batch(chunks)
           
           # 4. Guardar en knowledge_base
           for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
               kb_entry = MarketingKnowledgeBase(
                   project_id=project_id,
                   chat_id=chat_id,
                   content_type='user_document',
                   source_title=source_title,
                   chunk_text=chunk,
                   chunk_index=i,
                   metadata={
                       'document_id': str(document_id),
                       'file_type': file_type,
                       'total_chunks': len(chunks),
                   },
                   embedding=embedding
               )
               self.db.add(kb_entry)
           
           # 5. Actualizar metadata del documento
           doc = await self.db.get(MarketingUserDocument, document_id)
           doc.processed = True
           doc.chunks_count = len(chunks)
           
           await self.db.commit()
           
           return {
               'chunks_count': len(chunks),
               'success': True
           }
   ```

5. **Implementar endpoints de documentos:**
   
   **`backend/src/api/documents.py`:**
   ```python
   from fastapi import APIRouter, UploadFile, File, Depends, HTTPException, BackgroundTasks
   from pathlib import Path
   from uuid import UUID, uuid4
   import os
   from ..schemas.documents import *
   from ..services.document_processor import DocumentProcessor
   from ..middleware.auth import get_current_user
   
   router = APIRouter(prefix="/api/chats/{chat_id}/documents", tags=["documents"])
   
   STORAGE_PATH = Path("backend/storage/documents")
   STORAGE_PATH.mkdir(parents=True, exist_ok=True)
   
   @router.post("/upload", response_model=UploadDocumentResponse)
   async def upload_document(
       chat_id: UUID,
       file: UploadFile = File(...),
       background_tasks: BackgroundTasks,
       current_user: dict = Depends(get_current_user)
   ):
       """
       Subir documento y procesarlo en background.
       
       Validaciones:
       - Tipo de archivo: .txt, .pdf, .docx
       - Tama√±o m√°ximo: 10MB
       - Sanitizaci√≥n de nombre de archivo
       """
       # 1. Validaciones
       file_type = Path(file.filename).suffix.lower()
       if file_type not in ALLOWED_FILE_TYPES:
           raise HTTPException(400, f"File type not allowed. Allowed: {ALLOWED_FILE_TYPES}")
       
       file_size = 0
       content = bytearray()
       async for chunk in file.stream():
           file_size += len(chunk)
           if file_size > MAX_FILE_SIZE:
               raise HTTPException(400, f"File too large. Max: {MAX_FILE_SIZE / 1024 / 1024}MB")
           content.extend(chunk)
       
       # 2. Sanitizar nombre
       safe_filename = "".join(c for c in file.filename if c.isalnum() or c in "._- ")
       
       # 3. Guardar en storage
       document_id = uuid4()
       file_path = STORAGE_PATH / str(current_user['project_id']) / str(chat_id) / f"{document_id}{file_type}"
       file_path.parent.mkdir(parents=True, exist_ok=True)
       
       with open(file_path, 'wb') as f:
           f.write(content)
       
       # 4. Crear entrada en DB
       doc = MarketingUserDocument(
           id=document_id,
           chat_id=chat_id,
           project_id=current_user['project_id'],
           user_id=current_user['user_id'],
           filename=safe_filename,
           file_type=file_type,
           file_size=file_size,
           file_path=str(file_path),
           processed=False
       )
       db.add(doc)
       await db.commit()
       
       # 5. Procesar en background
       background_tasks.add_task(
           document_processor.process_document,
           document_id, file_path, file_type, chat_id,
           current_user['project_id'], safe_filename
       )
       
       return UploadDocumentResponse.model_validate(doc)
   
   @router.get("/", response_model=ListDocumentsResponse)
   async def list_documents(
       chat_id: UUID,
       current_user: dict = Depends(get_current_user)
   ):
       """Listar documentos subidos en este chat."""
       query = select(MarketingUserDocument).where(
           MarketingUserDocument.chat_id == chat_id,
           MarketingUserDocument.project_id == current_user['project_id']
       ).order_by(MarketingUserDocument.created_at.desc())
       
       result = await db.execute(query)
       docs = result.scalars().all()
       
       return ListDocumentsResponse(
           documents=[DocumentMetadata.model_validate(d) for d in docs],
           total=len(docs)
       )
   
   @router.delete("/{document_id}")
   async def delete_document(
       chat_id: UUID,
       document_id: UUID,
       current_user: dict = Depends(get_current_user)
   ):
       """
       Eliminar documento y sus chunks.
       
       Pasos:
       1. Eliminar chunks de marketing_knowledge_base
       2. Eliminar metadata de marketing_user_documents
       3. Eliminar archivo f√≠sico
       """
       # 1. Obtener documento
       doc = await db.get(MarketingUserDocument, document_id)
       if not doc or doc.project_id != current_user['project_id']:
           raise HTTPException(404, "Document not found")
       
       # 2. Eliminar chunks
       await db.execute(
           delete(MarketingKnowledgeBase).where(
               MarketingKnowledgeBase.project_id == current_user['project_id'],
               MarketingKnowledgeBase.chat_id == chat_id,
               MarketingKnowledgeBase.metadata['document_id'].astext == str(document_id)
           )
       )
       
       # 3. Eliminar metadata
       await db.delete(doc)
       await db.commit()
       
       # 4. Eliminar archivo f√≠sico
       try:
           Path(doc.file_path).unlink()
       except FileNotFoundError:
           pass  # Archivo ya no existe
       
       return {"success": True}
   ```

6. **Registrar router en main.py:**
   ```python
   from .api import auth, chat, documents
   
   app.include_router(auth.router)
   app.include_router(chat.router)
   app.include_router(documents.router)
   ```

**Criterios de aceptaci√≥n:**
- [ ] Parsers implementados para .txt, .pdf, .docx
- [ ] DocumentProcessor hace chunking correcto (1000 tokens, overlap 200)
- [ ] Embeddings generados correctamente con OpenAI
- [ ] Chunks guardados en `marketing_knowledge_base` con `project_id` y `chat_id`
- [ ] Metadata en `marketing_user_documents` actualizada
- [ ] Endpoints funcionan:
  - [ ] POST /api/chats/{id}/documents/upload
  - [ ] GET /api/chats/{id}/documents
  - [ ] DELETE /api/chats/{id}/documents/{doc_id}
- [ ] Validaciones funcionan (tipo, tama√±o)
- [ ] Procesamiento en background no bloquea respuesta
- [ ] Tests:
  - [ ] Subir archivo .txt y verificar chunks en DB
  - [ ] Subir archivo .pdf y verificar extracci√≥n
  - [ ] Subir archivo .docx y verificar extracci√≥n
  - [ ] Buscar chunks del documento en vector store
  - [ ] Eliminar documento y verificar cleanup completo

**Archivos a crear:**
- `backend/src/schemas/documents.py`
- `backend/src/utils/file_parsers.py`
- `backend/src/services/document_processor.py`
- `backend/src/api/documents.py`
- `backend/tests/test_documents.py`

**Comandos de validaci√≥n:**
```bash
# Tests
pytest backend/tests/test_documents.py -v

# Test manual (con archivo test.txt)
curl -X POST http://localhost:8000/api/chats/<chat_id>/documents/upload \
  -H "Authorization: Bearer $TOKEN" \
  -F "file=@test.txt"

# Verificar chunks en DB
psql $DATABASE_URL -c "SELECT COUNT(*) FROM marketing_knowledge_base WHERE content_type='user_document' AND chat_id='<chat_id>';"
```

---

### TAREA 4: Agente IA con Memoria (N√öCLEO DEL SISTEMA)

**Herramientas a utilizar:**
- ‚ö° MCP Archon: LangChain + Memoria + Agentes
  - Comando: `rag_search_knowledge_base(query="langchain agent memory conversation", source_id="e74f94bb9dcb14aa", match_count=8)`
  - Comando: `rag_search_knowledge_base(query="langgraph stateful agent", source_id="e74f94bb9dcb14aa", match_count=5)`
  - Comando: `rag_search_code_examples(query="langchain tools custom", source_id="e74f94bb9dcb14aa", match_count=5)`

- üîß MCP Serena: Analizar patr√≥n de agente de ejemplo
  - Comando: `find_symbol('research_agent', 'Context-Engineering-Intro/examples/main_agent_reference/research_agent.py', True)`
  - Comando: `find_symbol('ResearchAgentDependencies', 'Context-Engineering-Intro/examples/main_agent_reference/research_agent.py', True)`

- üìö Skills:
  - **agent-memory-systems** (dise√±o de sistema de memoria multi-nivel)
  - **autonomous-agents** (patrones de agentes aut√≥nomos y orquestaci√≥n)
  - **rag-implementation** (chunking, embeddings, b√∫squeda sem√°ntica)
  - **prompt-engineering** (t√©cnicas avanzadas de prompting para agentes)
  - **ai-agents-architect** (arquitectura de sistemas multi-agente)
  - **context-window-management** (gesti√≥n eficiente de contexto LLM)
  - **llm-app-patterns** (patrones de producci√≥n para apps LLM)
  - python-patterns (autom√°tico)
  - clean-code (autom√°tico)
  - brainstorming (ANTES de implementar - OBLIGATORIO)

**‚ö†Ô∏è IMPORTANTE: Esta tarea es el N√öCLEO del sistema. El agente NO genera contenido autom√°ticamente. Solo genera cuando el usuario lo solicita expl√≠citamente.**

**Objetivo:**
Implementar el sistema de agentes IA con 3 tipos de memoria (short-term, long-term, semantic) que:
1. Genera an√°lisis completo de buyer persona
2. Procesa documentos subidos por el usuario
3. Extrae insights
4. ENTREGA documento completo
5. ESPERA peticiones del usuario
6. Genera contenido SOLO cuando se le pide

**üìã DISE√ëO VALIDADO:** 
Este task implementa las decisiones documentadas en: `docs/plans/2026-01-27-agentes-memoria-design.md`

**Decisiones clave aplicadas:**
- ‚úÖ **DECISI√ìN 1:** LangGraph (state machine) para Router Agent
- ‚úÖ **DECISI√ìN 2:** MemoryManager centralizado (combina 3 tipos de memoria)
- ‚úÖ **DECISI√ìN 3:** Rule-based routing (sin LLM extra, m√°s r√°pido)
- ‚úÖ **DECISI√ìN 4:** LLM configurable (OpenAI/OpenRouter v√≠a variable de entorno)
- ‚úÖ **DECISI√ìN 5:** Implementaci√≥n incremental (Fase 1: Router + Buyer Persona)
- ‚úÖ **DECISI√ìN 6:** B√∫squeda sem√°ntica simple (mejorar en TAREA 5)
- ‚úÖ **DECISI√ìN 7:** Retry con exponential backoff (manejo robusto de errores)
- ‚úÖ **DECISI√ìN 8:** Prompt √∫nico con plantilla completa (40+ preguntas, NO saltarse ninguna)

**Archivos a crear en esta TAREA:**
```
backend/src/agents/
‚îú‚îÄ‚îÄ base_agent.py              # Clase base compartida
‚îú‚îÄ‚îÄ router_agent.py            # Orquestador (LangGraph state machine)
‚îî‚îÄ‚îÄ buyer_persona_agent.py     # Genera buyer persona (40+ preguntas)

backend/src/services/
‚îú‚îÄ‚îÄ memory_manager.py          # MemoryManager centralizado
‚îú‚îÄ‚îÄ llm_service.py             # LLM configurable (OpenAI/OpenRouter)
‚îî‚îÄ‚îÄ rag_service.py             # B√∫squeda sem√°ntica simple
```

**Pasos a seguir:**

1. **[OBLIGATORIO] Ejecutar Skill brainstorming ANTES de implementar:**
   ```
   @.cursor/skills/brainstorming/SKILL.md explorar dise√±o de sistema de agentes
   ```
   - ‚úÖ **COMPLETADO**: Ver `docs/plans/2026-01-27-agentes-memoria-design.md`
   - ‚úÖ Arquitectura: LangGraph seleccionado
   - ‚úÖ Memoria: MemoryManager centralizado
   - ‚úÖ LLM: Configurable (OpenAI/OpenRouter)

2. **Consultar Archon sobre agentes y memoria:**
   - Ejecutar comandos listados arriba
   - Leer sobre ConversationBufferWindowMemory
   - Estudiar ReAct agent pattern

3. **Analizar patr√≥n de agente de ejemplo con Serena:**
   ```python
   # Ver estructura de research_agent.py
   get_symbols_overview('Context-Engineering-Intro/examples/main_agent_reference/research_agent.py')
   
   # Leer implementaci√≥n completa
   find_symbol('research_agent', '...', include_body=True)
   ```

4. **Implementar servicios base:**
   
   **`backend/src/services/llm_service.py`:**
   ```python
   from anthropic import AsyncAnthropic
   from typing import AsyncIterator
   
   class LLMService:
       def __init__(self, api_key: str):
           self.client = AsyncAnthropic(api_key=api_key)
           self.model = "claude-3-5-sonnet-20241022"
       
       async def generate(self, prompt: str, system: str = "") -> str:
           """Generaci√≥n s√≠ncrona (para an√°lisis)."""
           response = await self.client.messages.create(
               model=self.model,
               max_tokens=4096,
               system=system,
               messages=[{"role": "user", "content": prompt}]
           )
           return response.content[0].text
       
       async def stream(self, prompt: str, system: str = "") -> AsyncIterator[str]:
           """Streaming (para chat en tiempo real)."""
           async with self.client.messages.stream(
               model=self.model,
               max_tokens=4096,
               system=system,
               messages=[{"role": "user", "content": prompt}]
           ) as stream:
               async for text in stream.text_stream:
                   yield text
   ```
   
   **`backend/src/services/embedding_service.py`:**
   ```python
   from openai import AsyncOpenAI
   from typing import List
   import asyncio
   
   class EmbeddingService:
       def __init__(self, api_key: str):
           self.client = AsyncOpenAI(api_key=api_key)
           self.model = "text-embedding-3-large"
           self.dimension = 1536
       
       async def generate_embedding(self, text: str) -> List[float]:
           """Generar embedding para un texto."""
           response = await self.client.embeddings.create(
               model=self.model,
               input=text
           )
           return response.data[0].embedding
       
       async def generate_embeddings_batch(self, texts: List[str]) -> List[List[float]]:
           """
           ‚ö†Ô∏è GOTCHA CR√çTICO APLICADO:
           **GOTCHA 5 - OpenAI Rate Limits**
           
           Problema: OpenAI tier free tiene l√≠mite de 3000 RPM (requests per minute).
           Si procesas 5000 chunks con requests individuales ‚Üí rate limit error.
           
           Soluci√≥n implementada:
           - Batch de 50 textos por request (reduce 5000 requests a 100)
           - Exponential backoff si hay rate limit (espera 2^n segundos)
           - Peque√±a pausa entre batches (0.5s) para evitar burst
           
           Resultado: 5000 chunks procesados en ~2 minutos sin errores vs timeout instant√°neo
           """
           batch_size = 50  # ‚úÖ OpenAI permite hasta 2048 inputs por request
           results = []
           max_retries = 5
           
           for i in range(0, len(texts), batch_size):
               batch = texts[i:i+batch_size]
               retry_count = 0
               
               while retry_count < max_retries:
                   try:
                       response = await self.client.embeddings.create(
                           model=self.model,
                           input=batch
                       )
                       results.extend([e.embedding for e in response.data])
                       break  # √âxito, salir del retry loop
                       
                   except Exception as e:
                       if "rate_limit" in str(e).lower():
                           wait_time = 2 ** retry_count  # Exponential backoff
                           print(f"‚ö†Ô∏è Rate limit. Retry {retry_count+1}/{max_retries} en {wait_time}s")
                           await asyncio.sleep(wait_time)
                           retry_count += 1
                           
                           if retry_count == max_retries:
                               raise Exception(f"Max retries alcanzado para batch {i//batch_size + 1}")
                       else:
                           raise
               
               # Peque√±a pausa entre batches para evitar burst
               await asyncio.sleep(0.5)
           
           return results
   ```
   
   **`backend/src/services/vector_search.py`:**
   ```python
   from typing import List, Dict, Optional
   from uuid import UUID
   
   class VectorSearchService:
       def __init__(self, db):
           self.db = db
       
       async def search(
           self,
           query_embedding: List[float],
           project_id: UUID,
           chat_id: Optional[UUID] = None,
           match_threshold: float = 0.7,
           match_count: int = 10
       ) -> List[Dict]:
           """
           Buscar documentos similares usando funci√≥n de Supabase.
           
           Busca en:
           - Conocimiento global (project_id NULL, chat_id NULL)
           - Documentos del usuario (project_id, chat_id espec√≠ficos)
           """
           query = f"""
           SELECT * FROM marketing_match_documents(
               ARRAY{query_embedding}::vector,
               {match_threshold},
               {match_count},
               '{project_id}'::uuid,
               {f"'{chat_id}'::uuid" if chat_id else 'NULL'}
           )
           """
           result = await self.db.execute(query)
           return [dict(row) for row in result]
   ```

5. **Implementar Memory Manager:**
   
   **‚ö†Ô∏è GOTCHA CR√çTICO APLICADO:**
   **GOTCHA 2 - LangChain ConversationBufferMemory crece indefinidamente**
   
   Problema: `ConversationBufferMemory` sin l√≠mite guarda TODOS los mensajes.
   - Despu√©s de 100+ mensajes: miles de tokens por request
   - Latencia alta (prompt enorme)
   - Costo $$$
   - Puede exceder l√≠mite de contexto
   
   Soluci√≥n implementada:
   - Usar `ConversationBufferWindowMemory(k=10)` con l√≠mite expl√≠cito
   - Solo mantiene √∫ltimos 10 turnos de conversaci√≥n
   - Guardar todos los mensajes en DB (long-term) pero solo cargar √∫ltimos 10
   
   **`backend/src/agents/memory_manager.py`:**
   ```python
   from langchain.memory import ConversationBufferWindowMemory  # ‚úÖ Window, NO Buffer
   from typing import Dict, List, Optional
   from uuid import UUID
   
   class AgentMemoryManager:
       """
       Gestiona 3 tipos de memoria:
       1. Short-term: Ventana de conversaci√≥n (√∫ltimas 10 interacciones) ‚Üê GOTCHA 2
       2. Long-term: Buyer persona + Customer Journey en DB
       3. Semantic: Embeddings para b√∫squeda vectorial
       """
       def __init__(self, db, embedding_service, vector_search):
           self.db = db
           self.embedding_service = embedding_service
           self.vector_search = vector_search
           
           # ‚úÖ CORRECTO: ConversationBufferWindowMemory con l√≠mite k=10
           self.short_term = ConversationBufferWindowMemory(
               k=10,  # Solo √∫ltimos 10 turnos (user + assistant = 1 turno)
               return_messages=True,
               memory_key="chat_history"
           )
           
           # ‚ùå INCORRECTO: ConversationBufferMemory sin l√≠mite
           # from langchain.memory import ConversationBufferMemory
           # self.short_term = ConversationBufferMemory()  # ‚Üê Crece indefinidamente!
           
           # Short-term memory (por chat_id)
           self.short_term_memories = {}
       
       def get_short_term_memory(self, chat_id: UUID) -> ConversationBufferWindowMemory:
           """
           Obtener memoria de ventana para un chat.
           
           GOTCHA: ConversationBufferMemory crece indefinidamente.
           Soluci√≥n: Usar BufferWindowMemory con k=10.
           """
           if chat_id not in self.short_term_memories:
               self.short_term_memories[chat_id] = ConversationBufferWindowMemory(
                   k=10,
                   return_messages=True,
                   memory_key="chat_history"
               )
           return self.short_term_memories[chat_id]
       
       async def get_long_term_context(self, chat_id: UUID, project_id: UUID) -> Dict:
           """
           Recuperar contexto de largo plazo desde DB.
           
           Retorna:
           - buyer_persona: An√°lisis completo
           - customer_journey: 3 fases
           - documents_uploaded: Lista de documentos del usuario
           """
           # Buscar buyer persona
           query = select(MarketingBuyerPersona).where(
               MarketingBuyerPersona.chat_id == chat_id,
               MarketingBuyerPersona.project_id == project_id
           )
           result = await self.db.execute(query)
           buyer_persona = result.scalar_one_or_none()
           
           # Buscar documentos subidos
           query = select(MarketingUserDocument).where(
               MarketingUserDocument.chat_id == chat_id,
               MarketingUserDocument.project_id == project_id,
               MarketingUserDocument.processed == True
           )
           result = await self.db.execute(query)
           documents = result.scalars().all()
           
           return {
               'buyer_persona': buyer_persona.full_analysis if buyer_persona else None,
               'customer_journey': buyer_persona.customer_journey if buyer_persona else None,
               'pain_points': buyer_persona.pain_points if buyer_persona else None,
               'documents_uploaded': [{'id': str(d.id), 'filename': d.filename} for d in documents]
           }
       
       async def get_semantic_context(
           self,
           query: str,
           project_id: UUID,
           chat_id: UUID,
           k: int = 10
       ) -> List[Dict]:
           """
           B√∫squeda sem√°ntica en knowledge_base.
           
           Busca en:
           1. Documentos del usuario (chat_id espec√≠fico)
           2. Conocimiento global (YouTubers + libros)
           """
           # Generar embedding del query
           query_embedding = await self.embedding_service.generate_embedding(query)
           
           # Buscar en vector store
           results = await self.vector_search.search(
               query_embedding=query_embedding,
               project_id=project_id,
               chat_id=chat_id,
               match_count=k
           )
           
           return results
       
       async def save_to_long_term(
           self,
           chat_id: UUID,
           project_id: UUID,
           data: Dict
       ):
           """
           Guardar an√°lisis en memoria de largo plazo.
           
           Tambi√©n genera embedding del an√°lisis completo.
           """
           # Generar embedding del an√°lisis
           full_text = str(data.get('full_analysis', ''))
           embedding = await self.embedding_service.generate_embedding(full_text)
           
           # Guardar en DB
           buyer_persona = MarketingBuyerPersona(
               chat_id=chat_id,
               project_id=project_id,
               initial_questions=data['initial_questions'],
               full_analysis=data['full_analysis'],
               forum_simulation=data['forum_simulation'],
               pain_points=data['pain_points'],
               customer_journey=data['customer_journey'],
               embedding=embedding
           )
           self.db.add(buyer_persona)
           await self.db.commit()
   ```

6. **Implementar agentes especializados:**
   
   **`backend/src/agents/buyer_persona_agent.py`:**
   ```python
   from typing import Dict, List
   from uuid import UUID
   
   # CR√çTICO: Cargar plantilla desde archivo real del proyecto
   # Ubicaci√≥n: contenido/buyer-plantilla.md
   
   BUYER_PERSONA_TEMPLATE = """
   Eres un experto en marketing digital con amplios conocimientos en mercadolog√≠a.
   
   CONTEXTO DE TU TAREA:
   Est√°s desarrollando un buyer persona detallado para una campa√±a publicitaria en ADS 
   y un plan de contenidos org√°nico.
   
   INFORMACI√ìN DEL NEGOCIO:
   - Empresa ofrece: {business_offering}
   - P√∫blico objetivo: {target_audience}
   - Tipo de negocio: {business_type}  # B2B o B2P
   
   DOCUMENTOS ADICIONALES DEL USUARIO:
   {context_from_documents}
   
   TU MISI√ìN:
   Desarrollar un buyer persona completo respondiendo a las preguntas de la plantilla base.
   
   IMPORTANTE:
   - Responde TODAS las preguntas de manera completa y eficiente
   - Los datos se usar√°n para campa√±as en Meta ADS y estrategia de content marketing
   - Las respuestas deben basarse en la REALIDAD del p√∫blico (no manipular para favorecer al negocio)
   - Con datos reales podremos dar soluciones reales
   - Establece un paso a paso: analiza primero, luego enf√≥cate en necesidades reales
   
   PLANTILLA A COMPLETAR (11 categor√≠as, ~40 preguntas):
   
   ## 1. ASPECTOS DEMOGR√ÅFICOS
   1. ¬øCu√°l es su nombre y edad?
   2. ¬øQu√© estudios tiene?
   3. ¬øCu√°nto cobra bruto al a√±o?
   4. ¬øEn d√≥nde vive?
   5. ¬øCu√°l es su estado civil?
   
   ## 2. HOGAR Y FAMILIA
   1. ¬øQui√©nes son los integrantes de su unidad familiar?
   2. ¬øCu√°les son sus principales actividades de ocio?
   3. ¬øCu√°les son sus principales responsabilidades en el hogar?
   
   ## 3. TRABAJO
   1. ¬øD√≥nde trabaja y qu√© cargo tiene?
   2. ¬øCu√°les son sus retos laborales?
   3. ¬øC√≥mo influye su vida laboral en la personal y viceversa?
   
   ## 4. COMPORTAMIENTO
   1. ¬øC√≥mo es la relaci√≥n con su pareja, familia y amigos?
   2. ¬øQu√© expresiones y lenguaje utiliza su grupo social?
   
   ## 5. PROBLEMA
   1. ¬øQu√© problema o dolor activa la b√∫squeda de una soluci√≥n?
   2. ¬øC√≥mo tu producto o servicio soluciona ese problema o dolor?
   
   ## 6. B√öSQUEDA DE LA SOLUCI√ìN
   1. ¬øD√≥nde busca soluciones a su problema o dolor?
   2. ¬øC√≥mo encuentra tu empresa?
   3. ¬øC√≥mo reacciona ante las propuestas comerciales?
   
   ## 7. OBJECIONES Y BARRERAS
   1. ¬øCu√°les son las barreras internas o externas por las que no compra?
   2. ¬øCu√°les son las alternativas o excusas que utiliza para no comprar?
   
   ## 8. MIEDOS E INSEGURIDADES
   1. ¬øQu√© odia encontrar cuando busca informaci√≥n sobre el producto?
   2. ¬øQu√© experiencias negativas ha tenido hasta la fecha?
   
   ## 9. COMPARACI√ìN CON LA COMPETENCIA
   1. ¬øQu√© factores compara antes de la contrataci√≥n o compra?
   2. ¬øCu√°les son las diferencias de tu producto con la competencia?
   3. ¬øEn qu√© es mejor tu producto o servicio?
   4. ¬øEn qu√© es peor?
   5. ¬øPor qu√© elige tu producto o servicio?
   
   ## 10. TU PRODUCTO O SERVICIO
   1. ¬øQu√© prestaciones o beneficios son percibidos sobre tu producto?
   2. ¬øQu√© prestaciones o beneficios NO son percibidos?
   3. ¬øCu√°les son los productos o servicios complementarios?
   4. ¬øCu√°les son las dudas y quejas m√°s habituales de postventa?
   
   EJEMPLO DE REFERENCIA (NO COPIAR, SOLO GU√çA DE FORMATO):
   Ver: contenido/buyer-plantilla.md - Caso de "Ana" (enfermera)
   
   FORMATO DE SALIDA:
   Responde en JSON estructurado con todas las categor√≠as y respuestas completas.
   """
   
   FORUM_SIMULATION_PROMPT = """
   Bas√°ndote en el buyer persona que acabas de crear, ahora TOMA EL PAPEL de esa persona.
   
   ESCENARIO:
   Est√°s en un foro de internet donde las personas se re√∫nen a quejarse o recomendar 
   este tipo de servicios.
   
   TU TAREA:
   1. Comienza a quejarte de los problemas que tienen las personas al contratar servicios similares
   2. Despu√©s de cada queja, da una soluci√≥n o lo que te gustar√≠a que ocurriese
   
   FORMATO:
   {
     "forum_posts": [
       {
         "queja": "descripci√≥n de la queja espec√≠fica",
         "solucion_deseada": "lo que me gustar√≠a que pasara"
       },
       // ... 5-7 posts
     ],
     "pain_points": [
       "punto de dolor 1",
       "punto de dolor 2",
       // ... total 10 puntos
     ]
   }
   
   LUEGO: Dame una lista de 10 PUNTOS DE DOLOR de ese personaje (buyer persona):
   - Todo lo que piensa y siente ANTES de realizar la compra
   - Criterios que eval√∫a
   - Comportamientos que tiene
   
   IMPORTANTE: S√© espec√≠fico y realista. Usa el lenguaje del buyer persona.
   """
   
   CUSTOMER_JOURNEY_PROMPT = """
   Act√∫a como un experto en content marketing.
   
   CONTEXTO:
   Bas√°ndote en:
   - El buyer persona completo
   - Su papel en el foro (quejas y puntos de dolor)
   
   TU TAREA:
   Crear el Customer Journey para la estrategia de contenidos con las 3 fases de conciencia EXTENDIDAS.
   
   DEFINICIONES:
   
   1. CONCIENCIA:
      - Todo lo que el cliente hace hasta tomar conciencia de su necesidad
      - Investigando opciones SIN intenci√≥n comercial
      - Preguntas: "¬øQu√© s√≠ntomas tengo?", "¬øPor qu√© me pasa esto?"
   
   2. CONSIDERACI√ìN:
      - Tiene 2-3 opciones identificadas
      - Investigaci√≥n en profundidad de cada opci√≥n
      - PUEDE tener alg√∫n inter√©s comercial
      - Preguntas: "¬øCu√°l es mejor?", "¬øQu√© diferencias hay?"
   
   3. COMPRA:
      - El cliente decide comprar
      - Basado en sus investigaciones previas
      - Preguntas: "¬øD√≥nde compro?", "¬øCu√°ndo es el mejor momento?"
   
   FORMATO DE SALIDA:
   {
     "awareness": {
       "mindset": "descripci√≥n de mentalidad en esta fase",
       "content_types": ["tipo 1", "tipo 2", ...],
       "busquedas_google": [
         "b√∫squeda 1", "b√∫squeda 2", ...
         // m√≠nimo 10, ideal 20
       ],
       "preguntas_cabeza": [
         "pregunta 1", "pregunta 2", ...
         // m√≠nimo 10, ideal 20
       ]
     },
     "consideration": {
       // misma estructura
     },
     "purchase": {
       // misma estructura
     }
   }
   
   IMPORTANTE:
   - Dame un M√çNIMO de 10 preguntas por cada fase (ideal 20)
   - Divide en: qu√© busca en internet + qu√© pasa por su cabeza
   - S√© EXTENSO y conoce muy bien a quien le daremos este contenido
   - Usa el conocimiento del buyer persona + foro para ser espec√≠fico
   """
   
   class BuyerPersonaAgent:
       def __init__(self, llm_service, memory_manager):
           self.llm = llm_service
           self.memory = memory_manager
       
       async def generate_analysis(
           self,
           chat_id: UUID,
           project_id: UUID,
           initial_questions: Dict,
           use_documents: bool = True
       ) -> Dict:
           """
           Generar an√°lisis completo de buyer persona.
           
           Pasos:
           1. Buscar en documentos del usuario si existen
           2. Crear prompt con plantilla + contexto
           3. Generar an√°lisis con Claude
           4. Estructurar respuesta
           """
           # 1. Buscar contexto en documentos del usuario
           context_from_docs = ""
           if use_documents:
               docs_context = await self.memory.get_semantic_context(
                   query="informaci√≥n del negocio empresa productos servicios",
                   project_id=project_id,
                   chat_id=chat_id,
                   k=5
               )
               if docs_context:
                   context_from_docs = "\n\nINFORMACI√ìN EXTRA√çDA DE DOCUMENTOS:\n" + \
                       "\n".join([f"- {d['content'][:500]}..." for d in docs_context])
           
           # 2. Crear prompt
           prompt = f"""
           RESPUESTAS INICIALES DEL USUARIO:
           1. Negocio: {initial_questions.get('q1_business')}
           2. Cliente ideal: {initial_questions.get('q2_ideal_customer')}
           3. Problema principal: {initial_questions.get('q3_main_problem')}
           4. Resultado deseado: {initial_questions.get('q4_desired_outcome')}
           5. Competencia: {initial_questions.get('q5_competition')}
           
           {context_from_docs}
           
           Con esta informaci√≥n, completa el an√°lisis de buyer persona usando la plantilla.
           Responde en formato JSON estructurado.
           """
           
           # 3. Generar an√°lisis
           response = await self.llm.generate(
               prompt=prompt,
               system=BUYER_PERSONA_TEMPLATE.format(context_from_documents=context_from_docs)
           )
           
           # 4. Parsear y retornar (asumiendo respuesta JSON)
           import json
           full_analysis = json.loads(response)
           
           return full_analysis
   ```
   
   **(Contin√∫a con ForumSimulatorAgent, PainPointsAgent, CustomerJourneyAgent, ContentGeneratorAgent siguiendo patrones similares...)**

7. **Implementar Router Agent (orquestador):**
   
   **`backend/src/agents/router_agent.py`:**
   - Usar c√≥digo de secci√≥n "Arquitectura de Agentes" del PRP
   - Implementar detecci√≥n de estado
   - Implementar detecci√≥n de petici√≥n de contenido
   - Orquestar flujo completo

**Criterios de aceptaci√≥n:**
- [ ] LLMService implementado (generate + stream)
- [ ] EmbeddingService con batch processing y rate limiting
- [ ] VectorSearchService usando funci√≥n de Supabase
- [ ] Memory Manager con 3 tipos de memoria
- [ ] BuyerPersonaAgent genera an√°lisis completo (puede usar docs del usuario)
- [ ] ForumSimulatorAgent simula foro
- [ ] PainPointsAgent extrae 10 puntos
- [ ] CustomerJourneyAgent genera 3 fases √ó 20 preguntas
- [ ] ContentGeneratorAgent genera SOLO cuando se le pide
- [ ] RouterAgent orquesta flujo correcto
- [ ] Tests >80% coverage
- [ ] Agente ENTREGA an√°lisis y ESPERA (no genera autom√°ticamente)
- [ ] Sistema de memoria funcional
- [ ] B√∫squeda sem√°ntica incluye documentos del usuario

**Archivos a crear:**
- `backend/src/services/llm_service.py`
- `backend/src/services/embedding_service.py`
- `backend/src/services/vector_search.py`
- `backend/src/agents/memory_manager.py`
- `backend/src/agents/buyer_persona_agent.py`
- `backend/src/agents/forum_simulator_agent.py`
- `backend/src/agents/pain_points_agent.py`
- `backend/src/agents/customer_journey_agent.py`
- `backend/src/agents/content_generator_agent.py`
- `backend/src/agents/router_agent.py`
- `backend/tests/test_agents.py`
- `backend/tests/test_memory.py`

**Comandos de validaci√≥n:**
```bash
# Tests unitarios de agentes
pytest backend/tests/test_agents.py -v

# Tests de memoria
pytest backend/tests/test_memory.py -v

# Test de integraci√≥n end-to-end
pytest backend/tests/integration/test_full_flow.py -v

# Verificar que agente usa documentos del usuario
# (crear chat, subir documento, generar buyer persona, verificar que usa info del doc)
```

---

### TAREA 5: Entrenamiento del Agente con Material de YouTube

**Herramientas a utilizar:**
- ‚ö° MCP Archon: Chunking strategies y document processing
  - Comando: `rag_search_knowledge_base(query="document chunking embedding strategies", match_count=5)`
  - Comando: `rag_search_code_examples(query="python pdf text extraction", match_count=3)`

- üîß MCP Serena: Analizar estructura de scripts existentes
  - Comando: `find_file('ingest', 'backend/scripts/')`

- üìö Skills:
  - **rag-implementation** (estrategias de chunking y embedding)
  - **context-window-management** (optimizaci√≥n de tokens)
  - python-patterns (autom√°tico)
  - clean-code (autom√°tico)

**Objetivo:**
Procesar las 9 transcripciones de YouTube de Andrea Estratega y cargarlas en `marketing_knowledge_base` como conocimiento global (project_id=NULL, chat_id=NULL) para que el ContentGeneratorAgent pueda usar estas t√©cnicas al generar contenido.

**‚ö†Ô∏è GOTCHA CR√çTICO APLICADO:**
**GOTCHA 5 - OpenAI Rate Limits**: Ya implementado en EmbeddingService.generate_embeddings_batch() de TAREA 4.

**üìã MEJORAS DE DISE√ëO (desde TAREA 4):**
Esta tarea implementa mejoras documentadas en: `docs/plans/2026-01-27-agentes-memoria-design.md`

**Decisi√≥n aplicada:**
- ‚úÖ **DECISI√ìN 6:** Migraci√≥n de b√∫squeda sem√°ntica simple ‚Üí h√≠brida con reranking
  - **TAREA 4:** Implement√≥ b√∫squeda vectorial simple (suficiente para MVP)
  - **TAREA 5:** Agregar filtrado por metadata + reranking con LLM
  - **Beneficios:** Mejor precisi√≥n, considera tipo de documento, reranking mejora relevancia

**Mejoras a implementar en `backend/src/services/rag_service.py`:**
```python
async def search_relevant_docs(
    chat_id: UUID,
    query: str,
    limit: int = 5,
    rerank: bool = True,
    metadata_filters: dict = None
) -> List[dict]:
    # 1. B√∫squeda vectorial (traer 3x resultados para reranking)
    initial_results = await self._vector_search(query, limit * 3)
    
    # 2. Filtrar por metadata (tipo, fecha, fuente)
    filtered = self._filter_by_metadata(initial_results, metadata_filters)
    
    # 3. Reranking con LLM (opcional, mejora relevancia)
    if rerank:
        reranked = await self._rerank_with_llm(query, filtered)
        return reranked[:limit]
    
    return filtered[:limit]
```

**Pasos a seguir:**

1. **Consultar Archon sobre estrategias de chunking:**
   - Ejecutar comandos listados arriba
   - Decidir tama√±o de chunk (recomendado: 500-1000 tokens con overlap de 100)

2. **Crear script de ingesta de datos:**
   
   **`backend/scripts/ingest_training_data.py`:**
   ```python
   import asyncio
   import os
   from pathlib import Path
   from typing import List, Dict
   from uuid import UUID
   import sys
   
   # A√±adir src al path
   sys.path.append(str(Path(__file__).parent.parent / 'src'))
   
   from services.embedding_service import EmbeddingService
   from db.database import get_db_connection
   
   CHUNK_SIZE = 800  # tokens (~3000 caracteres)
   CHUNK_OVERLAP = 100  # tokens
   
   def chunk_text(text: str, chunk_size: int = CHUNK_SIZE, overlap: int = CHUNK_OVERLAP) -> List[str]:
       """Divide texto en chunks con overlap."""
       words = text.split()
       chunks = []
       
       for i in range(0, len(words), chunk_size - overlap):
           chunk = ' '.join(words[i:i + chunk_size])
           if chunk:
               chunks.append(chunk)
       
       return chunks
   
   async def ingest_youtube_transcripts(source_dir: str):
       """Procesa transcripciones de YouTube y las sube a knowledge_base."""
       
       # 1. Inicializar servicios
       embedding_service = EmbeddingService(api_key=os.getenv('OPENAI_API_KEY'))
       db = await get_db_connection()
       
       # 2. Cargar archivos .txt
       transcript_dir = Path(source_dir)
       transcript_files = list(transcript_dir.glob('*.txt'))
       
       print(f"üìÅ Encontrados {len(transcript_files)} archivos de transcripciones")
       
       all_chunks = []
       
       # 3. Procesar cada archivo
       for file_path in transcript_files:
           with open(file_path, 'r', encoding='utf-8') as f:
               content = f.read()
           
           # Dividir en chunks
           chunks = chunk_text(content)
           
           for idx, chunk_text in enumerate(chunks):
               all_chunks.append({
                   'source_title': file_path.stem,  # Nombre del video
                   'chunk_text': chunk_text,
                   'chunk_index': idx,
                   'content_type': 'video_transcript'
               })
           
           print(f"  ‚úÖ {file_path.name}: {len(chunks)} chunks")
       
       print(f"\nüìä Total: {len(all_chunks)} chunks a procesar")
       
       # 4. Generar embeddings en batch (GOTCHA 5 manejado aqu√≠)
       print("\nüîÑ Generando embeddings (puede tomar 2-5 minutos)...")
       texts = [chunk['chunk_text'] for chunk in all_chunks]
       embeddings = await embedding_service.generate_embeddings_batch(texts)
       
       print(f"‚úÖ {len(embeddings)} embeddings generados")
       
       # 5. Insertar en base de datos
       print("\nüíæ Insertando en marketing_knowledge_base...")
       
       for chunk, embedding in zip(all_chunks, embeddings):
           await db.execute(
               """
               INSERT INTO marketing_knowledge_base 
               (project_id, chat_id, content_type, source_title, chunk_text, chunk_index, embedding, metadata)
               VALUES (NULL, NULL, $1, $2, $3, $4, $5, $6)
               """,
               chunk['content_type'],
               chunk['source_title'],
               chunk['chunk_text'],
               chunk['chunk_index'],
               embedding,
               {'source': 'youtube', 'author': 'Andrea Estratega'}
           )
       
       print(f"‚úÖ {len(all_chunks)} chunks insertados en knowledge_base")
       
       await db.close()
   
   if __name__ == "__main__":
       source = "contenido/Transcriptions Andrea Estratega/"
       asyncio.run(ingest_youtube_transcripts(source))
   ```

3. **Ejecutar script de ingesta:**
   ```bash
   cd backend
   python scripts/ingest_training_data.py
   ```
   
   Esperado:
   ```
   üìÅ Encontrados 9 archivos de transcripciones
     ‚úÖ 6 Carruseles en Instagram que te har√°n viral en 2025 (y su estructura).txt: 45 chunks
     ‚úÖ C√≥mo hacer 7 guiones virales en menos de 30 min CON UNA ESTRUCTURA copia y pega.txt: 38 chunks
     ... (7 m√°s)
   
   üìä Total: ~300-400 chunks a procesar
   
   üîÑ Generando embeddings (puede tomar 2-5 minutos)...
   ‚úÖ Batch 1: 50 embeddings
   ‚úÖ Batch 2: 50 embeddings
   ... (6-8 batches m√°s)
   ‚úÖ 350 embeddings generados
   
   üíæ Insertando en marketing_knowledge_base...
   ‚úÖ 350 chunks insertados
   ```

4. **Verificar que datos est√°n en la base:**
   ```sql
   -- Conectar a Supabase
   psql postgresql://postgres:b12f16dbcd20ef9b7097bea120576816@213.199.39.112:5432/postgres
   
   -- Verificar chunks insertados
   SELECT content_type, COUNT(*) 
   FROM marketing_knowledge_base 
   WHERE content_type = 'video_transcript'
   GROUP BY content_type;
   
   -- Esperado: video_transcript | 300-400
   
   -- Verificar t√≠tulos de videos
   SELECT DISTINCT source_title 
   FROM marketing_knowledge_base 
   WHERE content_type = 'video_transcript';
   
   -- Esperado: 9 t√≠tulos de videos
   ```

5. **Probar b√∫squeda sem√°ntica:**
   ```python
   # Test: Buscar t√©cnicas de contenido viral
   async def test_semantic_search():
       embedding_service = EmbeddingService()
       vector_search = VectorSearchService(db)
       
       # Generar embedding de query
       query = "t√©cnicas para crear carruseles virales en Instagram"
       query_embedding = await embedding_service.generate_embedding(query)
       
       # Buscar chunks similares
       results = await vector_search.search(
           query_embedding=query_embedding,
           project_id=None,  # B√∫squeda global
           chat_id=None,
           match_threshold=0.7,
           match_count=5
       )
       
       print(f"‚úÖ Encontrados {len(results)} chunks relevantes:")
       for r in results:
           print(f"  - {r['source_title']}: {r['similarity']:.2f}")
   ```

**Criterios de aceptaci√≥n:**
- [ ] Script `ingest_training_data.py` ejecuta sin errores
- [ ] 300-400 chunks insertados en `marketing_knowledge_base`
- [ ] Todos con `project_id=NULL` y `chat_id=NULL` (conocimiento global)
- [ ] Embeddings generados sin rate limit errors
- [ ] Query SQL retorna 9 source_title distintos
- [ ] B√∫squeda sem√°ntica retorna chunks relevantes con similarity >0.7
- [ ] √çndice HNSW funciona correctamente (queries <100ms)

**Archivos a crear:**
- `backend/scripts/ingest_training_data.py`

---

### TAREA 6: API de Chat con Streaming (SSE)

**Herramientas a utilizar:**
- ‚ö° MCP Archon: FastAPI Streaming + SSE
  - Comando: `rag_search_knowledge_base(query="fastapi streaming server sent events", source_id="c889b62860c33a44", match_count=5)`
  - Comando: `rag_search_code_examples(query="fastapi StreamingResponse async", source_id="c889b62860c33a44", match_count=3)`

- üîß MCP Serena: Analizar RouterAgent si existe
  - Comando: `get_symbols_overview('backend/src/agents/router_agent.py')`

- üìö Skills:
  - **api-patterns** (dise√±o de APIs streaming)
  - **autonomous-agents** (orquestaci√≥n de flujos multi-agente)
  - **llm-app-patterns** (streaming, RAG, y producci√≥n)
  - python-patterns (autom√°tico)
  - clean-code (autom√°tico)

**Objetivo:**
Implementar endpoint de chat que hace streaming de respuestas en tiempo real usando SSE (Server-Sent Events) y orquesta el flujo de agentes.

**‚ö†Ô∏è GOTCHA CR√çTICO APLICADO:**
**GOTCHA 3 - FastAPI Streaming + Middleware**

Problema: Middleware que lee `request.body()` rompe streaming.
Soluci√≥n: Excluir endpoints `/stream` y `/sse` del middleware de logging.

**Implementaci√≥n en `backend/src/main.py`:**
```python
@app.middleware("http")
async def logging_middleware(request: Request, call_next):
    # ‚úÖ Lista de paths que usan streaming
    streaming_paths = ["/api/chat/stream", "/api/sse"]
    
    if any(path in request.url.path for path in streaming_paths):
        # ‚úÖ NO leer body, pasar directamente
        return await call_next(request)
    
    # Para endpoints normales, procesar
    try:
        body = await request.body()
        logger.info(f"Request to {request.url.path}: {body[:100]}")
    except:
        pass
    
    response = await call_next(request)
    return response
```

**Pasos a seguir:**

1. **Modificar `main.py` con middleware que respeta streaming:**
   - Ver c√≥digo arriba (GOTCHA 3)

2. **Implementar endpoint de streaming:**
   
   **`backend/src/api/chat.py`:**
   ```python
   from fastapi import APIRouter, Request, HTTPException
   from fastapi.responses import StreamingResponse
   from ..agents.router_agent import RouterAgent
   from ..schemas.chat import ChatRequest
   from uuid import UUID
   
   router = APIRouter(prefix="/api/chat", tags=["chat"])
   
   @router.post("/stream")
   async def stream_chat(request: Request, chat_request: ChatRequest):
       """
       Endpoint de chat con streaming SSE.
       
       ‚ö†Ô∏è GOTCHA 3: Este endpoint est√° EXCLUIDO del middleware de logging
       para que el streaming funcione correctamente.
       """
       # Extraer user_id y project_id del middleware (GOTCHA 6)
       user_id = request.state.user_id
       project_id = request.state.project_id
       
       # Inicializar router agent
       router_agent = RouterAgent(
           db=db,
           llm_service=llm_service,
           memory_manager=memory_manager
       )
       
       async def generate_sse():
           """Genera eventos SSE (Server-Sent Events)."""
           try:
               # Procesar mensaje con agente
               async for chunk in router_agent.process_stream(
                   chat_id=chat_request.chat_id,
                   project_id=project_id,
                   user_id=user_id,
                   message=chat_request.message
               ):
                   # Formato SSE: "data: {contenido}\n\n"
                   yield f"data: {chunk}\n\n"
               
               # Se√±al de fin
               yield "data: [DONE]\n\n"
               
           except Exception as e:
               # Error en streaming
               yield f"data: {{\"error\": \"{str(e)}\"}}\n\n"
       
       return StreamingResponse(
           generate_sse(),
           media_type="text/event-stream",
           headers={
               "Cache-Control": "no-cache",
               "Connection": "keep-alive",
               "X-Accel-Buffering": "no"  # Nginx buffering off
           }
       )
   
   @router.post("/message")
   async def send_message(request: Request, chat_request: ChatRequest):
       """
       Endpoint sin streaming (espera respuesta completa).
       √ötil para an√°lisis completos (buyer persona, customer journey).
       """
       user_id = request.state.user_id
       project_id = request.state.project_id
       
       router_agent = RouterAgent(db, llm_service, memory_manager)
       
       # Procesar sin streaming
       result = await router_agent.process(
           chat_id=chat_request.chat_id,
           project_id=project_id,
           user_id=user_id,
           message=chat_request.message
       )
       
       return {"response": result}
   ```

3. **Implementar RouterAgent.process_stream():**
   
   **`backend/src/agents/router_agent.py`:**
   ```python
   from typing import AsyncIterator
   
   class RouterAgent:
       # ... (c√≥digo de TAREA 4)
       
       async def process_stream(
           self,
           chat_id: UUID,
           project_id: UUID,
           user_id: UUID,
           message: str
       ) -> AsyncIterator[str]:
           """
           Procesa mensaje y hace streaming de respuesta.
           
           Flujo:
           1. Detectar intenci√≥n
           2. Ejecutar agente apropiado con streaming
           3. Guardar en memoria
           """
           # 1. Guardar mensaje del usuario
           await self.memory.save_to_long_term(chat_id, project_id, 'user', message)
           
           # 2. Detectar estado/intenci√≥n
           state = await self.detect_state(chat_id, project_id, message)
           
           # 3. Ejecutar agente apropiado
           if state == AgentState.CONTENT_GENERATION:
               # Streaming de generaci√≥n de contenido
               async for chunk in self.content_generator.generate_stream(
                   chat_id, project_id, message
               ):
                   yield chunk
           
           elif state == AgentState.BUYER_PERSONA:
               # An√°lisis completo (sin streaming, pero enviar progreso)
               yield "Analizando buyer persona...\n"
               result = await self.buyer_persona_agent.generate_analysis(...)
               yield f"An√°lisis completo: {result}\n"
           
           # ... (otros estados)
       
       async def process(self, chat_id, project_id, user_id, message) -> Dict:
           """Versi√≥n sin streaming (retorna resultado completo)."""
           # Similar a process_stream pero sin yield
           pass
   ```

**Criterios de aceptaci√≥n:**
- [ ] Middleware NO lee body en endpoints de streaming (GOTCHA 3)
- [ ] Endpoint `/api/chat/stream` retorna SSE correctamente
- [ ] Endpoint `/api/chat/message` retorna JSON completo
- [ ] RouterAgent orquesta agentes correctamente
- [ ] Mensajes se guardan en `marketing_messages`
- [ ] Test con curl: `curl -N http://localhost:8000/api/chat/stream` retorna chunks
- [ ] Frontend puede consumir stream sin bloqueos

**Archivos a crear/modificar:**
- `backend/src/api/chat.py` (nuevo)
- `backend/src/main.py` (modificar middleware)
- `backend/src/agents/router_agent.py` (a√±adir process_stream)

**Comandos de validaci√≥n:**
```bash
# Test streaming con curl
curl -N http://localhost:8000/api/chat/stream \
  -H "Authorization: Bearer TOKEN_JWT" \
  -H "Content-Type: application/json" \
  -d '{"chat_id":"...","message":"Hola"}' \
  --no-buffer

# Esperado: Chunks en tiempo real
data: Hola
data: ,
data:  ¬øc√≥mo
data:  puedo
data:  ayudarte?
data: [DONE]
```

---

### TAREA 7: Frontend - Auth y Layout Base

**Herramientas a utilizar:**
- ‚ö° MCP Archon: Next.js 14 App Router + Auth patterns
  - Comando: `rag_search_knowledge_base(query="nextjs app router authentication middleware", source_id="77b8a4a07d5230b5", match_count=5)`
  - Comando: `rag_search_code_examples(query="nextjs cookies httponly", source_id="77b8a4a07d5230b5", match_count=3)`

- üîß MCP Serena: Analizar componentes de ejemplo
  - Comando: `find_file('layout.tsx', 'Context-Engineering-Intro/examples/')`

- üìö Skills:
  - **nextjs-best-practices** (App Router, Server Components, middleware)
  - **react-patterns** (hooks, composici√≥n, performance)
  - **tailwind-patterns** (sistema de dise√±o, tokens)
  - **clean-code** (c√≥digo limpio y mantenible)

**Objetivo:**
Crear estructura base del frontend con autenticaci√≥n, layout, y navegaci√≥n usando Next.js 14 App Router, toma en cuenta que mas adelant el proyecto se desplegara en vercel.

**‚ö†Ô∏è GOTCHA CR√çTICO APLICADO:**
**GOTCHA 10 - JWT en localStorage + Server Components**

Problema: `localStorage` no es accesible en Server Components.
Soluci√≥n: Usar cookies httpOnly + middleware de Next.js.

**Backend setea cookie (modificar en TAREA 2):**
```python
# backend/src/api/auth.py - Modificar endpoint login
from fastapi import Response

@router.post("/login")
async def login(request: LoginRequest, response: Response):
    # ... validar usuario ...
    
    # Generar JWT
    token = create_access_token({
        'user_id': str(user.id),
        'email': user.email,
        'project_id': str(user.project_id)
    })
    
    # ‚úÖ Setear cookie httpOnly (GOTCHA 10)
    response.set_cookie(
        key="auth_token",
        value=token,
        httponly=True,  # No accesible desde JavaScript
        secure=True,  # Solo HTTPS en production
        samesite="lax",
        max_age=604800  # 7 d√≠as
    )
    
    return {"user": {"id": user.id, "email": user.email}}
```

**Pasos a seguir:**

1. **Crear proyecto Next.js 14:**
   ```bash
   npx create-next-app@14 frontend --typescript --tailwind --app --src-dir=false
   cd frontend
   npm install zustand @tanstack/react-query
   ```

2. **Crear middleware de autenticaci√≥n:**
   
   **`frontend/middleware.ts`:**
   ```typescript
   import { NextResponse } from 'next/server'
   import type { NextRequest } from 'next/server'
   
   export function middleware(request: NextRequest) {
     // ‚úÖ Leer cookie httpOnly (GOTCHA 10)
     const token = request.cookies.get('auth_token')?.value
     
     // Rutas p√∫blicas
     const publicPaths = ['/login', '/register', '/reset-password']
     const isPublicPath = publicPaths.some(path => request.nextUrl.pathname.startsWith(path))
     
     // Si no hay token y la ruta es privada ‚Üí redirect a login
     if (!token && !isPublicPath) {
       return NextResponse.redirect(new URL('/login', request.url))
     }
     
     // Si hay token y est√° en p√°gina de login ‚Üí redirect a /
     if (token && isPublicPath) {
       return NextResponse.redirect(new URL('/', request.url))
     }
     
     return NextResponse.next()
   }
   
   export const config = {
     matcher: ['/((?!api|_next/static|_next/image|favicon.ico).*)']
   }
   ```

3. **Crear layout base:**
   
   **`frontend/app/layout.tsx`:**
   ```typescript
   import type { Metadata } from 'next'
   import { Inter } from 'next/font/google'
   import './globals.css'
   
   const inter = Inter({ subsets: ['latin'] })
   
   export const metadata: Metadata = {
     title: 'Marketing Second Brain',
     description: 'Sistema IA para estrategia de contenido',
   }
   
   export default function RootLayout({
     children,
   }: {
     children: React.ReactNode
   }) {
     return (
       <html lang="es">
         <body className={inter.className}>{children}</body>
       </html>
     )
   }
   ```

4. **Crear p√°ginas de autenticaci√≥n:**
   
   **`frontend/app/login/page.tsx`:**
   ```typescript
   'use client'  // ‚úÖ GOTCHA 4: Marcar como client component
   
   import { useState } from 'react'
   import { useRouter } from 'next/navigation'
   
   export default function LoginPage() {
     const [email, setEmail] = useState('')
     const [password, setPassword] = useState('')
     const [loading, setLoading] = useState(false)
     const router = useRouter()
     
     const handleLogin = async (e: React.FormEvent) => {
       e.preventDefault()
       setLoading(true)
       
       try {
         const response = await fetch('http://localhost:8000/api/auth/login', {
           method: 'POST',
           headers: { 'Content-Type': 'application/json' },
           body: JSON.stringify({ email, password }),
           credentials: 'include'  // ‚úÖ Importante para cookies
         })
         
         if (response.ok) {
           // Cookie ya seteada por backend
           router.push('/')
         } else {
           alert('Login failed')
         }
       } finally {
         setLoading(false)
       }
     }
     
     return (
       <div className="min-h-screen flex items-center justify-center">
         <form onSubmit={handleLogin} className="space-y-4">
           <input
             type="email"
             value={email}
             onChange={(e) => setEmail(e.target.value)}
             placeholder="Email"
             className="border p-2"
           />
           <input
             type="password"
             value={password}
             onChange={(e) => setPassword(e.target.value)}
             placeholder="Password"
             className="border p-2"
           />
           <button type="submit" disabled={loading}>
             {loading ? 'Loading...' : 'Login'}
           </button>
         </form>
       </div>
     )
   }
   ```

**Criterios de aceptaci√≥n:**
- [ ] Middleware de Next.js protege rutas privadas
- [ ] Cookies httpOnly funcionan (NO localStorage)
- [ ] Login exitoso setea cookie y redirige a /
- [ ] Logout limpia cookie y redirige a /login
- [ ] Layout base con navegaci√≥n
- [ ] Rutas p√∫blicas (/login, /register) accesibles sin auth
- [ ] Rutas privadas (/) redirigen a /login si no hay token

**Archivos a crear:**
- `frontend/middleware.ts`
- `frontend/app/layout.tsx`
- `frontend/app/page.tsx`
- `frontend/app/login/page.tsx`
- `frontend/app/register/page.tsx`

---

### TAREA 8: Frontend - Chat Interface con Streaming

**Herramientas a utilizar:**
- ‚ö° MCP Archon: React hooks + SSE en cliente
  - Comando: `rag_search_knowledge_base(query="react server sent events streaming", source_id="a931698c21fb8f24", match_count=5)`
  - Comando: `rag_search_code_examples(query="nextjs client component useState", source_id="77b8a4a07d5230b5", match_count=3)`

- üìö Skills:
  - **react-ui-patterns** (loading states, error handling, async data)
  - **frontend-design** (UI moderna y distintiva)
  - **nextjs-best-practices** (client vs server components)
  - **context-window-management** (gesti√≥n de mensajes largos)
  - react-patterns (autom√°tico)
  - clean-code (autom√°tico)

**Objetivo:**
Implementar interfaz de chat que consume el endpoint de streaming y muestra respuestas en tiempo real.

**‚ö†Ô∏è GOTCHA CR√çTICO APLICADO:**
**GOTCHA 4 - Next.js Server Components + useState**

Problema: `useState` NO funciona en Server Components (default en App Router).
Soluci√≥n: Directiva `'use client'` al inicio de componentes interactivos.

**Pasos a seguir:**

1. **Crear componente de chat (Client Component):**
   
   **`frontend/app/components/ChatInterface.tsx`:**
   ```typescript
   'use client'  // ‚úÖ GOTCHA 4: Marcar como client component
   
   import { useState, useEffect, useRef } from 'react'
   
   interface Message {
     role: 'user' | 'assistant'
     content: string
   }
   
   export function ChatInterface({ chatId }: { chatId: string }) {
     const [messages, setMessages] = useState<Message[]>([])
     const [input, setInput] = useState('')
     const [isStreaming, setIsStreaming] = useState(false)
     const messagesEndRef = useRef<HTMLDivElement>(null)
     
     const scrollToBottom = () => {
       messagesEndRef.current?.scrollIntoView({ behavior: 'smooth' })
     }
     
     useEffect(() => {
       scrollToBottom()
     }, [messages])
     
     const handleSend = async () => {
       if (!input.trim() || isStreaming) return
       
       const userMessage = input
       setInput('')
       
       // A√±adir mensaje del usuario
       setMessages(prev => [...prev, { role: 'user', content: userMessage }])
       
       // Iniciar streaming
       setIsStreaming(true)
       let assistantMessage = ''
       
       try {
         const response = await fetch('http://localhost:8000/api/chat/stream', {
           method: 'POST',
           headers: { 'Content-Type': 'application/json' },
           credentials: 'include',  // Cookies
           body: JSON.stringify({ chat_id: chatId, message: userMessage })
         })
         
         const reader = response.body?.getReader()
         const decoder = new TextDecoder()
         
         while (true) {
           const { done, value } = await reader!.read()
           if (done) break
           
           const chunk = decoder.decode(value)
           const lines = chunk.split('\n')
           
           for (const line of lines) {
             if (line.startsWith('data: ')) {
               const data = line.slice(6)
               
               if (data === '[DONE]') {
                 // Fin del streaming
                 break
               }
               
               // A√±adir chunk a mensaje del asistente
               assistantMessage += data
               
               // Actualizar UI en tiempo real
               setMessages(prev => {
                 const newMessages = [...prev]
                 const lastMsg = newMessages[newMessages.length - 1]
                 
                 if (lastMsg?.role === 'assistant') {
                   lastMsg.content = assistantMessage
                 } else {
                   newMessages.push({ role: 'assistant', content: assistantMessage })
                 }
                 
                 return newMessages
               })
             }
           }
         }
       } catch (error) {
         console.error('Streaming error:', error)
       } finally {
         setIsStreaming(false)
       }
     }
     
     return (
       <div className="flex flex-col h-screen max-w-4xl mx-auto p-4">
         {/* Lista de mensajes */}
         <div className="flex-1 overflow-y-auto space-y-4 mb-4">
           {messages.map((msg, idx) => (
             <div
               key={idx}
               className={`p-4 rounded-lg ${
                 msg.role === 'user' 
                   ? 'bg-blue-100 ml-auto max-w-[80%]' 
                   : 'bg-gray-100 mr-auto max-w-[80%]'
               }`}
             >
               {msg.content}
             </div>
           ))}
           <div ref={messagesEndRef} />
         </div>
         
         {/* Input box */}
         <div className="flex gap-2">
           <input
             type="text"
             value={input}
             onChange={(e) => setInput(e.target.value)}
             onKeyPress={(e) => e.key === 'Enter' && handleSend()}
             placeholder="Escribe tu mensaje..."
             disabled={isStreaming}
             className="flex-1 border rounded-lg p-3"
           />
           <button
             onClick={handleSend}
             disabled={isStreaming || !input.trim()}
             className="bg-blue-500 text-white px-6 rounded-lg disabled:opacity-50"
           >
             {isStreaming ? 'Enviando...' : 'Enviar'}
           </button>
         </div>
       </div>
     )
   }
   ```

2. **Crear p√°gina principal (Server Component wrapper):**
   
   **`frontend/app/page.tsx`:**
   ```typescript
   // ‚úÖ Server Component por defecto (NO necesita 'use client')
   import { ChatInterface } from './components/ChatInterface'
   
   export default async function HomePage() {
     // ‚úÖ Aqu√≠ podr√≠as hacer fetch de datos iniciales si fuera necesario
     // const chats = await fetchChats()  // Server-side fetch
     
     return (
       <main>
         <h1 className="text-2xl font-bold p-4">Marketing Second Brain</h1>
         {/* Client Component maneja interactividad */}
         <ChatInterface chatId="default-chat-id" />
       </main>
     )
   }
   ```

**Criterios de aceptaci√≥n:**
- [ ] Middleware protege rutas privadas
- [ ] Login funciona y setea cookie httpOnly (GOTCHA 10)
- [ ] ChatInterface es Client Component ('use client')
- [ ] HomePage es Server Component (sin 'use client')
- [ ] Streaming funciona en tiempo real
- [ ] Sin errores de hidrataci√≥n en consola
- [ ] UI responsiva (mobile + desktop)

**Archivos a crear:**
- `frontend/app/components/ChatInterface.tsx`
- `frontend/app/components/Sidebar.tsx`
- `frontend/app/components/MessageList.tsx`
- `frontend/app/page.tsx`

---

### TAREA 8.1: Memoria de Conversaci√≥n, Contexto Largo y Visualizaci√≥n de Datos

**Herramientas a utilizar:**
- üîß MCP Serena: Analizar implementaci√≥n actual de memoria
- ‚ö° MCP Archon: Documentaci√≥n de LangChain memory patterns
- üìö Skills:
  - **conversation-memory** (memoria persistente de conversaciones)
  - **context-window-management** (gesti√≥n de contexto largo)
  - **agent-memory-systems** (arquitectura de memoria)
  - **frontend-design** (UI para visualizaci√≥n)
  - python-patterns (autom√°tico)
  - clean-code (autom√°tico)

**Uso obligatorio de herramientas (en CADA PASO):**
- **Serena (obligatorio)**: antes de cambiar c√≥digo, ubicar s√≠mbolos/llamadas reales (memoria, routing, endpoints) y confirmar d√≥nde se integra.
- **Archon (obligatorio)**: antes de decidir el patr√≥n, consultar documentaci√≥n/patrones actuales (LangChain memory, context-window, retrieval) y documentar ‚Äúpor qu√©‚Äù la decisi√≥n.

**‚ö†Ô∏è PROBLEMAS IDENTIFICADOS (Ver `docs/DIAGNOSTICO_MEMORIA_Y_CONTEXTO.md`):**

1. **Memoria de conversaci√≥n NO se carga**: El agente no recuerda mensajes anteriores
2. **Router detecta TODO como contenido**: Keywords muy amplias activan siempre CONTENT_GENERATION
3. **No hay forma de ver buyer persona**: No hay endpoints/UI para visualizar datos generados
4. **Agentes faltantes**: Forum Simulator, Pain Points, Customer Journey no implementados
5. **RAG vs Contexto Largo**: Documentos solo se consultan, no est√°n siempre en contexto

**Objetivo:**
Corregir sistema de memoria, mejorar detecci√≥n de intenciones, crear visualizaci√≥n de datos, e implementar contexto largo para documentos.

**Pasos a seguir:**

#### PASO 1: Cargar Historial de Conversaci√≥n

**Problema:** `load_chat_history()` existe pero nunca se llama.

**Uso de herramientas (PASO 1):**
- üîß Serena: localizar d√≥nde se instancia `MemoryManager` y d√≥nde se atienden `GET /api/chats/{chat_id}`, `POST /messages`, `POST /stream` para insertar la carga de historial sin romper SSE.
- ‚ö° Archon: validar patr√≥n recomendado para memoria por conversaci√≥n (por `chat_id`) y ventana \(k\), incluyendo `return_messages`.

**Soluci√≥n:**
1. Modificar `GET /api/chats/{chat_id}` para cargar historial al abrir chat
2. Modificar `POST /api/chats/{chat_id}/messages` para cargar historial antes de procesar
3. Modificar `POST /api/chats/{chat_id}/stream` para cargar historial antes de streaming

**Archivos a modificar:**
- `backend/src/api/chat.py` - agregar llamadas a `memory_manager.load_chat_history()`

**Criterios:**
- [ ] Al abrir un chat, se cargan √∫ltimos 20 mensajes
- [ ] `ConversationBufferWindowMemory` est√° poblado antes de procesar nuevos mensajes
- [ ] El agente puede referirse a mensajes anteriores

---

#### PASO 2: Mejorar Detecci√≥n de Solicitudes de Contenido

**Problema:** Keywords muy amplias detectan cualquier mensaje como solicitud de contenido.

**Uso de herramientas (PASO 2):**
- üîß Serena: localizar `_is_content_request` y su uso en `route()` + ejemplos de mensajes que se clasifican mal.
- ‚ö° Archon: consultar patrones de intent detection (reglas + guardrails) y cu√°ndo conviene LLM vs reglas.

**Hallazgos (PASO 2):**
- **Serena**: `RouterAgent/_is_content_request` usa `any(keyword in message_lower ...)` con keywords demasiado amplias (incluye `"necesito"`), lo que dispara `CONTENT_GENERATION` en mensajes no-petici√≥n.
- **Archon**: para MVP, preferir **reglas estrictas** con guardrails (word boundaries + combinaci√≥n ‚Äúverbo de solicitud‚Äù + ‚Äúobjeto de contenido‚Äù) antes que keywords sueltas; dejar LLM intent-detection como opci√≥n posterior si siguen falsos positivos.

**Soluci√≥n:**
1. Usar LLM para detectar intenci√≥n (m√°s preciso)
2. O mejorar keywords con contexto:
   - Verificar que sea solicitud expl√≠cita (no pregunta)
   - Considerar contexto de conversaci√≥n
   - No activar si usuario est√° respondiendo preguntas del agente

**Archivos a modificar:**
- `backend/src/agents/router_agent.py` - mejorar `_is_content_request()` o usar LLM

**Criterios:**
- [ ] Solo activa CONTENT_GENERATION con solicitudes expl√≠citas
- [ ] Puede mantener conversaci√≥n normal sin generar contenido
- [ ] Considera contexto de mensajes anteriores

---

#### PASO 3: Crear Endpoints API para Visualizar Datos

**Problema:** No hay forma de ver buyer persona, foro, puntos de dolor, customer journey.

**Uso de herramientas (PASO 3):**
- üîß Serena: localizar modelos/relaciones (`MarketingBuyerPersona`, `MarketingChat`) y patrones existentes de endpoints/auth.
- ‚ö° Archon: consultar dise√±o de APIs REST para ‚Äúread-only views‚Äù + autorizaci√≥n por `project_id`.

**Hallazgos (PASO 3):**
- **Serena**: el backend actualmente incluye routers en `backend/src/main.py` (`auth`, `chat`, `documents`). Los endpoints siguen patr√≥n `APIRouter(prefix="/api/...", tags=[...])` y usan `Depends(get_current_user)` para aislar por `project_id`.
- **Archon**: para ‚Äúread-only views‚Äù conviene endpoints dedicados por recurso (ej. `/api/chats/{chat_id}/analysis`) y respuestas JSON estables; si una secci√≥n a√∫n no existe (forum/pain_points/journey), devolverla vac√≠a pero expl√≠cita (sin 404) para que UI pueda mostrar ‚Äúpendiente‚Äù.

**Soluci√≥n:**
1. Crear nuevos endpoints:
   - `GET /api/chats/{chat_id}/buyer-persona` - Obtener buyer persona completo
   - `GET /api/chats/{chat_id}/forum-simulation` - Obtener simulaci√≥n de foro
   - `GET /api/chats/{chat_id}/pain-points` - Obtener puntos de dolor
   - `GET /api/chats/{chat_id}/customer-journey` - Obtener customer journey
   - `GET /api/chats/{chat_id}/analysis` - Obtener todo el an√°lisis (resumen)

2. Crear schemas de respuesta:
   - `backend/src/schemas/buyer_persona.py` - Schemas para respuestas

**Archivos a crear:**
- `backend/src/api/buyer_persona.py` - Nuevos endpoints
- `backend/src/schemas/buyer_persona.py` - Schemas de respuesta

**Criterios:**
- [ ] Endpoints retornan datos en formato JSON estructurado
- [ ] Manejo de errores si no existe buyer persona
- [ ] Validaci√≥n de permisos (solo usuario del proyecto)

---

#### PASO 4: Crear Componentes Frontend para Visualizaci√≥n

**Problema:** No hay UI para ver los datos generados.

**Uso de herramientas (PASO 4):**
- üîß Serena: localizar componentes actuales (`ChatInterface`, layout) y d√≥nde insertar ‚Äúpanel de an√°lisis‚Äù sin romper SSR/Suspense.
- ‚ö° Archon: revisar patrones UI para estados (loading/error/empty) y manejo de context window (mostrar ‚Äúresumen‚Äù vs ‚ÄúJSON raw‚Äù).

**Soluci√≥n:**
1. Crear componentes React:
   - `BuyerPersonaView.tsx` - Visualizar buyer persona completo
   - `ForumSimulationView.tsx` - Visualizar simulaci√≥n de foro
   - `PainPointsView.tsx` - Visualizar puntos de dolor
   - `CustomerJourneyView.tsx` - Visualizar customer journey
   - `AnalysisSummaryView.tsx` - Vista resumen de todo el an√°lisis

2. Crear API client:
   - `frontend/lib/api-analysis.ts` - Cliente para endpoints de an√°lisis

3. Integrar en ChatInterface:
   - Agregar bot√≥n/tab para ver an√°lisis
   - Mostrar estado de generaci√≥n (completo, parcial, pendiente)

**Archivos a crear:**
- `frontend/app/components/BuyerPersonaView.tsx`
- `frontend/app/components/ForumSimulationView.tsx`
- `frontend/app/components/PainPointsView.tsx`
- `frontend/app/components/CustomerJourneyView.tsx`
- `frontend/app/components/AnalysisSummaryView.tsx`
- `frontend/lib/api-analysis.ts`

**Criterios:**
- [ ] UI muestra buyer persona de forma legible y organizada
- [ ] UI muestra foro, puntos de dolor, customer journey si existen
- [ ] UI indica qu√© partes est√°n completas y cu√°les pendientes
- [ ] Dise√±o consistente con el resto de la aplicaci√≥n

---

#### PASO 5: Implementar Contexto Largo para Documentos

**Problema:** Documentos solo se consultan v√≠a RAG, no est√°n siempre en contexto.

**Uso de herramientas (PASO 5):**
- üîß Serena: confirmar si existe columna/estructura para ‚Äúsummary‚Äù de documentos y d√≥nde se procesa upload.
- ‚ö° Archon: validar patr√≥n ‚Äúlong-term doc summaries + RAG‚Äù (resumen persistente + retrieval puntual) y l√≠mites de tokens.

**Hallazgos (PASO 5):**
- **Serena**: `marketing_user_documents` actualmente NO tiene columna `summary` (solo tracking: filename/file_path/chunks_count/processed). Por tanto, ‚Äúres√∫menes persistentes‚Äù requieren **migraci√≥n** (SQL) o alternativa (guardar resumen en `metadata` de KB / tabla nueva).
- **Archon**: patr√≥n recomendado para ‚Äúcontexto largo de docs‚Äù en producci√≥n suele ser **resumen persistente + RAG** (no meter todos los docs crudos al prompt). Implica: generar resumen al subir, guardar, e inyectar res√∫menes + top-k retrieval en prompts.

**Soluci√≥n:**
1. Cuando se sube un documento:
   - Generar resumen/extracto con LLM
   - Guardar resumen en tabla `marketing_user_documents.summary`
   - Mantener embeddings para RAG

**Implementaci√≥n (PASO 5):**
- Crear migraci√≥n: `backend/db/002_add_user_document_summary.sql` (ejecuci√≥n manual en Supabase).
- Backend:
  - `MarketingUserDocument.summary` (nullable) + exponer `summary` en schemas de documentos.
  - En `POST /api/documents/upload/{chat_id}` generar `summary` (sin bloquear si falla) y persistir.
  - `MemoryManager.get_context()` incluir `document_summaries` para inyectar en prompts como ‚Äúcontexto largo‚Äù.
  - `ContentGeneratorAgent` incluye `document_summaries` + RAG (top-k) en el prompt.

2. En `MemoryManager.get_context()`:
   - Incluir res√∫menes de documentos del chat en contexto largo
   - Mantener RAG para b√∫squeda espec√≠fica

3. En prompts de agentes:
   - Incluir res√∫menes de documentos siempre (no solo cuando se busca)

**Archivos a modificar:**
- `backend/src/services/memory_manager.py` - agregar `get_document_summaries()`
- `backend/src/api/documents.py` - generar resumen al subir documento
- `backend/src/agents/content_generator_agent.py` - incluir res√∫menes en prompt

**Criterios:**
- [ ] Documentos subidos tienen resumen generado
- [ ] Res√∫menes se incluyen en contexto largo del LLM
- [ ] El agente puede referirse a informaci√≥n de documentos sin buscar
- [ ] RAG sigue funcionando para b√∫squeda espec√≠fica

---

### TAREA 8.2: Buyer Persona Extendido (Plantilla + Foro + Customer Journey)

**Herramientas a utilizar:**
- üîß MCP Serena: localizar y editar `backend/src/agents/buyer_persona_agent.py`
- ‚ö° MCP Archon: revisar patrones de prompts largos y respuestas JSON robustas
- üìö Skills:
  - **conversation-memory**, **agent-memory-systems**
  - **python-patterns**, **clean-code**

**Objetivo:**
Completar el an√°lisis del buyer persona generando autom√°ticamente:
- `full_analysis` basado en la plantilla completa `contenido/buyer-plantilla.md`
- `forum_simulation` con posts de foro y soluciones deseadas
- `pain_points` con 10 puntos de dolor claros
- `customer_journey` con 3 fases (awareness, consideration, purchase) y listas de b√∫squedas/preguntas.

**Pasos a seguir:**

1. **Usar plantilla completa desde disco**
   - Leer `contenido/buyer-plantilla.md` desde el backend (ruta relativa al root del proyecto).
   - Actualizar `_build_buyer_persona_prompt()` de `BuyerPersonaAgent` para:
     - Inyectar el contenido completo del markdown en el prompt.
     - Indicar expl√≠citamente que debe usar solo las PREGUNTAS como gu√≠a y **no** copiar las respuestas de ejemplo (caso Ana).
     - Exigir salida en **JSON v√°lido**, estructurado por secciones (t√≠tulos de la plantilla como claves de primer nivel).

2. **Generar foro + puntos de dolor autom√°ticamente**
   - A√±adir m√©todo interno `BuyerPersonaAgent._generate_forum_and_pain_points(buyer_persona_data)`.
   - Basarse en el prompt de foro definido en `contenido/promts_borradores.md` (secci√≥n ‚ÄúPromt para simulacion en foro de buyer persona‚Äù).
   - El LLM debe devolver JSON con estructura:
     ```json
     {
       "posts": [{ "queja": "...", "solucion_deseada": "..." }],
       "pain_points": { "items": ["Punto 1", "...", "Punto 10"] }
     }
     ```
   - Guardar:
     - `forum_simulation = { "posts": [...] }`
     - `pain_points = { "items": [...] }`

3. **Generar customer journey (CJ) autom√°ticamente**
   - A√±adir m√©todo interno `BuyerPersonaAgent._generate_customer_journey(buyer_persona_data, forum_simulation, pain_points)`.
   - Basarse en el prompt ‚ÄúPrompt costumer journey‚Äù de `contenido/promts_borradores.md`.
   - El LLM debe devolver JSON con estructura:
     ```json
     {
       "awareness": { "busquedas": [...], "preguntas_cabeza": [...] },
       "consideration": { "busquedas": [...], "preguntas_cabeza": [...] },
       "purchase": { "busquedas": [...], "preguntas_cabeza": [...] }
     }
     ```
   - Guardar el resultado en `customer_journey`.

4. **Integraci√≥n en el flujo actual**
   - Dentro de `BuyerPersonaAgent.execute()`:
     - Despu√©s de generar y persistir `full_analysis`, llamar secuencialmente a:
       - `_generate_forum_and_pain_points(...)`
       - `_generate_customer_journey(...)`
     - Actualizar el registro de `MarketingBuyerPersona` reci√©n creado con:
       - `forum_simulation`, `pain_points`, `customer_journey`
       - `await db.commit()` al final de la secuencia.
   - No modificar:
     - `RouterAgent`
     - `MemoryManager`
     - endpoints existentes de an√°lisis (`/api/chats/{chat_id}/analysis`, `/buyer-persona`).

**Archivos a modificar:**
- `backend/src/agents/buyer_persona_agent.py`

**Criterios de aceptaci√≥n:**
- [ ] Primer buyer persona generado para un chat rellena:
  - `full_analysis`
  - `forum_simulation.posts` (‚â•1 post con `queja` + `solucion_deseada`)
  - `pain_points.items` (exactamente 10 √≠tems)
  - `customer_journey.awareness/consideration/purchase` con ‚â•10 `busquedas` y ‚â•10 `preguntas_cabeza` cada una.
- [ ] `GET /api/chats/{chat_id}/analysis` refleja `has_forum_simulation=true`, `has_pain_points=true`, `has_customer_journey=true` tras el primer an√°lisis.
- [ ] El panel de an√°lisis actual muestra la nueva informaci√≥n sin romper el flujo de TAREA 8.1.

---

#### PASO 6: Implementar Agentes Faltantes (Opcional - Futuro)

**Nota:** Estos agentes est√°n fuera del scope de TAREA 8.1, pero se documentan para futuro.

**Agentes a implementar:**
1. `ForumSimulatorAgent` - Simula comportamiento en foro
2. `PainPointsExtractorAgent` - Extrae puntos de dolor
3. `CustomerJourneyCreatorAgent` - Crea customer journey

**Archivos a crear (futuro):**
- `backend/src/agents/forum_simulator_agent.py`
- `backend/src/agents/pain_points_agent.py`
- `backend/src/agents/customer_journey_agent.py`

---

**Criterios de aceptaci√≥n generales:**
- [ ] El agente mantiene contexto de conversaci√≥n (recuerda mensajes anteriores)
- [ ] El agente solo genera contenido cuando se solicita expl√≠citamente
- [ ] Usuario puede ver buyer persona generado en UI
- [ ] Usuario puede ver foro, puntos de dolor, customer journey (si existen)
- [ ] Documentos subidos est√°n siempre en contexto largo del agente
- [ ] Conversaci√≥n fluida y natural, no solo generaci√≥n de contenido

**Archivos a crear:**
- `backend/src/api/buyer_persona.py`
- `backend/src/schemas/buyer_persona.py`
- `frontend/app/components/BuyerPersonaView.tsx`
- `frontend/app/components/ForumSimulationView.tsx`
- `frontend/app/components/PainPointsView.tsx`
- `frontend/app/components/CustomerJourneyView.tsx`
- `frontend/app/components/AnalysisSummaryView.tsx`
- `frontend/lib/api-analysis.ts`
- `docs/DIAGNOSTICO_MEMORIA_Y_CONTEXTO.md` ‚úÖ (ya creado)

**Archivos a modificar:**
- `backend/src/api/chat.py` - cargar historial
- `backend/src/agents/router_agent.py` - mejorar detecci√≥n de contenido
- `backend/src/services/memory_manager.py` - contexto largo de documentos
- `backend/src/api/documents.py` - generar res√∫menes
- `backend/src/agents/content_generator_agent.py` - incluir res√∫menes

---

### TAREA 9: MCP Custom del Proyecto

**Herramientas a utilizar:**
- ‚ö° MCP Archon: MCP Protocol y server creation
  - Comando: `rag_search_knowledge_base(query="MCP server python custom tools", match_count=5)`

- üîß MCP Serena: Analizar estructura de MCPs existentes
  - Comando: `get_symbols_overview('/home/david/.cursor/projects/home-david-brain-mkt/mcps/user-archon/')`

- üìö Skills:
  - **mcp-builder** (construcci√≥n de MCPs custom)
  - **agent-tool-builder** (dise√±o de herramientas para agentes)
  - python-patterns (autom√°tico)
  - clean-code (autom√°tico)

**Objetivo:**
Crear MCP custom que expone herramientas del proyecto para que Cursor pueda interactuar con el sistema.

**Pasos a seguir:**

1. **Crear estructura del MCP:**
   ```bash
   mkdir -p mcp-marketing-brain/src
   cd mcp-marketing-brain
   touch pyproject.toml README.md
   ```

2. **Implementar servidor MCP:**
   
   **`mcp-marketing-brain/src/server.py`:**
   ```python
   from mcp.server import Server
   from mcp.types import Tool, TextContent
   import asyncio
   
   app = Server("marketing-brain")
   
   @app.tool()
   async def get_buyer_persona(chat_id: str) -> str:
       """Obtiene buyer persona generado para un chat."""
       # Conectar a BD y obtener buyer persona
       pass
   
   @app.tool()
   async def list_chats(project_id: str) -> str:
       """Lista todos los chats de un proyecto."""
       pass
   
   @app.tool()
   async def generate_content_ideas(phase: str, content_type: str, count: int = 5) -> str:
       """Genera ideas de contenido basadas en buyer persona."""
       pass
   ```

3. **Registrar en Cursor:**
   ```json
   // ~/.cursor/mcp-config.json
   {
     "mcpServers": {
       "marketing-brain": {
         "command": "python",
         "args": ["-m", "mcp_marketing_brain"],
         "cwd": "/home/david/brain-mkt/mcp-marketing-brain"
       }
     }
   }
   ```

**Criterios de aceptaci√≥n:**
- [ ] MCP server funciona en Cursor
- [ ] Tools expuestas: get_buyer_persona, list_chats, generate_content_ideas
- [ ] Cursor puede llamar tools desde chat

**Archivos a crear:**
- `mcp-marketing-brain/src/server.py`
- `mcp-marketing-brain/pyproject.toml`

---

### TAREA 9.1: Edici√≥n y Eliminaci√≥n Segura de Chats

**Herramientas a utilizar:**
- üîß MCP Serena: inspecci√≥n de flujo de chats
  - `search_for_pattern("class ChatService", "backend/src")`
  - `search_for_pattern("Sidebar(", "frontend/app/components")`
- üìö Skills:
  - **senior-fullstack**, **senior-architect**
  - clean-code (autom√°tico)

**Objetivo:**
Permitir **renombrar** y **eliminar** chats desde el sidebar, asegurando que:
- Al eliminar un chat:
  - Se elimina el registro en `marketing_chats`
  - Se eliminan en cascada SOLO los datos ligados a ese chat:
    - mensajes (`marketing_messages`)
    - buyer persona + foro + pain points + customer journey (`marketing_buyer_personas`)
    - documentos y chunks asociados (`marketing_user_documents`, `marketing_knowledge_base`)
- El frontend refresca la lista sin romper la sesi√≥n actual.

**Notas de coherencia (estado actual):**
- Backend ya tiene:
  - `ChatService.update_chat_title(...)`
  - `ChatService.delete_chat(...)` con `ondelete='CASCADE'` en:
    - `MarketingMessage.chat_id`
    - `MarketingBuyerPersona.chat_id`
    - `MarketingKnowledgeBase.chat_id`
    - `MarketingUserDocument.chat_id`
  - Endpoints expuestos en `backend/src/api/chat.py`:
    - `PATCH /api/chats/{chat_id}/title`
    - `DELETE /api/chats/{chat_id}`
- Frontend:
  - `Sidebar.tsx` ya lista y crea chats, pero **a√∫n no expone** renombrar / eliminar.

**Pasos a seguir (sin romper lo existente):**

1. **Frontend ‚Äì API client:**
   - A√±adir en `frontend/lib/api-chat.ts`:
     - `updateChatTitle(chatId: string, title: string): Promise<ChatSummary>`
       - PATCH ` /api/chats/{chat_id}/title` con body `{ title }`
     - `deleteChat(chatId: string): Promise<void>`
       - DELETE `/api/chats/{chat_id}`

2. **Frontend ‚Äì Sidebar UI:**
   - En `Sidebar.tsx`:
     - A√±adir acciones por chat:
       - Renombrar: prompt simple (`window.prompt`) y llamada a `updateChatTitle`.
       - Eliminar: `window.confirm` y llamada a `deleteChat`.
     - Actualizar estado `chats` en memoria sin recargar toda la p√°gina.
     - Si se elimina el chat activo:
       - Seleccionar el siguiente chat disponible (o limpiar selecci√≥n) y actualizar URL (`/?chat=...`).

3. **Validaci√≥n:**
   - Crear >2 chats, cambiar nombres y borrar uno:
     - Confirmar que desaparece del sidebar.
     - Confirmar en DB (via SQL o tests) que:
       - El chat NO existe.
       - Buyer persona + foro + pain points + journey de ese chat NO existen.
       - Documentos y chunks con ese `chat_id` NO existen.

**Criterios de aceptaci√≥n:**
- [ ] Desde el sidebar se puede renombrar un chat sin perder mensajes.
- [ ] Desde el sidebar se puede eliminar un chat y:
  - Desaparece de la lista.
  - Si era el chat activo, se selecciona otro o se deja sin selecci√≥n.
- [ ] En base de datos no quedan registros hu√©rfanos asociados a ese `chat_id`.

---

### TAREA 10: Docker + Deployment

**Herramientas a utilizar:**
- ‚ö° MCP Archon: Docker best practices
  - Comando: `rag_search_knowledge_base(query="docker compose multi-container production", match_count=5)`

- üìö Skills:
  - **docker-expert** (containerizaci√≥n, optimizaci√≥n, seguridad)
  - **deployment-procedures** (estrategias de deployment)
  - **server-management** (gesti√≥n de procesos y recursos)
  - clean-code (autom√°tico)

**‚ö†Ô∏è GOTCHA CR√çTICO APLICADO:**
**GOTCHA 8 - Docker volumes en Windows**

Problema: Bind mounts (`./backend:/app`) tienen permisos raros en Windows/WSL.
Soluci√≥n: Usar named volumes para persistencia.

**Objetivo:**
Crear configuraci√≥n de Docker para desarrollo y producci√≥n.

**Pasos a seguir:**

1. **Crear Dockerfile del backend:**
   
   **`backend/Dockerfile`:**
   ```dockerfile
   FROM python:3.11-slim
   
   WORKDIR /app
   
   COPY pyproject.toml ./
   RUN pip install --no-cache-dir -e .
   
   COPY . .
   
   CMD ["uvicorn", "src.main:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]
   ```

2. **Crear Dockerfile del frontend:**
   
   **`frontend/Dockerfile`:**
   ```dockerfile
   FROM node:20-alpine
   
   WORKDIR /app
   
   COPY package*.json ./
   RUN npm install
   
   COPY . .
   
   CMD ["npm", "run", "dev"]
   ```

3. **Crear docker-compose.yml:**
   
   **`docker-compose.yml`:**
   ```yaml
   version: '3.8'
   
   services:
     backend:
       build: ./backend
       ports:
         - "8000:8000"
       environment:
         - SUPABASE_URL=${SUPABASE_URL}
         - SUPABASE_SERVICE_ROLE_KEY=${SUPABASE_SERVICE_ROLE_KEY}
         - SUPABASE_DB_URL=${SUPABASE_DB_URL}
         - OPENAI_API_KEY=${OPENAI_API_KEY}
         - OPENROUTER_API_KEY=${OPENROUTER_API_KEY}
         - JWT_SECRET_KEY=${JWT_SECRET_KEY}
       volumes:
         # ‚úÖ GOTCHA 8: Named volume en vez de bind mount
         - backend_storage:/app/storage
         # Para desarrollo: bind mount solo para c√≥digo
         - ./backend/src:/app/src:ro
       depends_on:
         - redis
     
     frontend:
       build: ./frontend
       ports:
         - "3000:3000"
       environment:
         - NEXT_PUBLIC_API_URL=http://localhost:8000/api
       volumes:
         - ./frontend/app:/app/app:ro
         - ./frontend/components:/app/components:ro
       depends_on:
         - backend
     
     redis:
       image: redis:7-alpine
       ports:
         - "6379:6379"
       volumes:
         # ‚úÖ GOTCHA 8: Named volume
         - redis_data:/data
   
   volumes:
     backend_storage:  # ‚úÖ Named volume (funciona en Windows/Linux/Mac)
     redis_data:
   ```

**Criterios de aceptaci√≥n:**
- [ ] `docker compose up` inicia todos los servicios
- [ ] Backend accesible en :8000
- [ ] Frontend accesible en :3000
- [ ] Named volumes persisten datos correctamente
- [ ] Sin problemas de permisos en Windows/WSL (GOTCHA 8)

**Archivos a crear:**
- `backend/Dockerfile`
- `frontend/Dockerfile`
- `docker-compose.yml`

---

### TAREA 11: Documentaci√≥n Final (tests E2E parciales / futura iteraci√≥n)

**Herramientas a utilizar:**
- ‚ö° MCP Archon: Pytest patterns
  - Comando: `rag_search_code_examples(query="pytest async fixtures", match_count=3)`

- üìö Skills:
  - **testing-patterns** (unit, integration, E2E)
  - **verification-before-completion** (validaci√≥n exhaustiva)
  - **systematic-debugging** (debugging met√≥dico)
  - **test-driven-development** (TDD cuando aplique)
  - python-patterns (autom√°tico)
  - clean-code (autom√°tico)

**Objetivo (ajustado):**
- En esta iteraci√≥n: **documentaci√≥n final del proyecto** (README, uso, gotchas, trazas).
- Dejar **plantilla y esqueletos** de tests E2E listos, sin exigir cobertura completa ahora.

**Pasos a seguir:**

1. **Tests end-to-end del flujo completo:**
   
   **`backend/tests/integration/test_full_flow.py`:**
   ```python
   import pytest
   
   @pytest.mark.asyncio
   async def test_complete_buyer_persona_flow():
       """Test del flujo completo: registro ‚Üí chat ‚Üí buyer persona."""
       
       # 1. Registrar usuario
       # 2. Login
       # 3. Crear chat
       # 4. Enviar mensaje con info del negocio
       # 5. Verificar que genera buyer persona completo
       # 6. Verificar que NO genera contenido autom√°ticamente
       
       pass
   
   @pytest.mark.asyncio
   async def test_document_upload_and_usage():
       """Test: documento subido se usa en an√°lisis."""
       
       # 1. Login
       # 2. Subir documento con info del negocio
       # 3. Generar buyer persona
       # 4. Verificar que an√°lisis menciona info del documento
       
       pass
   ```

2. **Actualizar README.md con instrucciones finales:**
   - Quick start completo
   - Troubleshooting
   - Ejemplos de uso

3. **Crear documentaci√≥n de API:**
   ```bash
   # FastAPI genera docs autom√°ticas
   # Verificar en: http://localhost:8000/docs
   ```

**Criterios de aceptaci√≥n (ajustados):**
- [ ] README completo y actualizado (incluye:
  - Setup backend/frontend
  - Variables de entorno
  - Flujo buyer persona ‚Üí an√°lisis ‚Üí contenido
  - Uso del panel Trace y flags de salud (`has_history`, `rag_used`, `training_injected`, `tecnicas_aplicadas_count`)
- [ ] Referencia r√°pida de endpoints clave (auth, chats, stream, documentos, an√°lisis)
- [ ] Esqueletos de tests E2E creados (`backend/tests/integration/test_full_flow.py`) pero sin requerir 100% coverage en esta fase
- [ ] Checklist de validaci√≥n manual actualizado (secci√≥n ‚ÄúChecklist de Validaci√≥n Final‚Äù)

**Archivos a crear:**
- `backend/tests/integration/test_full_flow.py`
- `README.md` (actualizar)

---

## üîÑ Bucle de Validaci√≥n

### Nivel 1: Sintaxis & Estilo
```bash
# Ejecutar PRIMERO - arreglar errores antes de proceder
ruff check backend/src/ --fix
mypy backend/src/

# Frontend
npx eslint frontend/app/ --fix
npx tsc --noEmit

# Esperado: Sin errores. Si hay errores, LEER y ARREGLAR
```

### Nivel 2: Tests Unitarios
```python
# Backend: CREAR tests siguiendo patrones
def test_buyer_persona_agent_generates_complete_analysis():
    """BuyerPersonaAgent retorna an√°lisis completo con 35+ campos."""
    agent = BuyerPersonaAgent(llm, memory)
    result = await agent.generate_analysis(chat_id, project_id, initial_questions)
    
    assert 'full_analysis' in result
    assert len(result['full_analysis']) >= 35
    assert 'demographics' in result['full_analysis']

def test_content_generator_not_called_automatically():
    """ContentGenerator NO se ejecuta sin petici√≥n expl√≠cita."""
    router = RouterAgent(memory, llm)
    state = await router.route(chat_id, "Hola, tengo un negocio de X")
    
    assert state != AgentState.CONTENT_GENERATION
    assert state in [AgentState.BUYER_PERSONA, AgentState.WAITING]

def test_document_processor_includes_in_search():
    """Documentos subidos aparecen en b√∫squeda sem√°ntica."""
    # Subir documento
    doc_id = await upload_document(chat_id, file)
    await process_document(doc_id)
    
    # Buscar
    results = await vector_search.search(query, project_id, chat_id)
    
    # Verificar que incluye chunks del documento
    assert any(r['content_type'] == 'user_document' for r in results)
```

```bash
# Ejecutar e iterar hasta que pasen:
pytest backend/tests/ -v --cov=backend/src --cov-report=html

# Frontend:
npm test -- --coverage
```

### Nivel 3: Test de Integraci√≥n
```bash
# 1. Iniciar servicios
docker-compose up -d

# 2. Test del flujo completo
curl -X POST http://localhost:8000/api/auth/register \
  -H "Content-Type: application/json" \
  -d '{"email":"test@example.com","password":"Test1234","full_name":"Test","project_id":"<uuid>"}'

# Obtener token
TOKEN=$(curl -X POST http://localhost:8000/api/auth/login \
  -H "Content-Type: application/json" \
  -d '{"email":"test@example.com","password":"Test1234","project_id":"<uuid>"}' \
  | jq -r '.access_token')

# Crear chat
CHAT_ID=$(curl -X POST http://localhost:8000/api/chats \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"title":"Test Chat"}' \
  | jq -r '.id')

# Subir documento
curl -X POST http://localhost:8000/api/chats/$CHAT_ID/documents/upload \
  -H "Authorization: Bearer $TOKEN" \
  -F "file=@test_business_info.txt"

# Esperar procesamiento (background task)
sleep 5

# Enviar mensaje para iniciar an√°lisis
curl -X POST http://localhost:8000/api/chats/$CHAT_ID/messages \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"content":"Mi negocio es una tienda online de productos eco-friendly"}'

# Verificar que agente respondi√≥ con an√°lisis
# Verificar que an√°lisis incluye info del documento subido

# 3. Test frontend
open http://localhost:3000
# Manual: Login, crear chat, subir doc, chat con agente, pedir ideas de contenido
```

---

## ‚úÖ Checklist de Validaci√≥n Final

**Funcionalidad Completa:**
- [ ] Autenticaci√≥n: register, login, recover password
- [ ] Chats: crear, listar, editar, eliminar
- [ ] Documentos: subir (.txt, .pdf, .docx), listar, eliminar
- [ ] Procesamiento: documentos se procesan en background
- [ ] Agente: genera buyer persona completo (con docs si existen)
- [ ] Agente: simula foro y extrae pain points
- [ ] Agente: genera customer journey (3 fases √ó 20 preguntas)
- [ ] Agente: ENTREGA an√°lisis y ESPERA (no genera autom√°ticamente)
- [ ] Memoria: funciona (short/long/semantic)
- [ ] Contenido on-demand: genera SOLO cuando usuario pide
- [ ] B√∫squeda sem√°ntica: incluye docs del usuario + knowledge base global
- [ ] Streaming: respuestas en tiempo real (<3s primera palabra)
- [ ] Aislamiento: datos por project_id (sin mezcla)

**Calidad de C√≥digo:**
- [ ] Sin errores de linting: `ruff check backend/src/`
- [ ] Sin errores de tipos: `mypy backend/src/`
- [ ] Sin errores TypeScript: `npx tsc --noEmit`
- [ ] Tests >80% coverage: `pytest --cov`
- [ ] Documentaci√≥n: README.md, API.md completos

**Performance:**
- [ ] B√∫squeda vectorial <100ms
- [ ] Streaming LLM <3s primera respuesta
- [ ] Frontend carga <2s (Lighthouse >90)
- [ ] API <200ms (endpoints simples)

**Seguridad:**
- [ ] JWT implementado correctamente
- [ ] Passwords con bcrypt (cost 12)
- [ ] RLS habilitado en Supabase
- [ ] API keys no expuestas
- [ ] Validaci√≥n de input en todos los endpoints
- [ ] Archivos: validaci√≥n MIME, tama√±o, sanitizaci√≥n

**Docker:**
- [ ] docker-compose up funciona sin errores
- [ ] Frontend accesible en :3000
- [ ] Backend accesible en :8000
- [ ] Redis funcional
- [ ] Vol√∫menes persisten datos
- [ ] Logs sin errores cr√≠ticos

---

## ‚ùå Anti-Patrones a Evitar

**C√≥digo:**
- ‚ùå No crear patrones cuando existentes funcionan
- ‚ùå No saltar validaci√≥n porque "deber√≠a funcionar"
- ‚ùå No ignorar tests fallidos
- ‚ùå No usar sync en contexto async
- ‚ùå No hardcodear valores (usar env vars)
- ‚ùå No usar catch-all exceptions
- ‚ùå No leer archivos completos sin Serena primero
- ‚ùå No buscar en web sin Archon primero

**Arquitectura:**
- ‚ùå No hacer que ContentGenerator se ejecute autom√°ticamente
- ‚ùå No mezclar datos entre proyectos (siempre filtrar por project_id)
- ‚ùå No usar Supabase Auth (implementar manual)
- ‚ùå No generar embeddings s√≠ncronamente
- ‚ùå No hacer queries N+1 en loops

**Base de Datos:**
- ‚ùå No crear √≠ndice ivfflat con <1000 rows (usar hnsw)
- ‚ùå No usar service role key confiando en RLS (validar project_id manualmente)
- ‚ùå No olvidar normalizar embeddings antes de insertar

**Agentes:**
- ‚ùå No usar descripciones vagas en tools (ser espec√≠fico)
- ‚ùå No asumir que ConversationBufferMemory es suficiente (usar WindowMemory)
- ‚ùå No olvidar buscar en documentos del usuario cuando genera contenido

---

## üìä Score de Confianza

**Evaluar este PRP en escala 1-10:**

**Criterios Actuales:**
- ‚úÖ Archon consultado exhaustivamente (Pydantic, FastAPI, LangChain, Supabase)
- ‚úÖ Source IDs espec√≠ficos incluidos
- ‚úÖ Queries de ejemplo proporcionadas
- ‚úÖ Serena: TAREA 0 obligatoria incluida
- ‚úÖ Comandos espec√≠ficos de Serena en cada tarea
- ‚úÖ Skills identificadas por fase con justificaci√≥n
- ‚úÖ Gotchas cr√≠ticos documentados (10 GOTCHAS + 4 WARNINGS)
- ‚úÖ Modelos completos (SQLAlchemy + Pydantic)
- ‚úÖ Estructura de tareas con herramientas, pasos, criterios
- ‚úÖ Validaci√≥n en 3 niveles (sintaxis, tests, integraci√≥n)
- ‚úÖ Anti-patrones espec√≠ficos del proyecto
- ‚úÖ Ejemplos de c√≥digo con contexto del proyecto
- ‚úÖ Comandos de validaci√≥n ejecutables

**Score Estimado: 9/10**

**Razones:**
- Contexto extremadamente completo
- MCPs integrados estrat√©gicamente
- Skills activadas en momentos correctos
- Tareas 0-3.5 con detalle quir√∫rgico
- Patr√≥n de desarrollo claro
- Gotchas de producci√≥n documentados
- Sistema de memoria bien dise√±ado
- Flujo de documentos del usuario integrado
- Aislamiento multi-tenant correcto

**√önica debilidad:** Tareas 4-11 necesitan completarse con mismo nivel de detalle que 0-3.5 (esto se har√° en siguiente iteraci√≥n o durante ejecuci√≥n).

**Implementaci√≥n one-pass:** ‚úÖ ALTA PROBABILIDAD

---

## üéØ Pr√≥ximos Pasos

1. **Ejecutar PRP:**
   ```bash
   # Abrir archivo PRP en Cursor
   code PRPs/marketing-brain-system-v3.md
   
   # Comenzar por TAREA 0 (instalar Serena)
   # Luego proceder secuencialmente
   ```

2. **Durante Ejecuci√≥n:**
   - Consultar Archon con queries proporcionadas
   - Usar Serena ANTES de leer archivos
   - Activar Skills en momentos indicados
   - Ejecutar validaci√≥n despu√©s de cada tarea
   - Iterar si tests fallan (leer error, entender, arreglar, re-ejecutar)

3. **Al Completar:**
   - Verificar checklist final
   - Ejecutar tests de integraci√≥n completos
   - Documentar cualquier decisi√≥n arquitect√≥nica tomada
   - Actualizar README.md con setup instructions

4. **Post-MVP:**
   - Deployment a VPS con Portainer
   - Features adicionales (exportar PDF, calendario de contenido)
   - Optimizaciones de performance
   - Fine-tuning de prompts basado en feedback

---

## üìö Referencias y Recursos

**Archon Source IDs:**
- Pydantic v2: `9d46e91458092424`
- FastAPI: `c889b62860c33a44`
- LangChain: `e74f94bb9dcb14aa`
- Supabase: `9c5f534e51ee9237`
- Next.js 14: `77b8a4a07d5230b5`
- React: `a931698c21fb8f24`
- TypeScript: `d7c76d077e634ab3`
- shadcn/ui: `bf102fe8a697ed7c`

**Ejemplos del Proyecto:**
- `Context-Engineering-Intro/examples/main_agent_reference/`: Patr√≥n de agentes con dependencies
- `Context-Engineering-Intro/examples/mcp-server/`: MCP server con auth y tools

**Skills Cr√≠ticas:**
- Planificaci√≥n: `planning-with-files`, `brainstorming`, `architecture`
- Desarrollo: `clean-code`, `python-patterns`, `react-patterns`
- Agentes: `agent-memory-systems`, `autonomous-agents`, `rag-implementation`
- Validaci√≥n: `lint-and-validate`, `verification-before-completion`, `systematic-debugging`

---

**üöÄ ESTE PRP EST√Å LISTO PARA EJECUCI√ìN**

**Recuerda:**
- TAREA 0 es OBLIGATORIA (instalar Serena)
- Consultar Archon ANTES que web
- Usar Serena ANTES de leer archivos
- Activar Skills en momentos correctos
- Validar despu√©s de cada tarea
- Iterar hasta que tests pasen

**¬°√âxito en la implementaci√≥n! üéâ**

